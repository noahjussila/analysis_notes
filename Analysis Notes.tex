\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{blindtext}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{mathrsfs}
\def\upint{\mathchoice%
	{\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
	{\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
	{\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
	{\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
	\int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\usepackage{hyperref}
\usepackage{tikz}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\ze}{\mathbf{0}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\uni}{\overset{\text{uni}}{\to}}
\DeclareMathOperator{\diam}{diam}
\newcommand{\Prr}[1]{\text{Pr}\left(#1\right)}
\pgfplotsset{my style/.append style={axis x line=middle, axis y line=
		middle, xlabel={$x$}, ylabel={$y$}, axis equal }}

\setcounter{section}{-1}
\theoremstyle{definition}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{ex}{Exercise}[section]
\newtheorem{note}{Notation}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\usepackage{cleveref}
\title{Real Analysis}
\author{Noah Jussila}
\date{\today}

\begin{document}
	\maketitle
	\bibliographystyle{chicago}
	\tableofcontents

\section{Introduction}
This collection of notes is an ongoing project that aims to combine four semesters of real analysis notes into one document. My goal in doing so is to not only stave off the boredom resulting from quarantining due to Covid-19, but also pick and choose from different presentations of standard material in real analysis. 
\subsection{Prerequisites}
These notes will assume familiarity with differential and integral calculus, linear algebra, multivariable calculus, basic set theory, and properties of functions (image, preimage, injectivity, etc.) At times, some material, particularly concepts in linear algebra, will be reviewed. Two very good video series that serve as reviews for calculus and linear algebra can be found at this amazing \href{https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw}{YouTube channel}.

I also will assume familiarity with basic styles of proof (contradiction, contrapositive, etc.). I will try \textit{very hard} to be as detailed as possible with every proof though. 
\subsection{Organization and Sources}
If these notes are ever finished (which is unlikely), they will span about four semesters worth of real analysis: two at an honors undergraduate level, and two at a introductory PhD level. 


My selection of sources is motivated not only by the books I really enjoy, but also those that are considered standard. For material presented in an undergraduate course, I absolutely love \cite{tao2006analysis} and \cite{tao2009analysis}. Tao starts at the most basic level of arithmetic and numbers are works all the way up to Lebesgue Integration in his two part series. He motivates nearly every topic, provides examples, and gives the reader a big picture of the subject. That being said, his approach left me confused at times when I first read his books. In the first volume, Tao eschews point-set topology entirely and opts to work exclusively in $ \R $. This is great because most people who learn real analysis only ever care about $ \R $, but it also makes some concepts overly technical. For instance, I think that seuqences and continuity are easier to present and motivate in general metric spaces. My hunch is that I'm in the minority here, as Terry Tao went the other direction, and he has one more Fields Medal than I do. Tao also doesn't include any figures. Another text that is awesome is the infamous ``Baby'' \cite{rudin1964principles}. This book is so widely used for a reason.\footnote{My uneducated hypothesis about this is that \cite{rudin1964principles} really only perfect makes sense if you already know all the material well, so professors who do not remember what it is like to learn analysis just assume everyone understands this book.} It's concise, clear, and presents the material with no frills. It makes a great textbook if you have a professor who motivates the material, provides examples, and draws the occasional illustration. If you do not have such a professor, than \cite{rudin1964principles} becomes pure torture. This book is very much the outline of the first 7 sections of these notes. Sections 8-11 pull mostly from...


\subsection{Presentation}
Many standard math textbooks assume the reader can create their own novel examples, fill in the blanks purposefully left in proofs, understand the motivation for the material, and pick up on the subtle ``tricks'' used in proofs. A good professor will help students do this during a lecture. I want my presentation to do this. In doing this, I'm very much inspired by a professor I had for a second semester honors analysis course I took at Boston College. His presentation of some of the material that will be treated not only will be replicated here, but also motivates how I approach other topics. This means lots of illustrations, footnotes, remarks, and asking hypothetical questions to motivate material. It may seem very pedantic if you're familiar with analysis, but in that case just read Rudin's books. 
\newpage
\section{The Real Numbers}
We begin by returning to the most basic concepts in math. What exactly is a number? We begin with the most basic possible set of numbers, and use those to define more complex sets of numbers, with our goal being to define the real numbers. Lastly, we will look at the ``size'' of these sets, and explore the concept of infinity. 
\subsection{Natural Numbers, Integers, and Rational Numbers}
First, a cursory overview of several sets of numbers is in order. It is given for the sake of exposition, and to illustrate how we define sets of numbers using previously defined sets. For the sake of time, the formal definition of the standard operations (addition, multiplication, etc.) on these sets will be forgone. Rest assured that the operations we are all familiar with are well defined on these sets, and this can be shown rigorously. An excellent reference for this within the context of real analysis can be found in \cite{tao2006analysis}, who takes nothing as given. 

 The most basic numbers are those we use to count. We will call these natural numbers. \begin{definition}
Define the set $ \N\coloneqq\{0,1,2,\ldots\} $ to be the \textit{{\color{red} natural numbers}}. 
\end{definition}
\noindent The operations of addition and multiplication are well defined on $ \N $, in that when adding or multiplying natural numbers, the result is a natural number. A more succinct way of putting this is saying that $ \N $ is \textit{closed} under addition and multiplication. While the natural numbers are great for things such as counting (a fact we will return to), it fails to be useful for much more. In particular, two basic operations we are familiar with are not well defined on $ \N $.
\begin{example}
	Suppose we want to find the difference in $ 2 $ and $ 5 $, both elements in $ \N $. The difference in question would be $ 2-5 $, but this is not an element of $ \N $! 
\end{example}

In order to address this shortcoming, we need to broaden our view. In effect, we need to ``add more'' numbers to $ \N $. We want to enlarge the set of natural numbers by the amount necessary for subtraction to be well defined. This can be done by taking the set of all differences of natural numbers. For all intents and purposes, this is how we define the integers. 
\begin{definition}
	Define the set $ \Z\coloneqq\{a-b\mid(a,b)\in\N^2\} $ to be the \textit{\color{red}integers}. Two integers are equal, $ a-b=c-d $, if and only if $ a+d=b+c $.\footnote{The added specification of when integers are equal can be avoided by defining $ \Z $ to be a set of equivalence classes. The equivelence relation $ \sim $ would be defined on $ \N $ as $ (a,b)\sim(c,d) $ when $ a+c=b+d $.}  
\end{definition}
\noindent We will take the ordering of $ \Z $, the negation of elements of $ \Z $, and all arithmetic properties of $ \Z $ to be given. There are two things worth noting. The first is that $ \N\subseteq\Z $. The identity element $ 0\in\N $ gives $ a-0=a $ for each $ a\in\N $, so any natural number can be written as the difference of two natural numbers. Secondly, we defined $ \Z $ only by using $ \N $. This is crucial, as we will define the rationals only by using $ \Z $, and in turn define the real numbers only by using the rationals.

While subtraction is well defined for $ \Z $, the same does not hold for division. 
\begin{example}
 Take the integers $ -3 $ and 6, and suppose we are interested in the ratio of the prior to the latter. Obviously, $$ \frac{-3}{6}=-\frac{1}{2} ,$$ but this is not an element of $ \Z $. 
\end{example}
We can now ``extend'' the integers to accommodate for division, in a similar fashion to when we defined the integers using the natural numbers. 
\begin{definition}
	Define the set $ \Q\coloneqq\{a/b\mid(a,b)\in\Z^2,\ b\neq0\} $ to be the \textit{\color{red}rational numbers}. Two rational numbers are equal, $ a/b=c/d $, if and only if $ ad=bc $.\footnote{Again we could use equivalence classes to define $ \Q $. The equivelence relation $ \sim $ would be defined on $ \N $ as $ (a,b)\sim(c,d) $ when $ ad=bc $.}  
\end{definition}
\subsection{``Holes'' in $ \Q $}
We now begin where the canonical \cite{rudin1964principles} opens. Our goal has been, and continues to be, to define the most comprehensive set of numbers possible. It may help to visualize what we have done so far with a number line. We can illustrate any ``gaps'' or ``holes'' by using red. This can be seen in Figure 1. 
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[thick, red, ->] (0,0) -- (14,0);
	\draw[thick] (7,.2) -- (7,-.2) node[anchor=north] {0};
	\draw[thick] (8,.2) -- (8,-.2) node[anchor=north] {1};
	\draw[thick] (9,.2) -- (9,-.2) node[anchor=north] {2};
	\draw[thick] (10,.2) -- (10,-.2) node[anchor=north] {3};
	\draw[thick] (11,.2) -- (11,-.2) node[anchor=north] {4};
	\draw[thick] (12,.2) -- (12,-.2) node[anchor=north] {5};
	\draw[thick] (13,.2) -- (13,-.2) node[anchor=north] {6};
	\draw (14,-.4) node {$ \N $};
		\draw[thick, red, <->] (0,-1) -- (14,-1);
		\draw[thick] (1,-.8) -- (1,-1.2) node[anchor=north] {-6};
		\draw[thick] (2,-.8) -- (2,-1.2) node[anchor=north] {-5};
		\draw[thick] (3,-.8) -- (3,-1.2) node[anchor=north] {-4};
		\draw[thick] (4,-.8) -- (4,-1.2) node[anchor=north] {-3};
		\draw[thick] (5,-.8) -- (5,-1.2) node[anchor=north] {-2};
		\draw[thick] (6,-.8) -- (6,-1.2) node[anchor=north] {-1};
	\draw[thick] (7,-.8) -- (7,-1.2) node[anchor=north] {0};
	\draw[thick] (8,-.8) -- (8,-1.2) node[anchor=north] {1};
	\draw[thick] (9,-.8) -- (9,-1.2) node[anchor=north] {2};
	\draw[thick] (10,-.8) -- (10,-1.2) node[anchor=north] {3};
	\draw[thick] (11,-.8) -- (11,-1.2) node[anchor=north] {4};
	\draw[thick] (12,-.8) -- (12,-1.2) node[anchor=north] {5};
	\draw[thick] (13,-.8) -- (13,-1.2) node[anchor=north] {6};
	\draw (14,-1.4) node {$ \Z $};
	\end{tikzpicture}
	\caption{The natural numbers and integers ordered number lines.}
\end{figure}
Clearly the natural numbers and integers are not ``comprehensive'' in that they have many gaps. This is what led us to define the rational numbers $ \Q $. It isn't immediate just how well the rationals do at covering the holes in the integers. We can get a sense of this by introducing a property of the rationals. 
\begin{proposition}{(Interspersing of integers by rationals)}
	For any $ x,y\in\Q $ where $ x<y $, there exists a third rational number $ z\in\Q $ such that $ x<z<y $.
\end{proposition}
\begin{proof}
	Let there be two rationals $ x,y\in\Q $ such that $ x<y $. We can define the third rational number of interest as $ z=(x+y)/2 $. We can show that $ x<z<y $ by using arithmetic.
	\begin{align*}
		x&<y\\\frac{x}{2}&<\frac{y}{2}\\\frac{x}{2}+\frac{y}{2}&<\frac{y}{2}+\frac{y}{2}\\z&<y
	\end{align*} 
	And we can arrive at $ x<z $ by adding $ x/2 $ to each side of the given inequality.
	\begin{align*}
	x&<y\\\frac{x}{2}&<\frac{y}{2}\\\frac{x}{2}+\frac{x}{2}&<\frac{y}{2}+\frac{x}{2}\\x&<z
	\end{align*} 
\end{proof}
\begin{example}

Take the rational numbers $ 0 $ and $ 1 $. Using the construction given in the previous proof we have $$ \frac{0+1}{2}=\frac{1}{2}$$ is between 0 and 1. We can now repeat this process using the pairs $ (0,1/2) $ and $ (1/2,1) $. \begin{align*}
	\frac{0+1/2}{2}=\frac{1}{4}\\\frac{1/2+1}{2}=\frac{3}{4}
\end{align*}
We could repeat this process an infinite number of times, in effect ``filling in'' gaps in $ \Z $ by successively taking the average of two rational numbers. Figure 2 shows this process on the unit interval in the rationals. 
\begin{figure}[h]
	\centering
\begin{tikzpicture}
\draw[thick, red, <->] (-1,0) -- (11,0);
\draw[thick] (0,.5) -- (0,-.5) node[anchor=north] {0};
\draw[thick] (5,.4) -- (5,-.4) node[anchor=north] {$ \frac{1}{2} $};
\draw[thick] (2.5,.3) -- (2.5,-.3) node[anchor=north] {$ \frac{1}{4} $};
\draw[thick] (7.5,.3) -- (7.5,-.3) node[anchor=north] {$ \frac{3}{4} $};
\draw[thick] (8.75,.2) -- (8.75,-.2) node[anchor=north] {$ \frac{7}{8} $};
\draw[thick] (1.25,.2) -- (1.25,-.2) node[anchor=north] {$ \frac{1}{8} $};
\draw[thick] (6.25,.2) -- (6.25,-.2) node[anchor=north] {$ \frac{5}{8} $};
\draw[thick] (3.75,.2) -- (3.75,-.2) node[anchor=north] {$ \frac{3}{8} $};
\draw[thick] (10,.5) -- (10,-.5) node[anchor=north] {1};
\draw[thick] (0.15625,.1) -- (0.15625,-.1);
\draw[thick] (0.3125,.1) -- (0.3125,-.1);
\draw[thick] (0.625,.1) -- (0.625,-.1);
\draw[thick] (0.46875,.1) -- (0.46875,-.1);
\draw[thick] (0.78125,.1) -- (0.78125,-.1);
\draw[thick] (0.9375,.1) -- (0.9375,-.1);
\draw[thick] (1.09375,.1) -- (1.09375,-.1);
\draw[thick] (1.40625,.1) -- (1.40625,-.1);
\draw[thick] (1.5625,.1) -- (1.5625,-.1);
\draw[thick] (1.71875,.1) -- (1.71875,-.1);
\draw[thick] (1.875,.1) -- (1.875,-.1);
\draw[thick] (2.03125,.1) -- (2.03125,-.1);
\draw[thick] (2.1875,.1) -- (2.1875,-.1);
\draw[thick] (2.34375,.1) -- (2.34375,-.1);
\draw (11,-.4) node {$ \Q $};

\end{tikzpicture}
\caption{}
\end{figure}\\
The key question is whether or not this fills \textit{all} the gaps in the integers.
\end{example}
While , and more formally , may lead us to believe that the rational numbers have no gaps, this is unfortunately not the case. There are two classic examples that arise from two of the most basic geometric constructions. 
\begin{example}
	Suppose we have a circle with diameter $ d $ and circumference $ c $. In this case, the ratio give by $ c/d $ is not an element of the rational numbers. This familiar ratio is written as $ \pi $. For the moment, we can take this as fact. We have not yet developed the tools required to proof that $ \pi\notin\Q $, but we will return to this.  
\end{example}
\begin{example}
	Suppose there is an isosceles right triangle with legs of length 1, as shown in Figure 3. We want to find the length of the hypotenuse $ x $. 
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\draw[thick] (0,0) -- (8,0);
		\draw[thick] (0,0) -- (0,8);
		\draw[thick] (8,0)--(0,8);
		\draw[thick] (0,0) rectangle (.5,.5);
		\draw[thick] (4,.2)--(4,-.2)node[anchor=north]{1};
		\draw[thick] (.2,4)--(-.2,4)node[anchor=east] {1};
		\draw (4.3,4.3) node {$ x $};
		\end{tikzpicture}
		\caption{}
	\end{figure}\\
This is a simple application of the Pythagorean Theorem. \begin{align*}
	1^2+1^2&=x^2\\2&=x^2
\end{align*}
But this equation has no rational solution, something we can formally prove. 
\end{example}
\begin{proposition}
	There exists no rational number $ x $ which satisfies $ x^2=2 $. 
\end{proposition}
\begin{proof}
	For the sake of contradiction, suppose that there exists a rational $ x $ which satisfies $ x^2=2 $. If this were the case, we could write $ x=m/n $ for some $ m,n\in\Z $, where $ m $ and $ n $ are not both even.\footnote{Otherwise we could write $ x $ in simpler terms as $ m $ and $ n $ would have a common factor of 2.}
\end{proof}
Any $ x $ which does satisfy $ x^2=2 $ would be \textit{irrational}, in that it is not an element of $ \Q $. 
\begin{definition}
	A number is {\color{red}\textit{irrational}} if it is not an element of $ \Q $. 
\end{definition}
\noindent There are \textit{many} irrational numbers, each of which is a gap in the rationals (see Figure 4). 
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[thick] (7,.2) -- (7,-.2) node[anchor=north] {0};
	\draw[thick, black, <->] (0,0) -- (14,0);
	\draw (14,-.4) node {$ \Q $};
	\draw[very thick, red ] (10.1415,.2) -- (10.1415,-.2) node[anchor=north] {\color{red}$ \pi $};
	\draw[very thick, red ] (8.4142,.2) -- (8.4142,-.2) node[anchor=north] {\color{red}$ \sqrt{2} $};
	\draw[very thick, red ] (5.5858,.2) -- (5.5858,-.2) node[anchor=north] {\color{red}$ -\sqrt{2} $};
	\draw[very thick, red ] (9.7182,.2) -- (9.7182,-.2) node[anchor=north] {\color{red}$ e $};
		\draw[very thick, red ] (12.403,.2) -- (12.403,-.2) node[anchor=north] {\color{red}$ 10\cos(1) $};
	\end{tikzpicture}
			\caption{}
\end{figure}\\
Our goal now becomes defining a set of numbers that includes not only the rationals, but also all of the irrationals. We began with the natural numbers, and then defined a set $ \Z $ which included the additive inverses of the natural numbers. Then we filled more of the gaps in the integers by taking the ratios of integers. We are now faced with the task of defining a set which eliminates the gaps caused by irrational numbers, and doing so eniterly with the set $ \Q $. 
\subsection{$ \sup $ and $ \inf $}
Before informally constructing the real numbers, it is worth thinking about why $ \Q $ has these ``holes'', and how it relates to a specific property of sets. It goes without saying that, all the sets of numbers we've discussed up until now have some ordering to them. We can make this formal by defining an ordered set.
\begin{definition}
	An \textit{\color{red}ordered set} is some set $ S $ with a binary relation, denoted by $ < $,\footnote{In this case, ``<'' can mean \textit{any} order. It just so happens that we use the same symbol as the familiar ``less than'' order, because it is the canonical example of such a relation.} which satisfies the following properties:
	\begin{enumerate}
		\item If $ x,y\in S $, then exactly one of the statements $$x<y,\ x=y,\ y<x $$ is true.
		\item If $ x,y,z\in S $, and both $ x<y $ and $ y<z $, then $ x<z $. 
	\end{enumerate}
\end{definition}  
The statement $ ``x<y'' $ is read as ``$ x  $ is less than $ y $.''We could also write $ y>x $ instead of $ x<y $. If we were to negative $ y<x $ (``$ y $ is not less than $ x $''), we would arrive at ``y is either greater than $ x $ or equal to $ x $.'' This is denoted as $ y\ge x $. 
\begin{example}
	The set $ \Q $ is a well ordered set if we define $ < $ in the following way for $ x,y\in\Q $:
	$$ x<y\coloneqq y-x \text{ is a postiive rational number}.   $$ Note that we can only relate objects that belong to $ \Q $. This means that we have no way of comparing rational numbers and the solution to the equation $ x^2=2 $.  
\end{example}
We can use the order relation on an ordered set to define bounds on sets.
\begin{definition}
	Suppose $ S $ is an ordered set, and $ E\subseteq S $. If there exists a $ \beta\in S $ such that $ x\le\beta $ for all $ x\in E $, $ E $ is \textit{\color{red}bounded above}, and $ \beta $ is an \textit{\color{red} upper bound} of $ E $. 
\end{definition}
\begin{definition}
	Suppose $ S $ is an ordered set, and $ E\subseteq S $. If there exists a $ \beta\in S $ such that $ x\ge\beta $ for all $ x\in E $, $ E $ is \textit{\color{red}bounded below}, and $ \beta $ is a \textit{\color{red} lower bound} of $ E $. 
\end{definition}
A subtlety in both definitions that is extremely important, is that upper and lower bounds must be elements of the ordered set $ S $. The next example highlights this. 
\begin{example}
 Take $ \Z $ to be an ordered set with the natural order. Pick the subset $ E=\{-2,-1,2\}\subseteq \Z $. This set has many upper and lower bounds. For upper bounds we have $ 2,3,4,\ldots $. For lower bounds we have $ -2,-3,-4,\ldots
  $. It may be tempting to say that a fraction such as $ 5/2 $ is an upper bound of $ E $, but it is not. This follows from the fact that $ 5/2\notin \Z $, so we have no means of relating it to elements in $ \Z $. In this particular case, there are upper and lower bounds of the set are included in the set. This need not be the case, as the next example shows.  
\end{example}
\begin{example}
	Let's look at the ordered set $ \Q $, and subset $ E=\{x\in\Q\mid 0<x<1\}\subseteq \Q $.\footnote{The use of the familiar interval notation of $ (0,1) $ will be properly defined and restricted to the real numbers in the following section.} In this case, each element of $ \{x\in\Q\mid x\le 0\} $ is a lower bound of $ E $, and  each element of $ \{x\in\Q\mid x\ge 1\} $ is an upper bound of $ E $. Even though $ 0,1\notin E $, they are still least and upper bounds of $ E $ respectively. 
\end{example}
\begin{remark}
	It is often obvious what exact order we are talking about when referring to an ordered set, like in the case of $ \Q $ and $ \Z $. In these cases, we'll just assume we're using the natural order.  
\end{remark}
We now will introduce two definitions that correspond to a special type of upper and lower bound.
\begin{definition}
	Suppose $ S $ is an ordered set, $ E\subseteq S $, and $ E $ is bounded above. We say that $ \alpha $ is a \textit{\color{red}leas- upper-bound} of $ E $ if:
	\begin{enumerate}
		\item $ \alpha $ is an upper bound of $ E $.
		\item If $ \gamma<\alpha $, then $ \gamma $ is not an upper bound of $ E $.
	\end{enumerate}
Alternatively, we can refer to $ \alpha $ as the \textit{\color{red}supremum} of $ E $, and write $ \alpha=\sup E $
\end{definition}
\begin{definition}
	Suppose $ S $ is an ordered set, $ E\subseteq S $, and $ E $ is bounded below. We say that $ \alpha $ is a \textit{\color{red}greatest-lower -bound} of $ E $ if:
	\begin{enumerate}
		\item $ \alpha $ is a lower bound of $ E $.
		\item If $ \gamma>\alpha $, then $ \gamma $ is not a lower bound of $ E $.
	\end{enumerate}
	Alternatively, we can refer to $ \alpha $ as the \textit{\color{red}infimum} of $ E $, and write $ \alpha=\inf E $
\end{definition}
\begin{remark}
	Both definitions use the definite article \textit{the} before supremum and infimum. This is because they are unique. This is also implied by the use of the superlative \textit{least} and \textit{greatest}. Nevertheless, this is a result of the definition, and can be properly proven. 
\end{remark}

\begin{example}
	If we return to Example 2.7, where $ E=\{-2,-1,2\}\subseteq\Z $, we have $ \sup E=2 $ and $ \inf E=-2 $. 
\end{example}
\begin{example}
	In example 2.8, $ \inf E=0 $ and $ \sup E=1 $.
\end{example}
\begin{example}
	Sticking with the set $ \Q$, consider the subset $ E=\{x\in\Q\mid x^2\le 2\}\subseteq \Q $. This set has no supremum, because the number satisfying $ x^2=2 $ is not an element of $ \Q $ (as shown in Proposition 2.2). We will formally prove this fact shortly. 
\end{example}
It is no coincidence that a subset of $ \Q $ fails to have a supremum, because of one of the ``holes'' in $ \Q $. The following definition will help us formalize this relationship. 
\begin{definition}
	Let $ S $ be an ordered set. If for all $ E\subseteq S $, where $ E $ is nonempty and bounded from above, $ \sup E $ exists, then $ S $ has the \textit{\color{red}least-upper-bound} property. 
\end{definition}
The least-upper-bound property ensures that any nontrivial subset of an ordered set has a supremum in that ordered set. We could define an equivelent property known as the greatest-lower-bound property. The next two propositions serves as nice examples of the least-upper-bound property, or lack there of,  in action.
\begin{proposition}
	The set $ \Z $ has the least-upper-bound property.
\end{proposition}
The idea behind the following proof takes advantage of the fact that $ \Z $ is discrete. For some set $ E\subseteq \Z $, we can always just look at an upper bound of it, and keep subtracting $ 1 $ until the resulting number is in $ E $. Then we will have found our upper bound.
\begin{proof}
	We will show that an arbitrary nontrivial set $ E\subseteq \Z $ has a supremum. Let $ x\in E $, and $ \beta $ be an upper bound of $ E $. We know that $ \beta\ge x $ for all $ x\in E $ We can show that $ \sup E $ exists via induction on $ \beta-x $ for our arbitrary $ x\in E $. Our base case is when $ \beta-x=0 $. If this holds, then $ \beta\in E $, so $ \beta\in\Z $ and $ \sup E=\beta $. Now suppose that this statement holds when $ \beta-x=k $ for $ k\in\N $ (this is our induction hypothesis). It is either the case that $ \beta\in E $ or $ \beta\notin E $. If $ \beta\in E $, then $ \sup E=\beta $. If $ \beta \notin E $, then let $ \beta'=\beta -1 $. Then $ \beta' $ is an upper bound of $ E $, and $$\beta'-x=\beta-1-x=\beta-x-1=k+1=1=k .$$ By the induction hypothesis, $ \sup E $ exists.   
\end{proof}
\begin{proposition}
	The set $ \Q $ does not have the least-upper-bound property.
\end{proposition}
To prove this, we will first establish that $ 2 $ is an upper bound of the set defined in Example 2.11, and then show the set has no supremum via contradiction.
\begin{proof}
	If suffices to find a single subset of $ \Q $ which fails to have a supremum. Let that set be $ E=\{x\in\Q\mid x^2\le 2\} $. 
	\begin{enumerate}
		\item Suppose for contradiction that $ 2 $ is not an upper-bound of $ E $. Then there exists an $ x\in E $ such that $ x>2 $. This would imply that $ x^2>4 $, which contradicts the assumption that $ x \in E $. 
		\item Suppose for contradiction that $ E $ has a supremum, and that $ \sup E=\alpha $ for $ \alpha\in\Q $. Define a new rational number $ y\in \Q $ as 
		\begin{equation}
		y=\alpha-\frac{\alpha^2-2}{x+2}=\frac{2(\alpha+1)}{\alpha+2}.
		\end{equation}
		Squaring this and subtracting $ 2 $ gives 
		\begin{equation}
		y^2-2=\frac{4(\alpha+1)^2}{(\alpha+2)^2}-\frac{2(\alpha+2)^2}{(\alpha+2)^2}=\frac{2(\alpha^2-2)}{(\alpha+2)^2}.
		\end{equation} We can use $ y $ to reach a contradiction in each possible case, those being: $ \alpha^2<2 $, $ \alpha^2=2 $, $ \alpha^2>2 $.
		\begin{enumerate}
			\item Suppose that $ \alpha^2<2 $. This means that $ \alpha^2-2<0 $, so Equation (1) implies that $ y>\alpha $. At the same time, Equation (2) implies that $ y^2-2<0 $, which means $ y^2<2 $. This gives that $ y\in E $, despite the fact that $ \alpha<y $. This contradicts the fact that $ \alpha $ is an upper-bound of $ E $
			\item Suppose $ \alpha^2=2 $. We already know this cannot be the case by Proposition 2.2. 
			\item  Finally, assume that $ \alpha^2>2 $, giving $ \alpha^2-2=0 $. Equation (1)	implies $ y<\alpha $ while Equation (2) implies $ y^2-2>0 $, meaning $ y^2>2 $. This establishes $ y $ as an upper bound for $ E $, but $ y<\alpha $, which contradicts $ \sup E=\alpha $.  
	 	\end{enumerate}
	\end{enumerate}
\end{proof}
The ``holes'' in $ \Q $ are a result of $ \Q $ not having the least-upper-bound property. In order to perform calculus, we need a set that has this property, otherwise things like continuity and differentiation would not work. This property is not sufficient in and of itself though. If that were the case then we would have stopped extending out set of numbers at $ \Z $. We want a set of numbers as ``comprehensive'' as $ \Q $, but with the least-upper-bound property. It turns out, that this (and a whole lot more) is what we will get from the real numbers.  
\subsection{The Real Numbers}
We will now construct the real numbers using only $ \Q $. First, we will define the algebraic structure that the real numbers will take on.
\begin{definition}
	A \textit{\color{red}field} is a set $ F $ with two operations, addition and multiplication, which satisfy the following axioms for all $ x,y,z\in F $:
	\begin{enumerate}
		\item Axioms for addition:
		\begin{enumerate}
			\item $ x+y\in F $ (closed under addition)
			\item $ x+y=y+x $ (commutative)
			\item $ (x+y)+z=x+(y+z) $ (associative)
			\item There exists an element $ 0\in F $ such that $ 0+x=x $ (identity element)
			\item There exists an element $ -x\in F $ such that $ x+(-x)=0 $ (inverse element)
		\end{enumerate}
		\item Axioms for multiplication:
		\begin{enumerate}
			\item $ xy\in F $ (closed under multiplication)
			\item $ xy=yx $ (commutative)
			\item $ (xy)z=x(yz) $ (associative)
			\item There exists an element $ 1\in F $ such that $ 1x=x $ (identity element)
			\item If $ x\neq 0 $, there exists an element $ 1/x\in F $ such that $ x(1/x)=1 $ (inverse element)
		\end{enumerate}
		\item The distributive property:
		$$ x(y+z)=xy+xz$$
	\end{enumerate}
\end{definition}
The study of fields is its own entire subject in math, and lives within the discipline if abstract algebra. For more details about fields, see \cite{dummit2004abstract}. These axioms can be used to reach several familiar conclusions about arithmetic in fields, and can be found as formal propositions in \cite{rudin1964principles}. A more specific type of field is that which is also an ordered set.
\begin{definition}
	An \textit{\color{red} ordered field} is a field $ F $ such that
	 \begin{enumerate}
		\item $ x+y<x+z $ if $ x,y,z\in F $ and $ y<z $,
		\item $ xy>0 $ if $ x,y\in F $, $ x>0 $, and $ y>0 $. 
	\end{enumerate}
\end{definition}
\begin{example}
	The set $ \Q $ is an ordered field. 
\end{example}

Our goal is now to construct an ordered field which not only contains $ \Q $, but also has the least-upper-bound property. In order to do this we'll use the fact that $ \Q $ has ``holes'' in it. We'll form a pair of set $ (A,B) $ that partition $ \Q $ such that each of these partitions corresponds to a real number. 

\begin{definition}
	A \textit{\color{red}Dedekind cut} $ x=(A,B) $ is a pair of subsets $ A,B\subseteq \Q $ satisfying the following:
	\begin{enumerate}
		\item $ A\cup B=\Q $, $ A\cap B=\emptyset $, $ A\neq\emptyset $, and $ B\neq\emptyset $.
		\item If $ a\in A $ and $ b\in B $, then $ a<b $.
		\item $ A $ contains no largest element.
	\end{enumerate}
\end{definition} 
\begin{example}
	Let $ A=\{y\in \Q \mid y<0\} $ and $ B=\{y\in\Q\mid y\ge 0\} $. Our cut is $ x=(A,B) $, and can be seen in Figure 5.
		\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\draw[thick] (7,.2) -- (7,-.2) node[anchor=north] {0};
		\draw[thick, black, <->] (0,0) -- (14,0);
		\draw (14,-.4) node {$ \Q $};
		\draw[ very thick, blue, ->] (7,0) -- (1,0) node[anchor=south] {A};
		\filldraw[color=blue, fill=red, very thick] (7,0) circle (3pt);
		\draw[ very thick, red, ->] (7,0) -- (13,0) node[anchor=south] {B};
		
		\end{tikzpicture}
		\caption{Dedekind cut corresponding to $ 0\in\Q $.}
	\end{figure}
This cut uniquely represents $ 0\in\Q $, as no other cut can be defined in this way ``at'' $ 0 $. 
\end{example}
\begin{example}
	Perhaps a better example is the cut defined by $ A=\{q\in\Q\mid q\le 0\text{ or }q^2<2 \} $ and $ A=\{q\in\Q\mid q> 0\text{ or }q^2>2 \} $. This cut corresponds to the solution of the equation $ x^2=2 $.  
\end{example}
\begin{definition}
	A \textit{\color{red}real number} is a Dedekind cut in $ \Q $. The set of real numbers is denoted by $ \R $. 
\end{definition}
\begin{definition}
	A real number $ x=(A,B) $ is a \textit{\color{red}rational number} if $ B $ contains a smallest element (namely $ x $).
\end{definition}
\begin{definition}
	A real number $ x=(A,B) $ is a \textit{\color{red}irrational number} if $ B $ contains no smallest element.
\end{definition}
\begin{example}
	The cut defined by $ A=\{y\in \Q \mid y<0\} $ and $ B=\{y\in\Q\mid y\ge 0\} $ is rational, as $ B $ has a smallest element in the form of $ 0 $.  
\end{example}
\begin{example}
	The cut defined by $A=\{q\in\Q\mid q\le 0\text{ or }q^2<2 \} $ and $ A=\{q\in\Q\mid q> 0\text{ or }q^2>2 \} $ has no smallest element. Therefore it is an irrational number. We will denote this particular number as $ \sqrt{2} $. 
\end{example}
Now that we have properly defined $ \R $, we can \textit{finally} refer to the quantity $ \sqrt{2} $! It is no longer a mysterious solution to an equation, and is well defined in $ \R $. This is a relatively small payoff, but the real rewards are the two following theorems. These are the main results of this section, and are of the utmost importance. The first will allow us to perform operations on $ \R $, and the second will play a key role in proving familiar theorems from calculus. The proof of the first result is rather long, and not very informative, so it is omitted. It is important to understand \textit{how} it would be proved though. Before the big reveal, we will define an order on $ \R $
\begin{definition}
	Given real numbers $ x=(A,B) $ and $ y=(C,D) $, we define the following order:$$x\le y\coloneqq A\subseteq C .$$ The inequality is strict if $ A\subseteq C $. 
\end{definition}
\begin{example}
	Let $ x=2=(A,B)=(\{y\in \Q \mid y<2\},\{y\in\Q\mid y\ge 2\})$, and $ y=3=(C,D)=(\{z\in \Q \mid z<3\},\{z\in\Q\mid z\ge 3\})$. It should come as no surprise that $ 2<3 $, but this is because $ A\subseteq C $.  
\end{example}
\begin{theorem}
	The set $ \R $ is an ordered field containing $ \Q $.
\end{theorem}
\begin{proof}
	See the appendix of chapter 1 in \cite{rudin1964principles}. The idea is that addition and multiplication of cuts must be define, and the all the axioms of fields and ordered fields must be verified using the cut definition of a real number. 
\end{proof}
\begin{theorem}[Completeness of the real numbers]
	The set $ \R $ has the least-upper-bound property.
	\begin{proof}
	We will show an arbitrary nonempty subset of $ \R $ has a supremum. Let $ E\subseteq \R $, where $ A\neq\emptyset $, have the upper bound $ \beta\in\R $. We may write $ \beta $ as a Dedekind cut, $ \beta=(A,B) $. Additionally, we may express each $ \alpha\in E $ as a cut $ \alpha=(L_\alpha,U_\beta) $. Now we will construct a real number by taking the union of all $ L_\alpha $.
	$$ \gamma=\left(\bigcup_{\alpha\in E}L_\alpha,\Q\backslash \bigcup_{\alpha\in E}L_\alpha\right)=(L,\Q\backslash
	 L)=(L,U)$$  
	I claim that $ \sup E=\gamma $.
	
	First we must verify that $ \gamma\in\R $ by showing that $ (L,U) $ is a valid Dedekind cut, and satisfies the requirements of Definition 2.13: 
	\begin{enumerate}
		\item The set $ E $ is nonempty, so there exists at least one $ \alpha=(L_\alpha,U_\alpha)\in E $. Because $ L_\alpha\neq\emptyset $ and $ U_\alpha\neq\emptyset $ by definition 2.13, we have $ L\neq\emptyset $ and $ U\neq\emptyset $. By the definition of $ U $ as $ \Q\backslash L $, we have that $  L\cup U=\Q$ and $ L\cap U=\emptyset $. 
 		\item {\color{red}FINISH THIS} 
		\item  {\color{red}FINISH THIS} 
	\end{enumerate}
	\end{proof}
Therefore $ \gamma\in\R $. By construction, $ \alpha\le\gamma $ for all $ \alpha\in E $, making $ \gamma $ an upper bound. To show that it is the least-upper-bound, we will now show any number lesser than it cannot be an upper bound. Now suppose $ \delta <\gamma $. This means $  C\subseteq L $, where $ \delta $ is expressed as a cut $ \delta=(C,D) $. This means there exists some $ s\in L $ such that $ s\notin C $. But $ s\in L $, so it is in $ L_\alpha $ for some $ \alpha\in E $. Hence, $ C\subseteq L_\alpha $, giving $ \delta <\alpha $. This shows that $ \delta $ is not an upper bound, meaning $ \sup E =\gamma$.
\end{theorem}

\begin{example}
	Consider the set of real numbers $ E=\{-1,-1/2,-1/3,-1/4,\ldots\} $. What is the supremum of this set? Intuitively, it should be $ 0 $, but we can verify this by constructing it like we did in the previous proof. Each number in $ E $ corresponds to a cut $ (L_n,U_n) $, for $ L_n=\{x\in\Q\mid x<-1/n\} $ and $ U_n=\{x\in\Q\mid x\ge -1/n\} $, where $ n\in\N $. Our supremum is $$\gamma=\left(\bigcup_{n\in \N}\{x\in\Q\mid x<-1/n\},\Q\backslash \bigcup_{n\in \N}\{x\in\Q\mid x<-1/n\}\right)=(\{x\in\Q\mid x<0\},\{x\in\Q\mid x\ge0\}). $$ Therefore, $ \gamma=0 $.  
\end{example}
You will often here the real numbers referred to as ``complete'' because they have the least-upper-bound property. This is because the least-upper-bound property ensures there are no ``gaps'' in the real line like there are in $ \Q $. Theorem 1.2 may be the most important result in real analysis. Remember it well, as it will be used often. Most disciplines in math build on themselves over time, and the fact that $ \R $ has the least-upper-bound property will be our foundation. One could argue it is $ \R $'s defining property.  

Finally, we will adopt the familiar notation of intervals in $ \R $, and add make an important addition to $ \R $. 
\begin{definition}
	We will use the following notation to refer to \textit{\color{red} intervals} of $ \R $:
	\begin{align*}
		(a,b)=\{x\in\R\mid a<x<b\}\\
		[a,b)=\{x\in\R\mid a\le x<b\}\\
		(a,b]=\{x\in\R\mid a<x\le b\}\\
		[a,b]=\{x\in\R\mid a\le x\le b\}
	\end{align*}
	for $ a<b $. 
\end{definition}
\begin{definition}
	The \textit{\color{red}extended real number system} consists of the real field $ \R $ and two symbols: $ \infty $, and $ -\infty $. The original order of $ \R $ is preserved, and we define $$ -\infty <x<\infty$$ for all $ x\in\R $. We will denote the extended real numbers as $ \overline{\R} $.  
\end{definition}
The extended real numbers do not form a proper field, but we can adopt some conventions for arithmetic using $ \infty $ and $ -\infty $ for $ x\in\R $:
\begin{enumerate}
	\item $ x+\infty=\infty $, $ x-\infty=-\infty $, $ x/\infty=x/-\infty=0 $. 
	\item For $ x>0 $, $ x(\infty)=\infty $, and $ x(-\infty)=-\infty $.
	\item For $ x<0 $, $ x(\infty)=-\infty $, and $ x(-\infty)=\infty $.
\end{enumerate}
The addition of an upper and lower bound on $ \R $ make the set $ \overline{\R} $ easier to work with in certain situations.\footnote{What we are really doing is working with $ \overline{\R} $ because it is a complete lattice. A complete lattice is a partially ordered set in which every subset has an infimum or supremum. The real line is not a complete lattice, as any set of the form $ (a,\infty)\subset\R $ has no supremum.} The most immediate result of working in $ \overline{\R} $ is that \textit{every} subset of $ \R $ has a supremum, not just bounded ones (the latter case being the only one stipulated by the least-upper-bound property). 
\subsection{Properties of $ \R $}
The importance of Theorem 2.2 can not be understated. It is perhaps \textit{the} defining property of $ \R $, and it gives rise to numerous results in analysis. For now, we can use it to prove two additional properties of $ \R $.
\begin{theorem}[Archimedean property of $ \R $]
For $ x,y\in \R $ where $ x>0 $, there exists an $ n\in\N $ such that $ nx>y $.
\end{theorem}   
\begin{proof}
Let $ A $ be the set of all $ nx $ for $ x\in\R $ and $ n\in\N $, where $ x>0 $. For contradiction, suppose that there exists no such $ n\in\N $ such that $ nx>y $ for $ y\in\R $. This makes $ y $ an upper bound of $ A $. By the completeness of $ \R $, $ \sup A=\alpha $ exists. Since $ x>0 $, $ \alpha-x<\alpha $, and $ a-x $ is not an upper bound of $ A $. This means there exists an $ m\in\N $ such that $ a-x<mx $. But this would imply $ \alpha<mx+x=m(x+1) $, where $ (m+1)x\in A $. This contradicts the fact that $ \alpha $ is an upper bound of $ A $. 
\end{proof}
\begin{example}
	Suppose $ x=10 $ and $ y=213 $. By the Archimedean property of $ \R $, we know there exists a multiple of $ 10 $ that is greater than $ 213 $. $$ 10(22)=220>213$$  
\end{example}
\begin{theorem}[$ \Q $ is dense in $ \R $]
For $ x,y\in \R $ where $ x<y $, there exists a $ p\in\Q $ such that $ x<p<y $. 
\end{theorem}  
\begin{proof}
We have $ x<y $, giving $ y-x>0 $. By the Archimedean property (Theorem 2.3), there exists an $ n\in\N $ such that 
\begin{equation}
n(y-x)>1.
\end{equation}
We can use Theorem 2.3 to find $ m_1,m_2\in\N $ for which:
\begin{align*}
	m_1>nx,\\m_2>-nx.
\end{align*}
We can combine these two inequalities to conclude $ -m_2<nx<m_1 $. This implies there exists an $ m\in\N $ (with $ -m_2\le m\le m_1 $) such that \begin{equation}
m-1\le nx<m.
\end{equation}  
If we combine $ (3) $ and $ (4) $ we get $$ nx<m\le 1+nx<ny.$$ Dividing by $ n $ (which is positive) gives $ x<\frac{m}{n}<y$. 
\end{proof}
The density of $ \Q $ in $ \R $ is both surprising and useful for constructing examples. In practice, it means that every irrational number has a rational number arbitrarily close to it. We can approximate any number in $ \R $ arbitrarily well with a rational number.
\begin{example}
We will now mimic the proof of Theorem 2.4 with actual numbers. We will find a rational $ p\in\Q $ such that $ e<p<\pi $.\footnote{You shouldn't even need to perform the construction to arrive at an answer, as $ e $ and $ \pi $ are not particularly ``close'' to each other. We could for instance take $ p=3 $. } We have $ \pi-e>0 $. We know $$ 3(\pi-e)>1 .$$ Next we pick whole numbers $ m_1=9 $ and $ m_2=8 $, and get the inequality $ 8<3e<9 $. Now take $ m $ to be $ 9 $ and reach our final inequality of $$ 3e<9<1+3e <3\pi.$$ Dividing by $ n=3 $ gives our desired rational number is $ 9/3=3 $.  
\end{example}
\begin{example}
	Let $ \sqrt{2}\in\R $, and let $ \varepsilon>0 $.\footnote{This is the first time we're using the infamous $ \varepsilon $. It just stands in for any arbitrarily small positive number.} By the density of $ \Q $ in $ \R $, there exists $ p\in\Q $ such that $ \sqrt{2}-\varepsilon<p<\sqrt{2} $. This will hold \textit{for all} $ \varepsilon>0 $ so as we let $ \varepsilon $ become smaller and smaller, we will have an increasingly accurate rational approximation of $ \sqrt{2} $.
\end{example}     
\subsection{Cardinality}
So far, I've been intentional in avoiding any discussion of the size of the sets we have been working with. When constructing $ \Z $ from $ \N $, it was never stated that $ \Z $ was somehow ``bigger'' than $ \N $. All we know is that $ \Z $ has elements that $ \N $ does not. The same can be said for $ \Z $ and $ \Q $, or $ \Q $ and $ \R $. It is now time that we turn our attention to this matter, and more generally the size, or ``cardinality'' of sets. 

Determining the size of a set amounts to counting the number of elements in that set. But how do we make the notion of counting formal? We will do this with functions. Before formally defining anything, consider how you may count something. If you are tasked with counting the number of elements in the set $ X=\{a,b,c\} $, you're answer will surely by 3. How did you get that number? You said assigned the number $ 1 $ to $ a $, $ 2 $ to $ b $, and $ 3 $ to $ c $. We should note three different things about this process:
\begin{enumerate}
	\item Each number we use is from $ \N $.
	\item Each element of $ X $ is assigned a number. We wouldn't have counted properly if we skipped some element. 
	\item No number in $ \N $ is assigned to multiple elements in $ X $. We do not want to count multiple elements as a single element.
\end{enumerate}
This process of assigning elements in $ \N $ to those in $ X $ is shockingly similar to the notion of a function, as we are mapping elements from one set to those in another set. Furthermore, the properties we must obey while counting have their own analogous forms with functions: surjectivity, and injectivity. For this reason, we will use functions to formalize the size of a set.

First, we will address the abstract case of when two sets have the same number of elements, and then we will transition to the size of sets.
\begin{definition}
	The \textit{\color{red}cardinality} of a set $ X $, denoted $ |X| $, is the number of elements that belong to the set.   
\end{definition} 
\begin{definition}
	Two sets $ X $ and $ Y $ have \textit{\color{red}the same cardinal number} if there exists a bijection $ f:X\to Y $ from $ X $ to $ Y $.  
\end{definition} 
\begin{proposition}
	Define the relation $ X\sim Y $ \textit{if and only if} $ X $ and $ Y $ have the same cardinal number ($ |X|=|Y| $). The relation $ \sim $ is an equivalence relation. 
\end{proposition} 
\begin{proof}
	We have that $ X\sim X $ by letting $ f:X\to X $ be $ f(x)=x $, so $ \sim $ is reflexive. If $ X\sim Y $, there exists a bijection $ f:X\to Y $. Since $ f $ is a bijection, it has an inverse $ f^{-1}:Y\to X $. This inverse is itself a bijection, so $ Y\sim X $, making $ \sim $ symmetric. Lastly, assume $ X\sim Y $ and $ Y\sim Z $. We have bijections $ f:X\to Y $ and $ g:Y\to Z $. The composition of two bijections is a bijection, so $ h:X\to Z $ is a bijection where $ h:X\to Z $. This makes $ \sim $ transitive.  
\end{proof}
\begin{example}
	Let $ X=\{1,2,3\} $ and $ Y=\{\sqrt{2},e,\pi\} $. We can define $ f:X\to Y $ as $$ f(x)=\begin{cases}
	\sqrt{2}\text{ if }x=1\\e\text{ if }x=2\\\pi\text{ if }x=3
	\end{cases}.$$
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}
		\draw (0,0) ellipse (0.7cm and 1.5cm);
		\draw (3,0) ellipse (0.7cm and 1.5cm);
			\filldraw (0,0) circle (2pt) node[anchor=north east] {$ 2 $};
			\filldraw (0,1) circle (2pt) node[anchor=north east] {$ 1 $};
			\filldraw (0,-1) circle (2pt) node[anchor=north east] {$ 3 $};
			\filldraw (3,0) circle (2pt) node[anchor=north west] {$ e $};
			\filldraw (3,1) circle (2pt) node[anchor=north west] {$ \sqrt{2} $};
			\filldraw (3,-1) circle (2pt) node[anchor=north west] {$ \pi $};
		
			\draw (0,-2) node {$ X $};
			\draw (3,-2) node {$ Y $};
			\draw[->] (0.2,0) -- (2.8,0) ;
			\draw[->] (0.2,1) -- (2.8,1) ;
			\draw[->] (0.2,-1) -- (2.8,-1) ;
		\end{tikzpicture}
		\caption{Bijection $ f:X\to Y $.}
	\end{figure}  \\
This function is clearly a bijection, and we have that $ |X|=|Y| $.
\end{example} 
\begin{example}
	Let $ \Z^-=\{x\in\Z\mid x<0\} $ and $ \Z^+=\{x\in\Z\mid x>0\} $. There exists a very natural bijection between these sets, namely that which maps each element in $ \Z^+ $ to it's negative counterpart in $ \Z^- $. Formally, $ f:\Z^+\to\Z^- $ is define as $ f(x)=-x $. This function is clearly a bijection, and its existence shows that $ |Z^+|=|Z^-| $. There are the same number of positive integers as negative integers  
\end{example}
\begin{remark}
	Because $ f:X\to Y $ is a bijection, it doesn't matter which set is the domain and which set is the codomain. A function is invertible if and only if it is a bijection, and a functions inverse is a bijection, so we would just have $ f^{-1}:Y\to X $ if we picked the sets in the other order. In example 2.23, we could have instead assigned each negative integer to it's positive counterpart and had $ f:\Z^-\to\Z^+ $. In this case we would still have $ f(x)=-x $, as this particular function is its own inverse! 
\end{remark}

As discussed earlier, counting is intrinsically linked to the set of natural numbers $ \N $. We will now make this formal by defining three types of sets: finite sets, countably infinite sets, and uncountably infinite sets.
\begin{definition}
	A set $ X $ is \textit{\color{red}finite} if there exists a subset of the whole numbers $ N\subseteq \N $ for which $ X $ and $ N $ have the same cardinal number.
\end{definition} 
\begin{definition}
	A set $ X $ is \textit{\color{red}countably infinite} if $ X $ has the same cardinal number as $ \N $. Alternatively, $ X $ is countably infinite if there exists a bijection $ f:X\to\N $. This is sometimes denoted as $ |X|=\aleph_0 $.\footnote{This symbol is an ``aleph'', and is the first letter of the Hebrew alphabet.} 
\end{definition}
\begin{definition}
	A set $ X $ is \textit{\color{red}countable} if it is countably infinite or finite.
\end{definition}
\begin{remark}
	From here on out, I'm going to use countable and countably infinite interchangeably, as nearly all the sets we are interested in are infinite.  
\end{remark}
\begin{definition}
	A set $ X $ is  \textit{\color{red}uncountably infinite} (or uncountable) if it is neither finite nor countably infinite.
\end{definition}
Before jumping into examples, let's unpack some of this. Finiteness and countable infiniteness depend on whether a set has the same cardinal number as a subset of $ \N $ or $ \N $ itself. This means we can find a bijection between the set and a subset of $ \N $ or $ \N $ itself. Definition 2.22 and 2.23 are often presented in terms of this hypothetical bijection. Secondly, two of these definitions involve infinity. A set can be either countably infinite or uncountably infinite. In a sense, some infinite sets have so many elements that they cannot even be counted, and are ``bigger'' than other uncountable sets! These two concepts of infinity will show up constantly in real analysis. Hopefully examples will make this clear. Some of the following examples are so important that they will be presented as formal results. 
\begin{example}
 We will modify Example 2.22. Let Let $ X=\{\sqrt{2},e,\pi\} $ and $ N=\{1,2,3\}\subseteq \N$. Take our bijection to be the inverse of the function defined previously in Example 2.22. This shows that $ X $ is finite, and $ |X|=3 $. 
\end{example}
\begin{proposition}
	The set $ \N $ is countably infinite.
\end{proposition}
\begin{proof}
	Define $ f:\N\to\N $ as $ f(n)=n $. This function is clearly a bijection. 
\end{proof}
\begin{proposition}
	The set $ \Z $ is countably infinite.
\end{proposition}
Before proving this result, it's worth acknowledging that it seem paradoxical. How can it be that $ \Z $ is the same size as $ \N $, despite $ \Z $ being defined as $ \N $ plus more elements? It would make more sense for $ \Z $ to have twice the cardinality as $ \N $, but this is not the case. The result may sit better if you consider just how we would count $ \Z $. If you started at $ 1\in\Z $, followed by $ 2\in\Z $, etc. then you would miss all the negative numbers! Instead, we need to be clever in the order in which we count $ \Z $. We will instead count in the following order: $ 0,1,-1,2,-2,\ldots $. We could count like this forever and never run out of $ \N $, and never miss any elements of $ \Z $. This is why we have $ |\N|=|\Z| $. 
\begin{proof}
Recall that we can select either $ \Z $ or $ \N $ to be the domain of our bijection. We will define $ f:\N\to\Z $ as
$$f(n)=\begin{cases}
\frac{n}{2}\text{ if }n\text{ is even}\\
-\frac{n-1}{2}\text{ if }n\text{ is odd}
\end{cases}.$$
This function counts $ \Z $ in the aforementioned manner of alternating between positive and negative integers. We now will verify that $ f $ is a bijection, by showing it is injective and surjective.\footnote{It would be quicker to show $ f $ has an inverse, but that approach is not as instructive.} Let $ y\in\Z $, and pick $ x\in\N $ such that $ x=2y $ if $ y $ is even, and $ x=-2y+1 $ if $ y $ is odd. This choice of $ x\in\N $ gives $ f(x)=y $, so $ f $ is surjective. Now suppose that $ f(x_1)=f(x_2) $. If $ x_1/2=x_2/2 $, then $ x_1=x_2 $. If $ -(x_1-1)/2=-(x_2-1)/2 $, then $ x_1=x_2 $. Therefore, $ f $ is injective.
\end{proof}
An even more surprising result is that not only $ |\N|=|\Q| $, but also $ |\N|=|\Z|=|\Q| $! Even if we add every possible fraction to the integers, the size of our set remains the same.
\begin{theorem}
	The set $ \Q $ is countably infinite. 
\end{theorem}

\begin{proof}
	We can enumerate the rational numbers in the following way: $$ \frac{0}{1},\frac{1}{1},\frac{-1}{1},\frac{1}{2}, \frac{1}{3}, \frac{2}{1}, \frac{-2}{1}, \frac{-1}{3}, \frac{1}{4}, \frac{1}{5}, \frac{-1}{4}, \frac{2}{3},\ldots $$ This particular ordering can be seen in Figure 7. The red arrows in Figure 7 show the order in which we count, and it becomes evident that we will eventually count every possible fraction. Note that we only count fractions which are expressed in simplest terms, with others in gray. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/Qcountable}
	\caption{}
	\label{fig:qcountable}
\end{figure}
\end{proof}
It may not be a surprise that it is not possible to count $ \R $. This puts $ \R $ in our second category of infinite sets, uncountably infinite.
\begin{theorem}
	The set $ \R $ is uncountably infinite. 
\end{theorem}
The proof of this theorem is a classic, and is due to Cantor. 
\begin{proof}
Suppose for contradiction that $ \R $ were countably infinite. There exists a bijection $ f:\N\to\R $, and we make a table of values the function takes on (see Table 1). Table 1 is just an example of what such a bijection may look like, and the exact values are moot.
\begin{table}[h!]
	\centering
	\begin{tabular}{cc}
		$ n\in\N  $       & $ f(n)\in\R $              \\ \hline
		1        & $4.3214875\ldots$ \\
		2        & $1.4918401\ldots$           \\
		3        & $3.0194510\ldots$                   \\
		4        & $9.0194510\ldots$                  \\
		5        & $0.3917293\ldots $               \\
		6        & $ 5.9184017\ldots $\\
		7        & $ 1.9284010\ldots $                \\
		$\vdots$ & $\vdots$         
	\end{tabular}
\end{table}
Because $ f $ is a bijection, this table should go on forever, and count every element of $ \R $. To reach a contradiction, we will simply show there exists a real number that was not counted.\footnote{There in fact exist \textit{many} that would go uncounted, but it suffices to find just one.} ``Construct'' this uncounted real number in the following way: let the $ n^{th} $ digit (decimal places included) take on the value of the $ n^{th}$ digit of $ f(n) $ minus one (if it is $ 0 $, set it to $ 9 $). For Table 1, the first digit would be the first digit of $ f(1) $ minus 1, which is $ 4-1=3 $. The second digit would be the second digit of $ f(2) $ minus 1, which is $4-1=3$. We repeat this process for all $ n\in\N $, and in our case we get $$3.308690\ldots $$
By construction, the $ n^{th} $ digit of this number is different from at least one of the $ n^{th} $ digits of $ f(n) $. This holds for every $ n\in\N $, so this number is different from every value of $ f(n) $, and was therefore not counted.  
\end{proof}
\begin{corollary}
	Every infinite subset of $ \R $ is uncountable.
\end{corollary}
\begin{example}
	Every interval $ [a,b]\subseteq\R $ is uncountable. 
\end{example}
Now that we've seen which familiar sets are and are not countable, there are several key results involving the cardinality of sets that deserve attention. These will establish what happens to the cardinality of sets when different set operations are performed. 
\begin{proposition}
	Let $ \{E_n\} $, $ n\in\N $, be a sequence of countably infinite sets, and let $ E=\cup_{n\in N}E_n $ be a countable union. The set $ E $ is countably infinite. 
\end{proposition}
\begin{proposition}
	We will prove via induction. Our base case is $ n=2 $. The sets $ E_1 $ and $ E_2 $ are countably infinite, so there exists bijections $ f:\N\to E_1 $ and $ g:\N\to E_2 $. Without loss of generality, assume $ E_1\cap E_2=\emptyset $.\footnote{Otherwise, we could replace $ E_1 $ with $ E_1\backslash E_2 $.} Define $ h:\N\to E_1\cup E_2 $ as $$ h(k)=\begin{cases}
	f(k/2)\text{ if }k  \text{ is even}\\
		g((k+1)/2)\text{ if }k  \text{ is odd}
	\end{cases}. $$ This function counts the elements in the set by alternating between those in $ E_1 $ and $ E_2 $ (like in the proof of Proposition 2.7). The function $ h $ is a bijection, so $ E_1\cup E_2 $ is countably infinite. Now suppose this holds for $ E_1,\ldots E_{n-1} $. We can write $ E $ as a union of two countably infinite sets by taking the union over $ E_1,\ldots E_{n-1} $, which is countably infinite by the induction hypothesis. 
	\begin{align*}
		E&=\bigcup_{n\in \N} E_n\\
		 &=E_1\cup E_2\cup\cdots E_{n-1}\cup E_n\\
		 &=(E_1\cup E_2\cup\cdots E_{n-1})\cup E_n
	\end{align*} 
	Therefore $ E $ is countably infinite. 
\end{proposition}
\begin{corollary}
	If $ X $ is uncountable, and $ E\subseteq X $ is countably infinite, then $ X\backslash E$ is uncountably infinite.
\end{corollary}
\begin{example}
	Let $ E_n=\{m/n\mid m\in\Z\} $ for $ n\in\N $. Each $ E_n $ is countable as there is a bijection $ f_n:E_n\to\Z $ defined as $ f_n(x)=nx $, and $ \Z $ is countably infinite.\footnote{This allows us to use the transitivity of sets having the same cardinality.} Note that $$\bigcup_{n\in \N}E_n=\Q, $$ which is indeed countably infinite. 
\end{example}
\begin{example}
	The set $ \R $ is uncountable. We have that $ \Q\subseteq \R $ is countable. By Corollary 2.1, $ \R\backslash \Q $ (the set of irrational numbers) is uncountable. This means that in a certain sense, there are more gaps in $ \Q $ than there aren't! We are not even capable of counting all the gaps, whereas we can count $ \Q $. 
\end{example}
\begin{proposition}
	Let $ X $ be a countable set. Any subset $ Y\subseteq X $ is countable
\end{proposition}
\begin{proof}
	There exists a bijection $ f:\N\to X $. If we restrict the codomain of $ f $ to be $ Y $, $ f $ is still a bijection. 
\end{proof}
\begin{example}
	Every subset of $ \Q $ is countable, because $ \Q $ is countable. We already know two such examples: $ \N $ and $ \Z $. 
\end{example}
\begin{proposition}
	Let $ \{E_n\} $, $ n=1,\ldots,m $, be a finite sequence of countably sets, and let $ E=\times_{n=1}^mE_n $ be a countable Cartesian product. The set $ E $ is countably infinite. 
\end{proposition}
\begin{proof}
	It suffices to show the result for two sets $ E_1 $ and $ E_2 $, and then apply induction using the same argument used in the proof of Proposition 2.9. We have bijections $ f:E_1\to\N$ and $ g:E_2\to\N $. Define $ h:E_1\times E_2 $ as $$ h((a,b))=2^{f(a)}3^{g(a)},$$ where $ (a,b)\in E_1\times E_2 $. Each element in $ h(E_1\times E_2) $ is a whole number with a prime factorization comprised of only $ 2 $ and/or $ 3 $. Because each element of $ \N $ is uniquely determined by its prime factorization, $ h $ is injective. Unfortunately, $ h $ is no surjective, as there exist many elements of $ \N $ with prime factorizations that include more than $ 2 $ and/or $ 3 $. If we restrict the codomain of $ h $ to just its image, we have a bijection $ h':E_1\times E_2\to  h(E_1\times E_2) $  We do have that $ h(E_1\times E_2)\subseteq \N  $, so by Proposition 2.10, $ |h(E_1\times E_2)|=\aleph_0 $ . By transitivity, $ |E_1\times E_2|=\aleph_0 $. The aforementioned induction can be applied to conclude $ |E|=\aleph_0 $. 
\end{proof}
\begin{example}
	The set of all pairs of rational numbers $ \Q^2 $ is countable.
\end{example}
\subsection{Exercises}
\begin{ex}
	Show that $ \sqrt{3} $ is irrational.
\end{ex}
\begin{ex}
	Let $ (S,<) $ be an ordered set, and $ T $ be a nonempty subset of $ S $. Verify that $ T $ has at most one supremum.
\end{ex}
\begin{ex}
	Let $ (S,<) $ be an ordered set,  and $ A $ and $ B $ be nonempty subsets of $ T $. Show that if $ A\subset B $, then $ \sup A\le B $ and $ \inf B\le\inf A $.
\end{ex}
\begin{ex}
	Let $ E $ be a nonempty subset of an ordered set; suppose $ \alpha $ is a lower bound of $ E $ and $ \beta $ is an upper bound of $ E $. Prove that $ \alpha\le\beta $. 
\end{ex}
\begin{ex}
	Let $ E $ be a nonempty subset of $ \R $ which is bounded below. Let $ -E=\{-x\mid x\in E\} $. Show that $ \inf E=-\sup(-E) $. 
\end{ex}
\begin{ex}
	A set has the least-upper-bound property \textit{if and only if} it has the greatest-lower-bound property. 
\end{ex}
\newpage

\section{Point-Set Topology in Metric Spaces}
One of the main goals of calculus is to study rates of change and limiting behavior. Both of these concepts require some notion of distance, and to that end we will study \textit{metric spaces}, sets equipped with a distance function. Any such space has induced ``topological'' properties. This is a fancy way of saying that we can use distance to categorize different types of sets. Of particular interest, will be the different types of sets in $ \R $ and $ \R^n $, as the properties these sets have will have major implications down the road.  
\subsection{Metric Spaces}
Our first definition will outline how we endow a set with some notion of distance. 
\begin{definition}
	A \textit{{\color{red}metric space}} is an ordered pair $ (M,d) $ where $ M $ is a set and $ d:M\times M\to[0,\infty] $ is a function which satisfies:
	\begin{enumerate}
		\item $ d(x,y)=0 $ $ \iff $ $ x=y$.
		\item $ d(x,y)=d(y,x) $ for all $ x,y\in M $ where $ x\neq y $.
		\item $ d(x,z)\le d(x,y)+d(y,z) $ for all $ x,y,z\in M $. (Triangle Inequality)
	\end{enumerate}
\end{definition}
The function $ d $ is often called the metric, and most of its properties are compatible with our everyday understanding of distance. Firstly, distance cannot be negative. There is no distance between a point and itself. The distance from $ x $ to $ y $ is the same from $ y $ to $ x $. The final property may not be as immediate, but it is extremely important.
 
Suppose you are traveling from point $ x $ to $ z $. If you decide to take a detour to point $ y $ before heading to $ z $, then the triangle inequality ensures that you travel a weakly greater distance. An illustration shown in Figure 8 of this gives rise to the inequalities name.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		\draw (0,0) node[anchor=north]{$x$}
		-- (10,0) node[anchor=north]{$z$}
		-- (5,2) node[anchor=south]{$y$}
		-- cycle;
		\draw (5,-0.5) node {$ d(x,z) $};
		\draw (2.5,1.5) node {$ d(x,y) $};
		\draw (7.5,1.5) node {$ d(y,z) $};
	\end{tikzpicture}
	\caption{The triangle inequality.}
\end{figure}  
Geometrically, this is equivalent to saying that the length of any side of a triangle cannot be greater than the sum of the other two lengths. Whenever presented with a weak inequality, it is often helpful to ask ``when does this hold with equality''? In this case the answer is when $ y $ is on the line segment formed by $ x $ and $ z $. In this case going to $ y $ isn't a detour at all, but just a trivial stop on the way from $ x $ to $ z $! 
\begin{example}[Euclidean Metric]
The real line $ \R $ is a metric space when equipped with the metric $ d(x,y)=|x-y| $. 
\end{example}
\begin{example}[Euclidean Metric]
Euclidean space $ \R^n $ is a metric space when equipped with the metric $$ d(\mathbf{x},\mathbf{y})=|\x-\y|=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2} .$$  The Euclidean metric is intimately linked to the concept of a \textit{\color{red}norm}. Recall from linear algebra that Euclidean space is a vector space where vectors are elements of $ \R^n $, and scalers are elements of $ \R $. This space is equipped with function $ \norm{\cdot}_2:\R^n\to\R $ that measures the length of vectors. $$ \norm{\x}_2=\sqrt{x_1^2+\cdots+x_n^2}$$ We can write the Euclidean metric as $ d(\x,\y)=\norm{\x-\y}_2 $. Right now, don't worry too much about norms.\footnote{I admittedly am not certain of when it is best to introduce the concept of a norm. I don't like how \cite{rudin1964principles} talks about it in passing in when reviewing Euclidean space. Introducing it latter on when covering functional analysis is also problematic, because we're going to use the $ \sup $-norm before that to measure the distance between two functions.} 
\end{example}
\begin{example}[Taxi-Cab Metric]
If our set is $ \R^2 $ we can let $ d(\mathbf{x},\mathbf{y})=|x_1-y_1|+|x_2-y_2| $. This is often referred to as the taxi-cab metric, as it is how you would measure distance if driving a car on a grid. We can extend to $ \R^n $ and let $ d(\mathbf{x},\mathbf{y})=\sum_{i=1}^{n}|x_i-y_i| $. 
\end{example}
\begin{example}[$ p $-Adic Metric]
The previous examples are easy to verify, but this may not always be the case. Suppose are set is $ \Z $ and $ d:\Z\times\Z\to[0,\infty] $ is defined as $$ d(x,y)=\begin{cases}
0\text{ if }x=y\\p^{-\max\{m\in\N|\ p^m|(x-y) \}}\text{ otherwise}
\end{cases}$$ for some prime number $ p $. Before we verify this is a metric, it's worth getting a feel for how the metric actually works. If $ x\neq y $, then the distance between two points is $ p $ raised to some negative power. That negative power is defined to be the maximum whole number $ m $ such that $ (x-y) $ is divisble by $ p^m $. This gives us the vague idea that distance between points $ x $ and $ y $ is somehow related to how many times $ p $ shows up in the prime factorization of $ (x-y) $ (where $ m $ is the number of times). Let's take $ p=3 $, and pick several points in $ \Z $ to measure the distance between.    
\begin{center}
\begin{tabular}{cccccc}
	$ x $ & $ y $ & $ x-y $ & prime factorization of $ x-y $ & $ m $ & $ p^{-m} $ \\ \hline
	$ 100 $& 19  &  $ 81 $   &          $ 3^4 $                  &  $ 4 $  &     $ 1/81 $                  \\
$ 368 $	& 8  &  $ 360 $   &            $ 2^3\cdot5\cdot9 $                &  $ 0 $  &  $ 1 $                     \\
$ 35 $	& $ 5 $  &  $ 30 $   &  $ 2\cdot 3\cdot 5 $                           &      $ 1 $ & $ 1/3 $                 
\end{tabular}
\end{center}
It turns out that the more factors of $ p $ that go into the prime factorization of $ (x-y)$, the closer $ x $ and $ y $ are. Furthermore, the maximum distance between any two points is $ 1 $, as $ p^0=1 $ for all $ p $. We will now verify that this is indeed a metric. 
\begin{enumerate}
\item The function $ d(x,y) $ is defined such that $ d(x,y)=0 $ if and only if $ x=y $. 
\item We have $ (x-y)=-(y-x) $. Therefore, the prime factorization of each number differ only in sign, and give the same value $ m $. This implies that $ d(x,y)=d(y,x) $.  
\item Note that to show $ d(x,z)\le d(x,y)+d(y,z) $ for all points in $ \Z $, it suffices to show that $ d(x,z)\le\max\{d(x,y),d(y,z)\} $. This inequality happens to be a stronger condition that implies the triangle inequality. Suppose $ p^m\mid(x-y) $ and $ p^n\mid(y-z) $. For some $ s,r\in\Z $, we having
\begin{align*}
	x-y=p^mr\\y-z=p^ns.
\end{align*}
We can combine these equations to conclude $$x-z=(x-y)+(y-z)=p^mr+p^ns. $$ If $ m>n $, then $ x-z=p^n(p^{m-n}r+s)$ and $ d(x,z)=d(y,z) $. Similarly, if $ n>m $, $ d(x,z)=d(x,y) $. Finally if $ n=m $, then $$x-z=p^n(r+s)=p^m(r+s), $$ and $ d(x,z)=d(x,y)=d(y,z) $. These three cases gives the desired inequality. 
\end{enumerate}
\end{example}
\begin{definition}
	Let $ X $ be a metric space. A set $ E\subseteq X $ is \textit{\color{red} bounded} if there is a positive number $ M\in\R $ and a point $ x\in X $ such that $ d(x,y)<M $ for all $ x\in E $. If a set is no bounded, we say it is \textit{\color{red}unbounded}. 
\end{definition}
Boundedness insures that a set doesn't ``go off to infinity''. 
\begin{example}
	The sets $ \N $, $ \Z $, $ \Q $, and $ \R $ are all unbounded. 
\end{example}
\begin{example}
Both the intervals $ [a,b] $ and $ (a,b) $ are bounded in $ \R $. For any $ x,y\in[a,b] $, $ d(x,y)<d(a,b)+1 $. The same holds for $ (a,b) $. 
\end{example}
The metric space we are most interested in is of course $ \R^n $ equipped with the familiar Euclidean metric. We can use this metric to define the notion of an open or closed ball in $ \R^n $. 
\begin{definition}
	If $ \x\in\R^n $ and $ r>0 $, the \textit{\color{red} open ball} with center $ \x $ and radius $ r $ is defined as $$ B_r(\x)=\{\y\in\R^n\mid|\y-\x|<r\}. $$ 
\end{definition} 
\begin{definition}
	If $ \x\in\R^n $ and $ r>0 $, the \textit{\color{red} closed ball} with center $ \x $ and radius $ r $ is defined as $$ \bar{B}_r(\x)=\{\y\in\R^n\mid|\y-\x|\le r\}. $$ 
\end{definition}
Open and closed balls in $ \R^n $ are a generalization of the open and closed intervals you were first introduced to in high school, and Figure 9 provides an illustration in $ \R^2 $. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\filldraw[color=black, fill=gray!15, thick, dashed] (0,0) circle (50pt);
	\filldraw[color=black, fill=gray!15, thick ] (6,0) circle (50pt);
	\end{tikzpicture}
	\caption{Open and closed balls in $ \R^2 $.}
\end{figure}  
\subsection{Open Sets, Closed Sets, and Boundaries}
We want to generalize the notion of open and closed balls in $ \R^n $ to any metric space. In order to do these, we'll need to outline a couple preliminary definitions that classify the elements of a metric space. 
\begin{definition}
	A \textit{\color{red}neighborhood } of $ x $ in a metric space $ X $ is defined as $ N_r(x)=\{y\in X\mid d(x,y)<r\} $ for a radius $ r>0 $.  
\end{definition}
As the name implies, a neighborhood centered at $ x $ is simply all the points ``around'' $ x $. A neighborhood is its own set, and we will use them constantly. They are sort of like ``sets of utility'', because we will use them as tools to analyze the properties of other sets. If there is a set $ E $ in a metric space $ X $, we can use neighborhoods in $ X $ to learn about the points in $ E $. Are some neighborhoods subsets of $ E $? Do some neighborhoods intersect $ E $? Will the answers to these questions change if we make $ r $ really big or really small? It is worth thinking about these questions taking $ E $ to be one of the balls defined in Definition 3.3. Do the answers change if $ E $ is an open ball versus a closed ball? 

 Much like the open ball of Definition 3.2, neighborhoods do not include the points that are exactly a distance of $ r $ away from the point $ x $. In fact, if our metric space is $ \R^n $ with the standard metric, a neighborhood and open ball are precisely the same. 
\begin{example}
	Let our metric space be $ \Z^2 $ equipped with the taxi-cab metric. Figure 10 shows the neighborhood centered at the origin of radius 3. 
		\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\draw[step=1cm,gray,very thin] (-2,-2) grid (6,6);
		\draw[thick,->] (2,-2.5) -- (2,6.5);
		\draw[thick,->] (-2.5,2) -- (6.5,2);
		\filldraw (2,2) circle (3pt) node[anchor=north west] {$ 0 $};
		\filldraw (2,3) circle (3pt);
		\filldraw (2,4) circle (3pt);
		\filldraw (2,1) circle (3pt);
		\filldraw (2,0) circle (3pt);
		\filldraw (1,2) circle (3pt);
		\filldraw (1,3) circle (3pt);

		\filldraw (1,1) circle (3pt);
		
		\filldraw (0,2) circle (3pt);
		
		\filldraw (3,2) circle (3pt);
		\filldraw (3,3) circle (3pt);
		\filldraw (3,1) circle (3pt);
	
		\filldraw (4,2) circle (3pt);
	
	
		\end{tikzpicture}
		\caption{The set $ N_{3}(\mathbf{0})=\{\y\in\Z^2\mid|y_1|+|y_2|<3 \} $.}
	\end{figure}
\end{example} 
 
\begin{example}
Let $ d_{2}:\R^2\times\R^2\to[0,\infty] $ be the euclidean metric, and $ d_{\ell^1}:\R^2\times\R^2\to[0,\infty] $ be the taxi-cab metric. A neighborhood in $ (\R^2, d_{1}) $ may have a different ``shape'' then it would in $ (\R^2, d_{2}) $. We will denote $ N_{3}(\mathbf{0})\in \R^2 $ as $ E $ and $ F $, in $ (X,d_{2}) $ and $ (X,d_{1}) $ respectively. These neighborhoods are shown in Figure 11.     
	\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw[step=1cm,gray,very thin] (-2,-2) grid (6,6);
	\draw[thick,->] (2,-2.5) -- (2,6.5);
	\draw[thick,->] (-2.5,2) -- (6.5,2);
	\filldraw[color=blue, fill=blue!15, thick, dashed, opacity=0.6] (2,2) circle (3cm);
	\filldraw[color=red, fill=red!15, thick, dashed, opacity=0.6] (2,5) -- (5,2) -- (2,-1) -- (-1,2) --(2,5) ;
	\draw[color = red] (2.5,2.5) node {$ F $};
	\draw[color = blue] (4.5,4.5) node {$ E $};
	\end{tikzpicture}
	\caption{$ N_{3}(\mathbf{0})\in \R^2 $ in $ (\R^2, d_{2}) $ and $ (\R^2, d_{1}) $. }
\end{figure}	
\end{example}
\begin{definition}
	Let $ X $ be a metric space. A point $ x\in X $ is a \textit{\color{red}limit point} of the set $ E\subseteq X $ if \textit{every} neighborhood of $ x $ contains a point $ y\in E $, where $ y\neq x $. We will denote the \textit{\color{red} set of all limit points} of $ E $ as $ E'=\{x\in X\mid x\text{ is a limit point of }E \}=\{x\in X\mid \in (N_r(x)\cap E)\backslash\{x\}\neq\emptyset\ \forall r>0\} $.
\end{definition}
\begin{note}
	For the remainder of Section 2, we will use $ X $ to denote a metric space, and $ E $ as some subset of $ X $. 
\end{note}
A limit point of a set is in some sense always ``close'' to points of the set. If $ x $ is a limit point of $ E\subseteq X $, then $ N_r(x) $ will always include points other than $ x $, no matter what we take $ r $ to be! We could make $ r $ smaller and smaller, but the set $ N_r(x) $ will never just be $ x $. In this sense, a limit point can always be ``approximated'' by elements in $ E $.  
\begin{remark}
Definition $ 3.5 $ never specifically said that a limit point of some set belongs to the set. As the next example shows, being a limit point has nothing to do with whether or not a point is included in the set in question. 	
\end{remark}
\begin{example}
$\R^2$ is a good starting place. Suppose we have a set $ E\subseteq \R $ that for the most part forms a rectangle. The ``border'' of the rectangle is no included in $ E $. Also note that $ E $ includes an ``isolated'' point $ z $ (see Figure 12). Let's consider three points in $ \R^2 $: $ x $, $ y $, and $ z $.  
		\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\filldraw[color=black, fill=gray!15, thick, dashed] (0,0) rectangle (8,4);
	\filldraw (4,2) circle (2pt)node[anchor=north west] {$ x $};
	\filldraw (5.6,4) circle (2pt)node[anchor=north west] {$ y $};
	\filldraw[color=black, fill=gray!15, thick, dashed] (9,3) circle (2pt)node[anchor=north west] {$ z $};
	\end{tikzpicture}
	\caption{The set $ E\subseteq \R^2 $.}
\end{figure}

The point $ x $ belongs to $ E $. Furthermore, no matter what we take $ r $ to be, $ N_r(x) $ will never become a singleton of just $ \{x\} $. For the sake or argument, suppose $ x=(2,2) $. If $ r=0.5 $, then $ (2,2.49)\in N_{0.5}(x) $. Of $ r=0.01 $, then we still have $ (2,2.001)\in N_{0.01}(x) $. In fact, for every $ r $, we have $ (2,2+r/2)\in N_r(x) $. This means that $ x $ is a limit point of $ E $. 

Now consider $ y $. This point does not belong to $ E $, but it is still a limit point! We could repeat the same argument we made for $ x $ without running into trouble, because every neighborhood of $ y $ will include points ``just below'' $ y $, all of which are in $ E $! What matters with limit points is not what set the point belongs to, but what set the points nearby it belong to. 

Lastly, the point $ z $ is not a limit point. If we took $ r $ to be sufficiently large, then $ N_r(z) $ would include points in $ E $ that form the rectangle. Unfortunately, we could easily take $ r$ to be so small that $ N_r(z)=\{z\} $. It only takes one such $ r $ to rule out the chance of $ z $ being a limit point. We can provide a definition that corresponds to points like $ z $.     
\end{example}

\begin{definition}
		Let $ X $ be a metric space. For a set $ E\subseteq X $, $ x\in E $ is an \textit{\color{red}isolated point} if it is not a limit point. That is, there exists an $ r>0 $ such that $ N_r(x)=\{x\} $. 
\end{definition}
By the definition of an isolated point, it is the opposite of a limit point, rendering the two definitions mutually exclusive. This definition also means any point $ x\in X $ is \textit{either} a limit point \textit{or} an isolated point.  An isolated point of any set is also included in the set, which is not the case for limit points. 
\begin{definition}
Let $ X $ be a metric space. A point $ x\in X$ is an \textit{\color{red}interior point} of $ E\subseteq X $ if there exists \textit{a single} $ r>0 $ such that $ N_r(x)\subseteq E $. 
\end{definition}
\begin{example}
	Again, let's look at an example in $ \R^2 $. Let $ E\subseteq \R^2 $ be a closed ball that is ``punctured'' at $ z\in\R^2 $ such that $ z\notin E$. This can be seen in figure 13. The point $ x $ is in an interior point, as we could find some small $ r $ for which $ N_r(x)\subseteq E $. The point $ y $ is not an interior point of $ E $, because every single $ N_r(y) $ will contain some point outside of $ E $, meaning $ N_r(y)\not\subseteq E $. Finally, the point $ z $ is not an interior point, as each neighborhood of $ N_r(z) $ contains $ z $, and $ z\notin\ E $. Even though we can make $ r $ small enough to guarantee the only point in $ N_r(z) $ which is not in $ E $ is $ z $ $ (N_r(z)\backslash E=\{z\}) $, this point is all it takes to guarantee $ N_r(z)\not\subseteq E $ for all $ r $. 
\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	
		\filldraw[color=black, fill=gray!15, thick ] (6,0) circle (50pt);
		\filldraw[color=black, fill=white, thick, dashed] (6,0) circle (2pt)node[anchor=north west] {$ z $};
		\filldraw (5,1) circle (2pt)node[anchor=north west] {$ x $};
		\filldraw (7.45,1) circle (2pt)node[anchor=south west] {$ y $};
	\end{tikzpicture}
	\caption{The set $ E\subseteq \R^2 $.}
\end{figure}	
\end{example}
\begin{remark}
	While a limit point of $ E\subseteq X $ need not be a point in $ E $, an interior point of $ E $ must be an element of $ E $. If $ x $ is an interior point, then $ x\in N_r(x)\subseteq E $ for some $ r $, so $ x\in E $. 
\end{remark}
\begin{example}
	It may be tempting to conclude that an interior point must be a limit point, after all, if we can find an $ N_r(x)\subseteq E $, then it is likely each neighborhood would contain infinite points of $ X $. This logic makes the dangerous assumption that $ X $ is in infinite, and $ d(x,y) $ ``behaves like'' the Euclidean metric. Consider a metric space $ X $ with the discrete metric $$ d(x,y)=\begin{cases}
	1\text{ if }x\neq y\\
	0\text{ if }x=y
	\end{cases}.$$ 
	For any $ x\in X $, $ N_{1/2}(x)=\{x\}\subseteq X $. We have that $ x $ is an interior point, but not a limit point. 
\end{example}
We now briefly introduce the idea of an exterior point. The only difference between the  definition of an interior point and exterior point, will be that the neighborhood of a point will be in the complement of $ E $ for an exterior point. This small change in language will make a big difference in meaning. 
\begin{definition}
Let $ X $ be a metric space. A point $ x\in X$ is an \textit{\color{red}exterior point} of $ E\subseteq X $ if there exists \textit{a single} $ r>0 $ such that $ N_r(x)\subseteq E^c $.
\end{definition}
\begin{remark}
Any point $ x\in X $ is \textit{either} an interior point \textit{or} an exterior point.
\end{remark}
\begin{example}
	Let $ [0,1]\in \R $. The point $ 2\in\R $ is an exterior point of $ [0,1] $. 
\end{example} 
\begin{remark}[VERY IMPORTANT THEME]
	Nearly every definition in this section specifies a metric space $ X $. This means the metric space we work in could affect how we classify a point (and later sets). If we have two metric spaces $ X $ and $ Y $ where $ X\subseteq Y $, a point $ x\in E\subseteq X $ may be a limit point/interior point/exterior point in $ X $ but not in $ Y $. 
	
	We will see this come up again, and again. How a set/point behaves or is classified is contingent on what space we are in. A small change, whether it be the inclusion of some additional points, or changing the metric, can make a big difference. This means it is important to specify what space we're in if it is ever unclear. On the bright side, this all makes for great examples!   
\end{remark}
\begin{example}
The set $ \R $ has no limit points when equipped with the discrete metric $$ d(x,y)=\begin{cases}
1\text{ if }x\neq y\\
0\text{ if }x=y
\end{cases}.$$  

\end{example}

We already generalized the idea of some open or closed interval in $ \R $ to the concept of an open or closed ball in $ \R^n $. Now we will go one step further, by bringing these concepts to any metric space. 
\begin{definition}
	Let $ X $ be a metric space. A set $ E\subseteq X $ is \textit{\color{red}open} if every point of $ E $ is an interior point.  
\end{definition}
\begin{definition}
	Let $ X $ be a metric space. A set $ E\subseteq X $ is \textit{\color{red}closed} if it contains all its limit points. That is, $ E'\subseteq E $. 
\end{definition}
\begin{example}
	Let $ (a,b)\subseteq\R $. This set is open, as for all $ x\in(a,b) $, we can find an $ r $ such that $ N_r(x)\subseteq(a,b) $. If $ d(a,x)\ge d(x,b) $, let $ r=d(x,b)/2 $. If $ d(x,b)>d(a,x) $ let $ r=d(x,a)/2 $.  Figure 14 shows this neighborhood for $ x $ where $ d(a,x)\ge d(b,x) $.
		\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\draw[very thick, color=red] (0,0) -- (2,0); 
		\draw[very thick, ->] (6,0) -- (8,0);
		\draw[very thick, ->] (-6,0) -- (-8,0);
		\draw[very thick] (-6,0) -- (6,0);
		\filldraw[fill=white] (-6,0) circle (2pt)node[anchor=south east] {$ a $};
		\filldraw[fill=white] (6,0) circle (2pt)node[anchor=south west] {$ b $};
		\filldraw[fill=black] (2,0) circle (2pt)node[anchor=south east] {$ x $};
		\draw[color=red, very thick] (4,0) circle (2pt)node[anchor=north west] {$ x+\frac{|b-a|}{2} $};
		\draw[color=red, very thick] (0,0) circle (2pt)node[anchor=north east] {$ x-\frac{|b-a|}{2} $};
		\draw[very thick, color=red] (0,0) -- (4,0) node[anchor=south east] {$ N_{\frac{d(x,b)}{2}}(x) $}; 
		\filldraw[fill=black] (2,0) circle (2pt)node[anchor=south east] {$ x $};
		\end{tikzpicture}
		\caption{The open interval $ (a,b)\subseteq \R $.}
	\end{figure}
By construction, our neighborhood will always be a proper subset of $ (a,b) $, so each point of $ x $ is an interior point. Therefore $ (a,b) $ is open. On the other hand, $ (a,b) $ is not closed, because $ a $ and $ b $ are limit points, but neither are in the set $ (a,b) $.
\end{example}
\begin{example}
	Let $ [a,b]\subseteq\R $. This set is not open, as $ a $ and $ b $ are not interior points. For instance, $ a-r/2\in N_r(a) $ for all $ r>0 $. The number $ a-r/2\notin[a,b] $, so $ N_r(a)\not\subseteq[a,b] $ for all $ r $. While $ [a,b] $ is not open, it is closed. Every point in $ [a,b] $ is a limit point because $ \R $ is complete. The interval $ [a,b] $ trivially contains itself, so it contains all its limit points and is closed. 
\end{example}
\begin{example}
Let $ X $ be a metric space, and $ E\subseteq X $. Any set with no limit points, $ E'=\emptyset $, is closed, because $ \emptyset\subseteq E $. This means any finite set is closed, as no finite set has limit points. Let $ X=\{x_1,\ldots, x_n\} $, and $ E\subseteq X $. If we let $ y\in E $, and set $ r=\min_{x\in X}d(x,y) $, then $ N_r(y)=\{y\} $. This means $ y $ fails to be a limit point for all $ y\in E $.
\end{example}
\begin{remark}
	The definitions of open and closed sets never imply that a set is either closed or open. It is possible for a set to be both closed and open, or be neither closed nor open.
\end{remark}
\begin{example}
The set $ \emptyset $ in any metric space $ X $ is closed and open. This set has no limit points $ (\emptyset'=\emptyset) $, and $ \emptyset\subseteq\emptyset $, so it is closed. The set also has no points, so every point is an interior point, making $ \emptyset $ open. 
\end{example}
\begin{example}
The set of rationals $ \Q $ is neither open nor closed in $ \R $. For all $ x\in\Q $, $ N_r(x) $ will contain irrational numbers for all $ r $, meaning $ N_r(x)\not\subseteq\Q $. Therefore no elements of $ \Q $ are interior points. The set $ \Q $ also does not contain all of its limit points, as any irrational number is a limit point as a result of Theorem 2.4. For example, any neighborhood of $ \sqrt{2} $ will contain elements of $ \Q $ (which are not $ \sqrt{2} $), making it a limit point. 
\end{example}
\begin{remark}[Relative to Which Space]
	The point made in Remark 2.4 is especially relevant for open and closed sets. A set could be open in one metric space, but closed in another. For example, if we have the space of real numbers $ [0,1] $ with the Euclidean metric, $ [0,0.5) $ is open in $ Y $. We also have that $ [0,1] $ is open in $ Y $! In most cases it's clear what metric space we are working in, but sometimes it is not. In cases where it is vague, it's always best to say a set is \textit{open in $ X $} or \textit{closed in $ X $}. For this reason it is a good practice to either specify $ E\subseteq X $, or include ``in $ X $.'' Many topics in analysis concern the metric space $ \R^n $ or $ \R $, so if you say a set is closed or open in conversation, it is usually assumed the metric space is one of these spaces. For example, if you were to ask someone ``are the integers closed or open?", they would most likely assume you mean ``are the integers closed or open in $ \R $?" 
\end{remark}

\begin{example}
	Suppose we want to determine if $ \Z $ is open or closed in $ \Z $. Every point is an interior point as $ N_{1/2}(x)={x}\subseteq\Z $ for all $ x\in\Z $, so $ \Z $ is open in $ \Z $. The set $ \Z $ has no limit points in $ \Z $, as $ N_{1/2}(x) $ does not include any points $ y\in\Z $ where $ y\neq x $. This gives that $ \Z'=\emptyset $,\footnote{This also means that every point of $ \Z $ is isolated.} so $ \Z\subseteq \Z' $, and $ \Z $ is closed in $ \Z $.
	
	 Now let our metric space be $ \R $. Is $ \Z $ open in $ \R $? Let $ x\in\Z $. For any $ N_r(x) $ such that $ r<1 $, $ x-r/2\in N_r(x) $, where $ x-r/2\notin\Z $. If $ r\ge 1 $, then $ x-1/2\in N_r(x) $, where $ x-1/2\notin\Z $.\footnote{The case where $ r\ge 1 $ handles the situation where $ r/2\in\Z $. If this were the case, then $ x-r/2\in\Z $. This is not a problem, as $ N_r(x) $ would still contain an uncountably infinite number of reals numbers, but it makes explicitly finding one of those reals a little tricky. It's easier to just add or subtract $ 1/2 $ from $ x $ and call it a day.} Therefore, there exists no $ r $ such that $ N_r(x)\subseteq \Z $, so $ \Z $ is not open in $ \R $. We have that $ \Z $ is closed in $ \R $, as each point of $ \Z $ is still isolated.  
\end{example}

Before proving some useful properties of open and closed sets, there is one more definition that can prove helpful at times. It formalizes the notion of points in a set that are just on the border of a set, like the endpoints of $ [a,b]\subseteq\R $.  
\begin{definition}
	Let $ X $ be a metric space, and $ E\subseteq X $. The \textit{\color{red}boundary} of $ E $, denoted $ \partial E $, is the set of points in $ X $ such that every neighborhood of $ p $ contains at least one point of $ E $ and at least one point not of $ E $. $$ \partial E=\{x\in X\mid N_r(x)\cap E\neq\emptyset\text{ and }N_r(x)\cap  E^c\neq\emptyset \ \forall r>0  \} $$ 
	Any element of $ \partial E $ is a \textit{\color{red}boundary point}.
\end{definition}
There are several equivalent definitions of $ \partial E $, many of which are more popular than this specific one. These other definitions use terms that we will cover in Section 3.4, so we will circle back then and discuss the boundary of a again. The first example one's mind should jump to are open and closed balls in $ \R^n $. 
  \begin{figure}[h]
  	\centering
  	\begin{tikzpicture}
  	\draw[color=black, thick ] (0,0) rectangle (12,6);
  	\filldraw[color=blue, fill=gray!15, thick ] (4,3) circle (2.5cm);
  	\draw (12.2,6.4) node {$ X $};
  	\draw (2,3) node {$ E $};
  	\draw (10,3) node {$ E^c $};
  	\filldraw (6.1,2.7) circle (1pt) node[anchor = south east] {$ y $};
  	\filldraw (6.5,3) circle (1pt) node[anchor = south east] {$ x $};
  	\filldraw (7.2,3.2) circle (1pt) node[anchor = south east] {$ z $};
  	\draw[color=red,dashed, thick ] (6.5,3) circle (1cm);
  	\draw[color = red] (8,3.3) node {$ N_r(x) $};
  	\draw[color = blue] (5.4,5.4) node {$ \partial E $};
  	\end{tikzpicture}
  	\caption{The boundary of a set $ E $ is shown in blue. Let $ \x\in X $. No matter how small we make $ r $, $ N_r(x) $ will always contain some $ y\in E $ and some $ z\in E^c $, so $ x\in\partial E $. } 
  \end{figure}
\begin{example}
	Let $ B_r(\x) $ be the open ball of radius $ r $ centered at $ x\in\R^n $ (we could also denote this as $ N_r(\x) $). As the name suggests, the boundary is just all the points that are exactly a distance of $ r $ away from $ \x $, meaning $ \partial B_r(\x)=\{\y\in X\mid |\x-\y|=r\} $. In this particular case, no points in the boundary are in $ B_r(\x) $, so we have $ B_r(\x)\cap\partial B_r(\x)=\emptyset  $. If we take $ \bar{B}_r(\x) $ to be the closed ball, then we have the same boundary. $$\partial \bar{B}_r(\x)=\partial B_r(\x)=\{\y\in X\mid |\x-\y|=r\} $$ We also have $ \partial \bar{B}_r(\x)\cap \bar{B}_r(\x)=\bar{B}_r(\x)  $, and $ B_r(\x)\cup\partial B_r(\x)=\bar{B}_r(\x) $. 
\end{example}
\begin{remark}
	It is very tempting to think all boundary points are limit points. At first glance, the definition of a boundary point seems to imply a point $ x\in\partial E $ is not only a limit point of $ E $, but also a limit point of $ E^c $. This is not true! Suppose $ x\in E $ is a limit point. The definition of a limit point not only requires that $ N_r(x)\cap E $ for all $ r $, but also requires that there are points \textit{other than} $ x $ in $ N_r(x) $. A boundary point needn't satisfy this second requirement, so even if $ N_r(x)\cap E=\{x\} $ for all $ r $, $ x $ can still be a boundary point! 
\end{remark}

\begin{example}
	Let $ E\subseteq \R^2 $ be the union of a disk punctured at $ z $ and an isolated point $ y $ (Figure 16).  
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\filldraw[color=black, fill=gray!15, thick ] (6,0) circle (50pt);
		\filldraw[color=black, fill=white, thick, dashed] (6,0) circle (2pt)node[anchor=north west] {$ z $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (9,1) circle (2pt)node[anchor=north west] {$ y $};
		\end{tikzpicture}
		\caption{The set $ E\subseteq \R^2 $.}
	\end{figure}

The points $ y $ and $ z $ are both boundary points. Despite this, $ y $ is not a limit point of $ E $, and $ z $ is not a limit point of $ E^c $. This follows from the reasoning in the previous remark.  	
\end{example}

\begin{remark}
	We have now introduced five different definitions that classify points: limit points, isolated points, interior points, boundary points, and exterior points. This is \textit{a lot} to take in all at once. By far the most important concepts introduced here were open and closed sets. Being able to determine if a set is open and/or closed is one of the most important skills to have for this section, and those that follow. 
\end{remark}
\subsection{Properties of Open and Closed Sets}
Open set and closed sets will play a roll in many of the results and theorems to come, so it is important to be able to identify which sets are open and which are closed. We will know introduce several tools that make this easier.  
\begin{proposition}
	Every neighborhood is an open set.
\end{proposition}
\begin{proof}
	Suppose we have a metric space $ X $, and some point $ x\in X $. We will show that any point $ y\in N_r(x) $ is an interior point. There exists some $ h $ such that $$d(x,y)=r-h .$$ I claim that $ N_{r'}(y)\subseteq E $ for $ r'<h $. For all points $ z $ such that $ d(y,z)=r'<h $, the triangle inequality gives $$d(x,z)\le d(x,y)+d(y,z)<r-h+h=r, $$ so $ z\in N_r(x)=E $ for all $ z $ by the definition of $ N_r(x) $. This implies that $ N_{r'}(y)\subseteq E $, making $ y $ an interior point. (Figure 17)
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\filldraw[color=black, fill=gray!15, thick, dashed ] (6,0) circle (4cm);
		\draw[very thick, dashed, color=blue] (6,0) -- (8,0);
		\draw[color=blue] (7.8,-0.3) node[anchor = east] {$ r-h $};
		\draw[very thick, dashed, color=green] (8,0) -- (10,0);
		\draw[color=green] (9,-0.3) node[anchor = east] {$ h $};
		\draw[color=black] (9.8,3.8) node[anchor = east] {$ E=N_r(x)$};
		\filldraw (6,0) circle (1pt) node[anchor = south east] {$ x $};
		\filldraw (8,0) circle (1pt) node[anchor = south west] {$ y $};
		\filldraw (8,0.7) circle (1pt) node[anchor = south east] {$ z $}; 
		\draw[color=red] (8.5,1.4)  node[anchor = south east] {$ N_{r'}(y) $};
		\draw[color=red, thick, dashed] (8,0) circle (1.4cm); 
		\end{tikzpicture}
		\caption{The set $ E\subseteq X $. We constructed a neighborhood $ N_{r'}(y) $ for an arbitrary $ y\in N_r(x) $ such that $ N_{r'}(y)\subseteq E $. }
	\end{figure}
\end{proof}

\begin{proposition}
	If $ x\in X $ is a limit point of $ E $ ($ x\in E' $), then every neighborhood of $ x $ contains infinitely many points of $ E $.
\end{proposition}
\begin{proof}
	Let $ x\in X $. Suppose for contradiction, there exists some $ N_r(x) $ which contains only a finite number of points of $ E $. Let this finite set of points be $ \{y_1, \ldots, y_n\}\subseteq N_r(x)\cap E $. Pick the radius of $ N_r(x) $ to be the distance between $ x $ and the point to which it is closest in the finite set $ \{y_1,\ldots,y_n\} $: $$ r=\min_{1\le m\le n} d(x,y_m). $$ By construction, $ N_r(x) $ contains no point $ y\in E $ such that $ y\neq x $, so $ x $ is not a limit point of $ E $. This is a contradiction. (Figure 18)
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\filldraw[color=black, fill=gray!60, thick, dashed] (1,0) circle (2pt)node[anchor=north west] {$ y_1 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (0,0) circle (2pt)node[anchor=north west] {$ x $};
		
		\filldraw[color=black, fill=gray!60, thick, dashed] (1.5,0) circle (2pt)node[anchor=north west] {$ y_2 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (6.5,0) circle (2pt)node[anchor=north west] {$ y_3 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (-2.3,0) circle (2pt)node[anchor=north west] {$ y_4 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (-3.5,0) circle (2pt)node[anchor=north west] {$ y_5 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (5,0) circle (2pt)node[anchor=north west] {$ y_6 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (3.1,0) circle (2pt)node[anchor=north west] {$ y_7 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (4,0) circle (2pt)node[anchor=north west] {$ y_8 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (4.5,0) circle (2pt)node[anchor=north west] {$ y_9 $};
		\filldraw[color=black, fill=gray!60, thick, dashed] (6,0) circle (2pt)node[anchor=north west] {$ y_{10} $};
		\draw[color=red, thick, dashed] (0,0) circle (1cm);
		\draw[color=red] (3,.5)  node[anchor = south east] {$ N_{r}(x)=\{x\} $};
		\end{tikzpicture}
		\caption{If our finite set of points of $ E $ is $ \{y_1,\ldots,y_{10}\} $, then we can reach a contradiction by constructing a neighborhood of $ x $ with $ r=\min_{1\le m\le n} d(x,y_m)=d(x,y_1) $. This will hold no matter where the points  $ \{y_1,\ldots,y_{10}\} $ happen to be in $ E $. In this case $ x\in E $, but remember that this isn't a requirement.}
	\end{figure}
\end{proof}
	
\begin{corollary}
A finite set has no limit points. (see Example 3.13)
\end{corollary}
The next theorem and its corollary allows us to determine if a set is open or closed based on its complement. At first, this may not seem helpful, but if it is not clear if $ E $ is open or closed, one can just use $ E^c $! We will provide examples where using $ E^c $ is easier.
\begin{theorem}
	A set $ E $ is open \textit{if and only if} its complement is closed.
\end{theorem}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ E $ is open. Let $ x\in X $ be a limit point of $ E^c $. Every neighborhood $ N_r(x) $ contains a point of $ E^c $, so $ N_r(x)\not\subseteq E $, meaning $ x $ is \textit{not} and interior point of $ E $. But we have assumed every point of $ E $ is an interior point, so $ x\in E^c $. Therefore $ E^c $ includes all its limit points and is closed.   
		\item [$ (\Longleftarrow) $] Suppose $ E^c $ is closed. Let $ x\in E $. We have $ x\notin E^c $, so $ x $ is not a limit point of $ E^c $ (otherwise it would be in $ E^c $, as $ E^c $ is closed). If $ x $ is not a limit point of, then there exists an $ N_r(x)\cap E^c=\emptyset $, giving $ N_r(x)\subseteq E $. Thus $ x\in E $ is an interior point, and $ E $ is open.  
	\end{enumerate}
\end{proof}
\begin{corollary}
	A set $ E $ is closed \textit{if and only if} its complement is open.
\end{corollary}
One practical consequence of these results, is that if you find it more difficult to check if a set is open or closed (or vice versa), you can always just work with the complement. 

\begin{example}
Let $ X $ be any metric space. Recall from Example 3.14 that $ \emptyset $ is closed and open. This means that $ \emptyset^c=X $ is closed and open as well. This allows us to conclude that $ \R $ in $ \R $ is open and closed.
\end{example}
\begin{example}
	The set $ [a,b]\subseteq \R $ is closed. This implies that $ [a,b]^c=(-\infty,a)\cup(b,\infty) $ is open.
\end{example}
We often define some set of interest as a union or intersection of a collection of sets. For instance, the proof of Theorem 1.2, the supremum of a set in $ \R $ was defined as the union of the Dedekind cuts that comprise the set. In Example 1.26, $ \Q $ was written as the countably infinite union of intervals. In situations like this, it is possible to know if a set is open or closed if the sets over which we take the union/intersection are open or closed. 
\begin{theorem}
	Let $ \{G_\alpha\} $ and $ \{F_\alpha\} $ be an arbitrary collection of open sets and closed sets respectively. Let $ G_1,\ldots,G_n $ and $ F_1,\ldots, F_n $ be a finite collection of open sets and closed sets respectively. In this case we have:
	\begin{enumerate}
		\item $ \bigcup_\alpha G_\alpha $ is open.
		\item $ \bigcap_\alpha F_\alpha $ is closed.
		\item $ \bigcap_{i=1}^n G_i$ is open.
		\item $ \bigcup_{i=1}^n F_i$ is closed.
	\end{enumerate}

\end{theorem}

\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item Suppose $ G_\alpha $ is open for all $ \alpha $. Let $ x\in \bigcup_\alpha G_\alpha $. For some $ \alpha $, $ x\in G_\alpha $. There exists a neighborhood $ N_r(x) $ such that $ N_r(x)\subseteq G_\alpha $, because $ G_\alpha $ is open. Therefore $ \bigcup_\alpha G_\alpha $ is open, as $ N_r(x)\subseteq G_\alpha\subseteq \bigcup_\alpha G_\alpha $. 
		\item Suppose $ F_\alpha $ is closed for all $ \alpha $. It suffices to show that $ (\bigcap_\alpha F_\alpha)^c $ is open using Theorem 2.1. By the aforementioned theorem, $ F_\alpha^c $ is closed for all $ \alpha $. Therefore the union of $ F_\alpha^c $ is open by part (1). This completes our proof, as De Morgan's Law gives $$ \left(\bigcap_\alpha F_\alpha\right)^c=\bigcap_\alpha F_\alpha^c.$$
		\item Suppose the sets $ G_1,\ldots,G_n $ are open. Let $ x\in \bigcap_{i=1}^n G_i$. For all $ x\in\bigcap_{i=1}^n G_i $, there exists neighborhoods $ N_{r_i}(x) $ with radii $ r_i $, such that $ N_{r_i}(x)\subseteq G_i $ for all $ i $. Let $ r=\min\{r_1,\ldots,r_n\} $. This radius gives us $ N_r(x)\subseteq G_i $ for all $ i $, meaning $ N_r(x)\subseteq \bigcap_{i=1}^n G_i $. Therefore $ x $ is an interior point, and $ \bigcap_{i=1}^n G_i $ is open. (Figure 19)   
		\begin{figure}[h!]
			\centering
			\begin{tikzpicture}
				\filldraw[color=blue, fill=blue!15, thick, dashed, opacity=1] (0,0) circle (3cm);\filldraw[color=red, fill=red!15, thick, dashed, opacity=0.8] (3.5,0) circle (3cm);
				\draw[color=blue] (-1,2.8)  node[anchor = south east] {$ G_1 $};
				\draw[color=red] (4.8,2.8)  node[anchor = south east] {$ G_2 $};
				\filldraw(1,0)  circle (1pt);
				\draw (1.1,0) node[anchor = south east] {$ x $};
				\draw[dashed, thick] (1,0) circle (1.8cm);
				\draw[dashed, thick] (1,0) circle (0.4cm);
				\draw(-0.5,1)  node[anchor = south east] {$ N_{r_1}(x) $};
				\draw(2.5,0.4)  node[anchor = south east] {\tiny$ N_{r_2}(x)=N_r(x) $};
			\end{tikzpicture}
			\caption{In this simplified setting, we have two open sets: $ G_1 $, and $ G_2 $. By letting $ r=\min\{r_1,r_2\}=r_2 $, we find a neighborhood $ N_r(x) $ such that $ N_r(x)\subseteq G_1\cap G_2 $. }
		\end{figure}
		\item  Suppose the sets $ F_1,\ldots,F_n $ are closed. It suffices to show that $ (\bigcap_{i=1}^n F_i)^c $ is open using Theorem 2.1. By the aforementioned theorem, $ F_i^c $ is open for all $ i$. By part (2) the the intersection of all $ F_i^c $ is open This complete out proof, as De Morgan's Law gives 
		 $$ \left(\bigcap_{i=1}^n F_i\right)^c=\bigcap_{i=1}^n F_n^c.$$
	\end{enumerate}
\end{proof}
Part (2) and (4) of Theorem 2.2 require that the collection of sets is finite, otherwise the minimum over the finite number of radii of neighborhoods many not be well defined. The following two examples show that Theorem 2.2 does not hold if we take these collections to be infinite. 
\begin{example}
	Let $ G_n=(1/n,1+1/n) $ for all $ n\in\N $. The set $ G_n $ is open in $ \R $ for all $ n $. Taking the intersection gives $$\bigcap_n G_n=[0,1] .$$ The interval $ [0,1] $ is closed, despite each $ G_n $ being open.  
\end{example}
\begin{example}
	Let $ F_n=[1/n,\infty) $ for all $ n\in\N $. The set $ F_n $ is closed in $ \R $ for all $ n $. Taking the union gives $$ \bigcup_{n}F_n=(0,\infty).$$ This interval is open in $ \R $, despited each $ F_n $ being closed.
\end{example}
\subsection{Closures, Interiors, Dense Sets, and Perfect Sets} 
There are a handful of other definitions related to open set and closed sets that deserve a bit of attention.
\begin{definition}
Let $ X $ be a metric space. The \textit{\color{red}interior} of a set $ E\subseteq X $, denoted $ E^\circ $, is the set of all interior points of $ E $. $$ E^\circ=\{x\in X\mid x\text{ is an interior point of }E \} $$
\end{definition}
The set $ E^\circ $ is clearly open, as by definition it is comprised only of interior points. Because interior points of $ E $ must be in $ E $, we have $ E^\circ \subseteq E $. Informally, we can think of $ E^\circ $ as the smallest open set contained within $ E $. This interpretation leads to the conclusion that if $ E $ is open, then $ E=E^\circ $.
\begin{example}
	Let $ [a,b]\subseteq\R $. The interior of this set is $ (a,b) $. 
\end{example}
\begin{definition}
	Let $ X $ be a metric space. The \textit{\color{red}closure} of a set $ E\subseteq X $ is $ \bar{E}=E\cup E' $. 
\end{definition}
The closure $ \bar{E} $ is the opposite of the interior in a certain sense. The closure can be though of as the smallest closed set that $ E $ is contained in. If $ E $ is closed, then $ E'\subseteq E $, and $ E=\bar{E} $.
\begin{example}
	Let $ (a,b)\subseteq\R $. The closure of this set is $ [a,b] $. 
\end{example}
\begin{definition}
	Let $ X $ be a metric space. The set $ E\subseteq X $ is \textit{\color{red}dense in $ X $} if every point of $ X $ is a limit point of $ E $, or a point of $ E $. ($ \bar{E}=X $)  
\end{definition}

Informally, if $ E $ is dense in $ X $, then we can approximate any point of $ X $ with a point in $ E $ arbitrarily well. This follow from the fact that any point in $ X $ is either in $ E $, or a limit point of $ E' $, or both.  We have already seen one example of this with Theorem 1.4.
\begin{example}
	The set $ \Q $ is dense in $ \R $, that is $ \bar{\Q}=\R $. One implication of this fact is that the irrational numbers $ \R\backslash\Q $ are limit points of $ \Q $. We also have that the irrationals $ \R\backslash\Q $ are dense in $ \Q $!
\end{example}
Many results involving approximation can be stated in terms of dense sets. One of these is the Weierstrass Approximation Theorem. This will be formally treated and proved in Section 7, but for now we will give the result as an example of a dense set.
\begin{example}[Weierstrass Approximation Theorem]
	Let $ C([a,b])=\{f\mid f:[a,b]\to \R\text{ and } f\text{ continuous}\} $ be the set of real valued continuous functions with domain $ [a,b] $. Now let $ \mathscr{P}([a,b]) $ be the set of all real valued polynomials with domain $ [a,b] $.\footnote{This set is traditionally denoted as $ \R[x] $ in abstract algebra.} The set $ \mathscr{P}([a,b])  $ is dense in $ C([a,b]) $. We will always be able to approximate a continuous function on a bounded interval arbitrarily well with polynomials. This result, and spaces of functions, will be discussed again in Section 7. 
\end{example}
\begin{definition}
	Let $ X $ be a metric space. The set $ E\subseteq X $ is \textit{\color{red}perfect} if every point of $ E $ is a limit point of $ E $ $ (E=E') $. 
\end{definition}
If $ E $ is perfect, every point in $ E $ can be approximated arbitrarily well by other points in $ E $. 
\begin{example}
	The real line $ \R $ is a perfect set. 
\end{example}
\subsection{Compact Sets}
We have encountered infinity several times now. Sets can have an infinite number of element, in which case they are either countable or uncountable. A set can ``take up an infinite amount of space'' if it is unbounded. Each limit point of a set contains an infinite number of points in the set. These factors can result in sets that are difficult to work with. For example, suppose a set is unbounded. It can be hard to determine how functions or sequences behave on sets like this, because the distance between points can become arbitrarily large. What kind of headaches to limit points cause? Any neighborhood of a limit point will contain an infinite number of points in that set (Proposition 2.2). If the limit point is not in the set, this can also pose problems. As we will see later on, it could be possibly to get arbitrarily close to that limit point while never leaving the set. In a sense, we would be getting closer to a point in a set, where the destination is not even included in the set. To prevent this from happening, all limit points should be included in a set, i.e the set should be closed. Later on when working with sequences and continuous functions, two concepts intrinsically linked by the idea of getting arbitrarily close to a point, sets that are ``nice'', and will not illicit the two mentioned complications, will lead to nice results. Our goal now is to characterize these sets, and attempt to motivate their characterization.\footnote{Motivating the main definition of this subsection is infamously difficult, as it is not clear how it will be used in the future. It would be like explaining what a hammer is to someone who has no idea what a nail is. }   

One somewhat trivial way to guarantee a set is both closed and bounded is by restricting out attention to finite sets. If $ E $ is finite, it has no limit points and is trivially closed. A finite set must also be bounded. While finite may have the nice properties we are looking for, they are not that interesting. Real analysis almost always involves the real numbers, an uncountably infinite set. So what criteria would guarentee an infinite set, whether it be countable or uncountable, will behave like a finite set?

The way we will go about defining these sets is by looking how we can ``cover'' them with a collection of open sets. The idea is that a set may be infinite, but perhaps we can cover it with a finite collection of open sets. 
\begin{definition}
	Let $ X $ be a metric space. An \textit{\color{red}open cover} of a set $ E\subseteq X $ is a collection of open sets $ \mathcal\{G_\alpha\}\subseteq X $ such that $ E\subseteq\cup_\alpha G_\alpha $.  
\end{definition}
  \begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[color=black, thick] (0,0) rectangle (12,6);
	\filldraw[thick, fill=gray!15] (6,3) circle (2cm);
	\draw (12.2,6.4) node {$ X $};
	\draw (6,3) node {$ E $};
	\draw[thick, dashed] (6,2) circle (1.5cm);
	\draw (6,0.2) node {$ G_1 $};
	\draw[thick, dashed] (7.8,3) circle (1.5cm);
	\draw (9.6,3) node {$ G_2 $};
	\draw (8.5,5) node {$ G_3 $};
	\draw (4,5.4) node {$ G_4$};
	\draw[thick, dashed] (7,4) circle (1.5cm);
	\draw[thick, dashed] (5,3.5) circle (1.9cm);
	\end{tikzpicture}
	\caption{The collection of set $ \{G_1,\ldots G_4\} $ forms an open cover of $ E $.} 
\end{figure}
An open cover is a \textit{collection} of sets. Each element of an open cover is a single open set. This means that the cardinality of an open set has \textit{nothing} to do with the cardinality of the sets it is comprised of. The collection $ \{\R,(0,1),(0,2)\} $ is 3. We do not care whatsoever about the fact that each of the sets in the collection are uncountably infinite. The emphasis here is due to the fact that the cardinality of these covers (and a second type we will define soon) will be the bases of our criteria of what makes a set ``nice''.
\begin{example}
	Let $ E=N_r(x) $ be a subset of a metric space $ X $. One open cover of the set $ E $ is the single set $ N_{r+1}(x) $. Another would be $ N_{r+2}(x) $. 
\end{example}
\begin{example}
	Let $ \R $ be the entire real line. We can cover this with the union of all sets of the form $G_n=(-n,n)$ for $ n\in\N $. $$ \R\subseteq\bigcup_{n\in \N}(-n,n)$$
\end{example}
It is not enough to require that a set has a finite open cover. Any set $ E $ can trivially be covered by itself, forming an open cover consisting of one element. We could require that every open cover is finite, but this could never be satisfied. For example, take the closed interval $ (a,b)\subseteq\R $. We could cover this with a finite open cover $ \{(a,b)\} $. We could also cover it with the infinite open cover $ \{(a,b),(-1,1),(-2,2),(-3,3),\ldots\} $. We can just take the cover $ \{(a,b)\} $ and throw in an infinite number of random intervals of $ \R $ and still end up with an open cover. This is complete ``overkill'' when it comes to covering $ (a,b) $! To address the fact that we will always have infinite open covers, we will introduce a new type of open cover that is both finite, and limits any redundant additions to the cover. 

\begin{definition}
Let $ X $ be a metric space, and $ E\subseteq X $. A \textit{\color{red}finite subcover} of an open cover $ \{G_\alpha\} $ of $ E $  is a collection of open sets $ \{G_{\alpha_1},\ldots,G_{\alpha_n}\} $ such that $$ E\subseteq \bigcup_{i=1}^nG_{\alpha_i}\subseteq\bigcup_\alpha G_\alpha. $$
\end{definition}
\begin{example}
	Let $ X $ be a metric space and $ E\subseteq X $. The collection $ \{E\} $ is a trivial open cover. We also have a trivial finite open subcover in $ \{E\} $. 
\end{example}
\begin{example}
	Let $ \{(a,b),(-1,1),(-2,2),(-3,3),\ldots\} $ be an open cover of $ (a,b)\subseteq \R $. One finite subcover of this open cover is $ \{(a,b)\} $. Another finite subcover is $ \{(a,b),(-1,1)\} $. In fact, any set $ \{(a,b),(-1,1),\ldots(-n,n)\} $ is a finite subcover. 
\end{example}
This simple example shows that some open covers actually have an infinite number of finite subcovers. The introduction of finite subcovers may be a bit confusing, so it is worth recapping what we have done before presenting the main definition of this subsection: 
\begin{itemize}
	\item We have some metric space $ X $ and some set $ E\subseteq X $. We can cover this set with a collection of sets $ \{G_\alpha\} $ called an open cover, the cardinality of which is determined by the number of sets in the collection $ \{G_\alpha\} $. We like the idea of finite open covers.
	\item There exists an infinite number of open covers of a set. There also always exists a finite open cover of a set, so we need to do better than just having a finite open cover.  
	\item A finite subcover $ \{G_{\alpha_1},\ldots,G_{\alpha_n}\} $ of the open cover $ \{G_\alpha\} $ is a subset of $ \{G_\alpha\} $, which also covers $ E $. The finite subcover of one open cover $ \{G_\alpha\} $ \textit{is not necessarily} a finite subcover of another open cover $ \{G'_\alpha\} $, so whenever we talk about finite subcovers, it is with respect to some fixed open cover. Some open covers of sets have an infinite number of finite subcovers (Example 2.29).
\end{itemize} 
We are now ready give a proper definition and name to the ``nice'' sets we want to characterize. In doing so, we will answer an important question about finite subcovers that may have arisen by now -- some open covers of sets have an infinite number of finite subcovers, but do \textit{all} open covers of a set have \textit{at least one} finite subcover? 
\begin{definition}
	Let $ X $ be a metric space, and $ K $ be a subset of $ X $. The set $ K $ is \textit{\color{red}compact} if \textit{every} open cover of $ K $ contains \textit{at least one} finite subcover. 
\end{definition} 
The answer to our question turns out to be no. If it were yes, then there would be no need to define compactness, because every set would be compact. Compact sets turn out to be the nice sets we were looking for. As we'll see, they can be infinite, but they do not cause the complications with infinity that we discussed at the open of this subsection. In some sense, compact sets are the next best thing to finite sets. 
\begin{example}
	The set $ (a,b)\subseteq\R $ is not compact. In order verify this, we just need to find a single open cover that has no finite subcover. Let $ \{G_n\} $ be an open cover where $ G_n=(a+1/n,b) $. We have that $$ (a,b)\subseteq\bigcup_{n\in \N} G_n=\bigcup_{n\in \N}(a+1/n,b)=(a,b).$$ No finite subset of $ \{G_n\} $ will be a finite subcover of $ (a,b) $. If we had a finite subset of $ \{G_n\} $ then there would exist some $ N\in\N $ such that $ (a,a+1/N) $ is ``uncovered''. Therefore any open interval in $ \R $ is not compact. 
\end{example}
\begin{example}
	The real line $ \R $ is not compact. The open cover $ \{G_n\} $ where $ G_n=(-n,n) $ has no finite subcover. If we had a finite subset of $ \{G_n\} $, then there would exist some $ N\in\N $ such that $ (-\infty,-N)\cup(N,\infty) $ is ``uncovered''. Therefore the real line $ \R $ is not compact. 
\end{example}

These two examples should not be entirely surprising. When compactness was motivated, complications involving two types of sets were cited: unbounded sets, and sets that were not closed. $ (a,b) $ and $ \R $ both fall into exactly one of these categories.
\begin{example}
	Suppose $ X=\{x_1,\ldots,x_n\} $ is a finite metric space. The entire space $ X $ is compact. Let $ \{G_\alpha\} $ be an open cover of $ X $. For each $ x_i\in X $, there exists an $ G_{\alpha_i} $ such that $ x_i\in G_{\alpha_i} $. Therefore, the open cover has a finite subcover in $ \{G_{\alpha_1},\ldots ,G_{\alpha_n}\} $.    
\end{example}
Using the definition of compactness to verify that a set is not compact takes some creativity, but only requires one to find a single counterexample. This is opposed to verifying a set is compact. This requires we somehow verify that every single open cover has a finite subcover. This is prohibitively difficult to do in most cases. This is why we want to find conditions that are easy to verify and that imply compactness. Of chief concern, is doing this for subsets of $ \R^n $. 
\subsection{Properties of Compact Sets}
Before restricting our attention to $ \R^n $, we need to establish some properties of compact sets that will allow come in handy when working with them. Unfortunately, we still do not have any nontrivial examples of compact sets, so some of these results will not have examples presented alongside them. Once we are able to identify compact sets in $ \R^n $ by means other then the definition of compactness, these results can be verified. 
\begin{lemma}
	Suppose $ Y\subseteq X $. A subset $ E\subseteq Y$ is open in $ Y $ \textit{if and only if} $ E=Y\cap G $ for some open $ G\subseteq X $. 
\end{lemma}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item[$ (\Longrightarrow) $] Suppose $ E\subseteq Y $ is open in $ Y $. For each $ x\in E $, there exists a $ r_x $ such that $ N_{r_x}(x)\subseteq E $. By the definition of a neighborhood, for all $ y\in Y $ satisfying $ d(x,y)<r_x $, we have $ y\in E $. Denote $ N_{r_x}(x)=V_x $ for all $ x\in E $, and define $$G=\bigcup_{x\in E}V_x .$$ The set $ G $ is a union of open sets, so it is open (Figure 21). I now claim that $ E=G\cap Y $, which is our desired result. For $ x\in E $, we have $ x\in V_x $ for all $ x\in X $, so $ x\in E $ and $ x\in Y $. This gives $ E\subseteq G\cap Y $. Now let $ x\in G\cap Y $. For the corresponding $ V_x $, $ V_x\cap Y\subseteq E $. This implies $ G\cap Y\subseteq E $.    
		\item[$ (\Longleftarrow) $] Suppose $ E=Y\cap G $ for some open $ G $ in $ X $. The set $ G $ is open, so for all $ x\in E $ there exists a neighborhood $ N_r(x)\subseteq E $. This gives $ N_r(x)\subseteq G $, as $ E=Y\cap G\subseteq G $. Intersecting $ N_r(x) $ with $ Y $ yields $ N_r(x)\cap Y\subseteq E $, so $ E $ is open in $ Y $.  
	\end{enumerate}	
 \begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw[color=black, thick] (0,0) rectangle (12,6);
	\draw[color=black, thick] (0.5,0.5) rectangle (7,5.5);
	\draw[black] (7.5,5.5) node {$ Y $};
	\draw (12.2,6.4) node {$ X $};
	\filldraw[color=blue, thick, dashed, fill=blue!15]  (4,3) circle (2cm);
	\draw[blue] (4,5.3) node {$ E $};
	\draw[thick, dashed]  (4.5,3) circle (1.4cm);
	\filldraw (4.5,3) circle (1pt) node[anchor= south east] {${x_1}  $}; 
	\draw (4.5,4.6) node {$V_{x_1} $};
	
	\draw[thick, dashed]  (3,2) circle (.4cm);
	\filldraw (3,2) circle (1pt) node[anchor= south east] {${x_2}  $}; 
	\draw (2.8,2.6) node {$V_{x_2} $};
		\draw[thick, dashed]  (3.5,4) circle (.8cm);
	\filldraw (3.5,4) circle (1pt) node[anchor= south east] {${x_3}  $}; 
	\draw (2.6,3.6) node {$V_{x_3} $};

	\end{tikzpicture}
	\caption{We take $ V_x $ to be the neighborhood of $ x $ contained in $ E $. We will always be able to find such a neighborhood because $ E $ is open in $ Y $. In this case, we have shown only three such neighborhoods. The (possibly infinite) union of all such neighborhoods is $ G $, which itself is open.}  
\end{figure}
\end{proof}
\begin{proposition}
	Suppose $ K\subseteq Y\subseteq X $, where $ Y $ and $ X $ are metric spaces. The subset $ K $ is compact in $ X $ \textit{if and only if} $ K $ is compact in $ Y $.  
\end{proposition}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item[$ (\Longrightarrow) $] Suppose $ K $ is compact in $ X $. Let $ \{V_\alpha\} $ be an arbitrary collection of open sets in $ Y $ which cover $ K $. By Lemma 2.1, there exist sets $ G_\alpha $, open in $ X $, such that $ V_\alpha=Y\cap G_\alpha $ for all $ \alpha $. By the compactness of $ K $, there exists a finite subcover $ \{G_{\alpha_1},\ldots,G_{\alpha_1}\} $. We have $$ K\subseteq\bigcup_{i=1}^n G_{\alpha_i} ,$$ but $ K\subseteq Y $, so $$ K\subseteq\bigcup_{i=1}^n V_{\alpha_i} . $$ This makes $ \{V_{\alpha_1},\ldots,V_{\alpha_1}\} $ a finite subcover, so $ K $ is compact in $ X $. 
		\item[$ (\Longleftarrow) $] Suppose $ K $ is compact in $ Y $. Let $ \{G_\alpha\} $ be an open cover of $ K $ in $ X $. If we let $ V_\alpha=G_\alpha\cap Y $, then $ \{V_\alpha\} $ is an open cover of $ K $ in $ Y $. By the compactness of $ K $ we have a finite subcover $ \{V_{\alpha_1},\ldots,V_{\alpha_n}\} $ in $ Y $. Since $ V_\alpha\subseteq G_\alpha $ for all $ \alpha $, then $ \{G_\alpha\} $ has a finite subcover in $ \{G_{\alpha_1},\ldots,G_{\alpha_n}\} $, so $ K $ is compact in $ X $. 
	\end{enumerate}
	
\end{proof}
This result does not seem particularly interesting, but it is novel if you consider open and closed sets. We gave several examples of sets that may be open or closed in some intermediate space, but not a larger space. For instance, $ \Z $ is open in $ \Z $, but it is not open in $ \R $. This result tells us that results like this are not possible with compactness!
\begin{example}
	Suppose $ E\subseteq \Z $ is compact in $ \Z $. This implies that $ E $ is compact in $ \Q $ and $ \R $. If we had another compact set $ F\subseteq\R $ which is compact in $ \R $, then it is compact in $ \Z $ and $ \Q $ as well. 
\end{example}

The next two theorems establish that all compact sets are both closed and bounded. This should feel somewhat natural, as compactness can be interpreted as a generalization of closed and bounded sets.
\begin{theorem}
	Let $ X $ be a metric space, and $ K\subseteq X $ be compact. The set $ K $ is closed. 
\end{theorem} 
\begin{proof}
Let $ K $ be a compact subset of a metric space $ X $. It suffices to show that $ K^c $ is open in $ X $. Suppose $ x\in K^c $, and $ y\in K $. For $ r<\frac{1}{2}d(x,y) $, let $ V_y=N_r(x) $ and $ W_y=N_r(y) $ (Figure 24). For our fixed $ x\in K^c $, we will repeat this process for multiple points in $ K $. By compactness, we know there exists a finite set of $ \{y_1,\ldots,y_n\} $ such that $ \{W_{y_1},\ldots,W_{y_n}\} $ is a finite subcover of $ K $. In constructing this subcover, we also constructed the corresponding sets $ \{V_{y_1},\ldots, V_{y_n}\} $ (Figure 23). If we let $ V=\cap_{i=1}^n V_{y_i} $, then we have $ x\in V\subseteq K^c $, making $ x $ an interior point of $ K^c $. Therefore $ K^c $ is open.
 \begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\filldraw[color=black, thick, fill=red!15] (0,0) rectangle (12,6);
	\filldraw[color=blue, thick, fill=blue!15] (3,3) circle (1.5cm);
	\draw[thick, red, dashed, thick]  (3,3) circle (1.5cm);
	\draw[blue] (3.5,4.7) node {$ K $};
	\draw (12.2,6.4) node {$ X $};
	\draw[red] (11,5) node {$ K^c $};
	\filldraw (4,3) circle (1pt) node[anchor= south east] {$ y $};
	\filldraw (8,3) circle (1pt) node[anchor= south east] {$ x $};
	\draw[thick, dashed] (4,3) circle (1cm);
	\draw[thick, dashed] (8,3) circle (1cm);
	\draw[thick, dashed] (4,3) -- (8,3);
	\draw[thick] (6,3.1)--(6,2.9);  
	\draw[black] (6,3.2) node {\tiny$ \frac{1}{2}d(x,y) $};
	\draw[black] (5,4) node {$ W_{y}$};
	\draw[black] (7,4) node {$ V_{y}$};
	\end{tikzpicture}
	\caption{For some points $ x\in K^c $ and $ y\in K $, we construct neighborhoods $ W_y=N_r(y)$ and $ V_y=N_r(x) $ such that $ r<\frac{1}{2}d(x,y) $. This choice of radius ensures $ V_y\cap W_y=\emptyset $.}  
\end{figure}
 \begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\filldraw[color=black, thick, fill=red!15] (0,0) rectangle (12,6);
	\filldraw[color=blue, thick, fill=blue!15] (3,3) circle (1.5cm);
	\draw[thick, red, dashed, thick]  (3,3) circle (1.5cm);
	\draw[blue] (3.5,4.7) node {$ K $};
	\draw (12.2,6.4) node {$ X $};
	\draw[red] (11,5) node {$ K^c $};
	\filldraw (4,3) circle (1pt) node[anchor= south east] {$ y_1 $};
	\filldraw (8,3) circle (1pt) node[anchor= south east] {$ x $};
	\draw[thick, dashed] (4,3) circle (1cm);
	\draw[thick, dashed] (8,3) circle (1cm);
		\draw[thick, dashed] (8,3) circle (2cm);
				\draw[thick, dashed] (8,3) circle (0.6cm);
		\draw[thick, dashed] (3.8,2) circle (0.6cm);
	\draw[thick, dashed] (8,3) circle (1.5cm);
	\filldraw (3,4) circle (1pt) node[anchor= south east] {$ y_2 $}; 
		\filldraw (2,3) circle (1pt) node[anchor= north east] {$ y_3 $}; 
		\draw[thick, dashed] (2,3) circle (2cm);
		\draw[thick, dashed] (3,4) circle (1.5cm);
		\filldraw (3.8,2) circle (1pt) node[anchor= north west] {$ y_4 $};
		\draw[black] (8.8,3) node {\tiny$ V_{y_4}$};
		\draw[black] (9.3,3) node {\tiny$ V_{y_1}$};
		\draw[black] (9.8,3) node {\tiny$ V_{y_2}$};
		\draw[black] (10.3,3) node {\tiny$ V_{y_3}$};
		\draw[black] (4,1.2) node {\tiny$ W_{y_4}$};
		\draw[black] (5.4,3) node {\tiny$ W_{y_1}$};
		\draw[black] (3,5.7) node {\tiny$ W_{y_2}$};
		\draw[black] (2,0.7) node {\tiny$ W_{y_3}$};
		
	\end{tikzpicture}
	\caption{For the fixed value $ x\in K^c $, repeat the process illustrated in Figure 21 until we have a finite subcover of $ K $, $ \{W_{y_1},\ldots,W_{y_4}\}.$ If we let $ V $ be the intersection of all $ V_{y_i} $, then $ x\in V\subseteq K^c $, rendering $x $ an interior point of $ K^c $.}  
\end{figure}
\end{proof}
\begin{example}
	The set $ \Q $ in $ \R $ is not closed (Example 2.18), so it is not compact in $ \R $. 
\end{example}

\begin{theorem}
	Let $ X $ be a metric space, and $ K\subseteq X $ be compact. The set $ K $ is bounded. 
\end{theorem}
\begin{proof}
Let $ x\in K $. The collection of neighborhoods $ \{N_r(x)\} $ for $ r\in\N $ forms an open cover of $ K $. By compactness, this open cover has a finite subcover $ \{N_{r_1}(x),\ldots,N_{r_n}\} $. If we take $ r^*=\max\{r_1,\ldots,r_n\} $, then $ K\subseteq N_{r^*}(x) $, and $ d(x,y)<r^* $ for all $ y\in K $. (Figure 24)
  \begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw[color=black, thick] (0,0) rectangle (12,8);
	\filldraw[thick, fill=gray!15] (6,4) circle (3.5cm);
	\draw (12.2,6.4) node {$ X $};
	\draw (6,5.4) node {$ K $};
	\filldraw (6,4) circle (1pt) node[anchor = south east] {$ x $};
	\draw[thick, dashed, red] (6,4) circle (1cm);
	\draw[thick, dashed, red] (6,4) circle (2cm);
	\draw[thick, dashed, red] (6,4) circle (3cm);
	\draw[thick, dashed, green] (6,4) circle (4cm);	
	\draw[red] (7.4,4) node {\footnotesize$ N_{1}(x) $};
	\draw[red] (8.1,5) node {\footnotesize$ N_{2}(x) $};
	\draw[red] (9,5.5) node {\footnotesize$ N_{3}(x) $};
	\draw[green] (10.4,6.5) node {\footnotesize$ N_{4}(x)=N_{r^*}(x) $};
	\end{tikzpicture}
	\caption{We cover the set $ K $ with an infinite open cover comprised of neighborhoods of radii in $ \N $. By compactness there is an open subcover, such as $ \{N_1(x),\ldots,N_4(x)\} $. The set is bounded by the maximum radii $ r^*=4 $ in this finite collection.}  
\end{figure}
\end{proof}
\begin{example}
The set $ (0,\infty) $ in $ \R $ is not bounded so it is not compact in $ \R $. 
\end{example}
\begin{remark}
	While compactness implies closed and bounded, the converse \textit{is not necessarily true}. Soon, we will see that the converse will hold in $ \R^n $, but in general, this is not the case. The next example illustrates this.
\end{remark}
\begin{example}
	Let $ X=\{1/n\mid n\in\N\} $ be a metric space equipped with $$ d(x,y)=\begin{cases}
	0\text{ if }x=y\\1\text{ if }x\neq y
	\end{cases}.$$
The set $ X $ is closed, as it is the whole space. It is also bounded, as $ d(x,y)\le 1 $ for all $ x,y\in X $. Let $ G_n=\{1/n\} $. Each $ G_n $ is open, as $ N_{1/2}(1/n)\subseteq G_n $. This makes $ G_n $ an open cover of $ X $, as $ X\subseteq\bigcup_{n\in \N} G_n=X $. The set $ X $ fails to be compact, because this open cover has no finite subcover. Any finite cover $ \{G_{1},\ldots,G_{N}\} $ would not ``cover''  $ \{1/(N+1),1/(N+2),\ldots\}\subseteq X $.	
\end{example}

We often restrict our attention to subsets of compact sets, so it would be nice to know if subsets of compact sets are compact. Unfortunately, this is not true in general, but it becomes true if we require the subset satisfy one condition. 
\begin{proposition}
	Closed subsets of compact sets are compact.
\end{proposition}
\begin{proof}
	Suppose $ K $ in $ X $ is compact, and $ F\subseteq K $ is closed in $ X $. Let $ \{V_\alpha\} $ be an arbitrary open cover of $ F $. If we add $ F^c $ to this collection of open sets, we have an open cover $ \Omega=\{V_{\alpha_1},V_{\alpha_2},\ldots,F^c\} $ of $ K $, because $ K=F\cup F^c $. $$ K\subseteq\left(\bigcup_\alpha V_\alpha\right)\cup F^c.$$ The set $ K $ is compact, so there exists a finite open cover of $ \Omega $, $ \Phi=\{V_{\alpha_1},\ldots,V_{\alpha_n}, F^c\} $. We can now remove $ F^c $ from $ \Phi $, resulting in a finite subcover for $ \{V_\alpha\} $. 
\end{proof}
\begin{corollary}
	If $ F $ is closed and $ K $ is compact, $ F\cap K $ is compact. 
\end{corollary}
An interesting property of compact sets is that if we have a decrease sequence of nested compact intervals, than their intersection is nonempty. This result follows as a corollary of a more general result.
\begin{proposition}
	If $ \{K_\alpha\} $ is a collection of compact subsets of a metric space $ X $ such that the intersection of every finite subcollection of $ \{K_\alpha\} $ is nonempty, then $ \cap_\alpha K_\alpha\neq\emptyset $. 
\end{proposition}
\begin{proof}
For the sake of contradiction, assume that $ \cap_\alpha K_\alpha=\emptyset $. This means there is some fixed $K_1\in \{K_\alpha\}$ such that no point of $ K_1 $ belongs to every $ K_\alpha $. Let $ G_\alpha= K_\alpha^c $. The collection $ \{G_\alpha\} $ forms an open cover of $ K_1$. Since $ K_1 $ is compact, there exists a finite subcover $ \{G_{\alpha_1},\ldots,G_{\alpha_n}\} $. $$ K\subseteq\bigcup_{i=1}^nG_{\alpha_i}$$ This inclusion, along with the definition of $ G_\alpha $ implies that $ K_1\cap\left(\bigcap_{i=1}^nK_{\alpha_i}\right)=\emptyset $. This contradicts our assumption that every finite subcollection of $ \{K_\alpha\} $ is nonempty. 
\end{proof}
\begin{corollary}[Cantor's Intersection Theorem]
If $ \{K_\alpha\} $ is a sequence of nonempty compact sets such $ K_{n}\supset K_{n+1} $ for $ n\in\N $, then $ \cap_{i=1}^\infty K_n $ is not empty.
\end{corollary}
\begin{example}
	Recall that open intervals in $ \R $ are not compact (Example 2.35), and do not satisfy the requirement of Cantor's Intersection Theorem. Let $ G_{n}=(0,1/n) $. We have $ G_{n+1}\subseteq G_{n} $ for all $ n\in\N $, so $ \{G_n\} $ is a decreasing nested sequence of intervals. $$ \cdots\subseteq \left(0,\frac{1}{4}\right)\subseteq\left(0,\frac{1}{3}\right)\subseteq\left(0,\frac{1}{4}\right)\subseteq\left(0,\frac{1}{2}\right)\subseteq(0,1)$$  
The intersection of these sets is empty.	
\end{example}
\begin{proposition}[Bolzano-Weierstrass Property]
	If $ E $ is an infinite subset of a compact set $ K $, then $ E $ has a limit point in $ K $. 
\end{proposition}
\begin{proof}
	For the sake of contradiction, assume that no point of $ K $ is a limit point of $ E $. For each $ x\in K $, there exists some $ r $ such that $ V_x=N_r(x)=\{x\}  $ if $ x\in E $, or $ V_x=N_r(x)=\emptyset $ if $ x\notin E $. The collection $ \{V_x\} $ is an open cover of $ E $. The set $ E $ is infinite, so we cannot find a finite subcover for $ \{V_x\} $, as each set is at most a singleton. Because $ E\subseteq K $, the same is true with respect to $ K $, which contradicts the assumption that $ K $ is compact. 
\end{proof}
Informally, the Bolzano-Weierstrass Property tells us that we can approximate some point of $ K $ with points in an infinite subset $ E $. Right now, this may not seem significant, but it will become important when we work in $ \R^n $. 
\begin{remark}
	The Bolzano-Weierstrass Property is often referred to as \textit{\color{red}limit point compactness}. In the context of metric spaces, limit point compactness and compactness are equivalent\footnote{Proving this requires some concepts that are beyond this treatment of metric spaces.}, so in most real analysis texts the prior is never even given a name. As we'll see \textit{much} later on, the two are not equivalent when we explore point-set topology in general (Section 15).   
\end{remark} 
\subsection{Compact Sets in $ \R^n $}
Now we can start working towards sufficient conditions for compactness in $ \R^n $. This will culminate in the famed Heine-Borel Theorem. This theorem establishes sufficient conditions for compactness in $ \R^n $. Specifically, the converses of Theorem 2.3 and Theorem 2.4 will hold in $ \R^n $.  

In order to make the proof of this result a bit more clear, we will show a series of results beforehand which build to the Heine-Borel Theorem.
\begin{lemma}
	If $ \{I_n\} $ is a sequence of closed intervals in $ \R $, such that $ I_n\supset I_{n+1} $  for $ n\in\N $, then $ \cap_{i=1}^n I_n\neq\emptyset$.
\end{lemma}  
		\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[very thick, <->] (-8,0) -- (8,0);
	\filldraw[fill=red, color=red] (-6,0)circle (2pt);
	\filldraw[fill=red, color=red] (6,0) circle (2pt);
	\draw[very thick, color = red] (-6,0)--(6,0);
	\draw[red] (-5,.3) node {$ I_1$};
		\filldraw[fill=blue, color=blue] (-4,0)circle (2pt);
	\filldraw[fill=blue, color=blue] (4,0) circle (2pt);
	\draw[very thick, color = blue] (-4,0)--(4,0);
	\draw[blue] (-3,.3) node {$ I_2$};
		\filldraw[fill=green, color=green] (-2,0)circle (2pt);
	\filldraw[fill=green, color=green] (2,0) circle (2pt);
	\draw[very thick, color = green] (-2,0)--(2,0);
	\draw[green] (-1,.3) node {$ I_3$};
	\end{tikzpicture}
	\caption{The first three intervals in the type of sequence $ \{I_n\} $ described in Proposition 2.7.}
\end{figure}
\begin{proof}
	Suppose $ I_n=[a_n,b_n] $. Define $ E $ to be the set of all the $ a_n $. The set $ E $ is nonempty and bounded above by $ b_1 $, because $$\cdots a_3\le a_2\le a_1\le b_1\le b_2\le b_3\le \cdots .$$ We have $ E\subseteq \R $ and is bounded, so it has a supremum in $ \R $. Let $ x=\sup E $. For all $ m,n\in\N $, $$a_n\le a_{m+n}\le b_{m+n}\le b_m ,$$ so $ x\le b_m $ for all $ m $. By the definition of $ \sup E $, $ a_m\le x $. If $ x\le b_m $ and $ a_m\le x $, then $ x\in[a_m,b_m]=I_m $ for all $ m\in\N $. Therefore $ x\in\cap_{n\in\N} I_n $.
\end{proof}
\begin{example}
	If we modify Example 2.42 so the open intervals are closed, then we have a sequence $ \{I_n\} $ where $ I_n=[0,1/n] $. This gives $ \cap_{n\in\N} I_n=\{0\}\neq\emptyset $. If we worked through the proof of Lemma 2.2 with this particular example, we would find that $ 0 $ is the supremum of the set of lower bounds of $ I_n $. 
\end{example}
This result should look familiar. Cantor's Intersection Theorem states a similar result for compact sets. This in and of itself does not show that closed intervals are compact, but it should catch our attention, as closed intervals and compact sets share a noteworthy property. It should also be noted that this property of closed intervals follows from the least-upper-bound property of $ \R $. This is one of the magical results we get because $ \R $ is complete.  We can generalize Proposition 2.7 to \textit{\color{red}$ k $-cells} in $ \R^k $. An  $ k $-cell is the set of all points $ \x=(x_1,\ldots,x_k)\in\R^k $ which satisfy $ a_i\le x_i\le b_i $ for $ i=1,\ldots,k $, where $ a_i,b_i\in\R $, and $ a_i\le b_i $. 
\begin{lemma}
	Let $ k\in\N $. If $ \{I_n\} $ is a sequence of $ k- $cells such that $ I_n\supset I_{n+1} $ for all $ n\in\N $, then $ \cap_{n\in\N} I_n\neq\emptyset $. 
\end{lemma}
		\begin{figure}[h]
	\centering
	\begin{tikzpicture}
\draw[thick] (0,0) rectangle (7,9);
\draw (7.3,9.3) node {$ I_1$};
\draw[thick] (0.5,1) rectangle (6,8);
\draw (6.3,8.3) node {$ I_2$};
\draw[thick] (2,2) rectangle (5.5,5);
\draw (5.8,5.3) node {$ I_3$};
\draw[thick] (3,3) rectangle (4,4);
\draw (4.3,4.3) node {$ I_4$};
	\end{tikzpicture}
	\caption{The first four $ 2- $cells in the type of sequence $ \{I_n\} $ described in Proposition 2.8.}
\end{figure}
\begin{proof}
	Let $ I_n $ be the set of points $ \x=(x_1,\ldots,x_k) $ such that $ a_{n,j}\le x_j\le b_{n,j} $ for $ j=1,\ldots, k $ and $ n\in\N $, and write $ I_{n,j}=[a_{n,j},a_{b,j}] $. By Lemma 2.2, for each $ I_{n,j} $, $ \cap_{n\in\N}I_{n,j}\neq\emptyset $, so there is some $ x_j^*\in\cap_{n\in\N} I_{n,j} $ which satisfies $$ a_{n,j}\le x_j^*\le b_{n,j}$$ for $j=1,\ldots, k $ and $ n\in\N $. If we set $ \x^*=(x_1^*,\ldots,x_k^*) $, then $ \x^*\in I_n $ for all $ n\in\N $. Therefore  $ \cap_{n\in\N} I_n\neq\emptyset $. 
\end{proof}
We will now prove that each $ k -$cell is compact. The definition of a $ k -$cell is equivalent to a closed and bounded set in $ \R $, so this result will give us our sufficient conditions for compactness in $ \R^n $. This result will give rise to the Heine-Borel Theorem which is an equivalence result, which will follow immediately from the compactness of $ k $-cells, Theorem 2.3, and Theorem 2.4. That being said, the proof that each $ k -$cell is compact is not immediate, and is on the more difficult side for a proof in an introductory analysis course.    
\begin{lemma}
	Every $ k -$cell is compact. 
\end{lemma}
\begin{proof}
	Let $ I=\{\x\in\R^k\mid a_j\le x_j\le b_j\ j=1,\ldots,k\} $ be a $ k- $cell. Let $ \delta $ be the maximum distance between any two points in $ I $.
	$$ \max\limits_{\x,\y\in I}d(\x,\y)=\delta=\left(\sum_{j=1}^k(b_j-a_j)^2\right)^{1/2}$$
	For all $ \x,\y\in I $, $ |\x-\y|\le\delta $ (Figure 27 shows this for $ k=2 $). 
			\begin{figure}[h!]
		\centering
		\begin{tikzpicture}
	
		\draw[step=1cm,gray!15,very thin] (0,0) grid (8,9);
		\filldraw[thick, fill = gray!15] (1,1) rectangle (7,8);
		\draw (7.3,8) node {$ I$};	
		\draw[thick,->] (0,0) -- (0,9);
		\draw[thick,->] (0,0) -- (8,0);
		\draw[thick] (1,0.2) -- (1,-0.2);
		\draw[thick] (7,0.2) -- (7,-0.2);
		\draw[thick] (-0.2,1) -- (0.2,1);
		\draw[thick] (-0.2,8) -- (0.2,8);
		\draw (1,-0.4) node {$ a_1$};	
		\draw (7,-0.4) node {$ b_1$};
		\draw (-0.4,1) node {$ a_2$};
		\draw (-0.4,8) node {$ b_2$};
		\draw[thick, dashed] (1,8) -- (7,1) ;
		\draw (4,4) node {$ \delta $};
		\end{tikzpicture}
		\caption{If $ I $ is a 2-cell.}
	\end{figure}

For the sake of contradiction, assume that there exists some arbitrary open cover $ \{G_\alpha\} $ of $ I $ which contains no finite subcover of $ I $. Let $ c_j=(a_j+b_j)/2 $. The intervals $ [a_j,c_j] $ and $ [c_j,b_j] $ give rise to $ 2^k $ $ k $-cells $ Q_i $ whose union is $ I $. If each of these cells $ Q_i $ could be covered by a finite subcollection of $ \{G_\alpha\} $, then $ I $ could be covered by the union of all these finite subcollections, which is finite. Since we've assumed $ K $ is not compact, then it must be that there exists at least one $ Q_i $, call it $ I_1 $, that cannot be covered by any finite subcollection of $ \{G_\alpha\} $.  
	\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	
	\draw[step=1cm,gray!15,very thin] (0,0) grid (8,9);
	\filldraw[thick, fill = gray!15] (1,1) rectangle (7,8);
	\draw (7.3,8) node {$ I$};	
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (8,0);
	\draw[thick] (1,0.2) -- (1,-0.2);
	\draw[thick] (7,0.2) -- (7,-0.2);
	\draw[thick] (-0.2,1) -- (0.2,1);
	\draw[thick] (-0.2,8) -- (0.2,8);
	\draw (1,-0.4) node {$ a_1$};
	\draw (4,-0.4) node {$ c_1$};	
	\draw (7,-0.4) node {$ b_1$};
	\draw (-0.4,1) node {$ a_2$};
	\draw (-0.4,8) node {$ b_2$};
	\draw (-0.4,4.5) node {$ c_2$};
	\draw[thick] (-0.2,4.5) -- (0.2,4.5);
	\draw[thick] (4,0.2) -- (4,-0.2);
	\draw[thick] (1,4.5)-- (7,4.5);
	\draw[thick] (4,1)-- (4,8);
	\draw (2.5,3) node {$ Q_3$};
	\draw (5.5,3) node {$ Q_4$};
	\draw (2.5,6) node {$ Q_1=I_1$};
	\draw (5.5,6) node {$ Q_2$};
	\end{tikzpicture}
	\caption{We partition $ I $ into $ 2^k $ cells $ Q_i $. If $ I $ is not compact, then there is some $ Q_i $ which is not compact. Suppose in this case $ Q_1 $ is not compact, and call it $ I_1 $. }
\end{figure}
Now we divide $ I_1 $ into $ 2^k $ $ k -$cells and repeat this process indefinitely, giving us a sequence $ \{I_n\} $ (Figure 29). 
	\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	
	\draw[step=1cm,gray!15,very thin] (0,0) grid (8,9);
	\filldraw[thick, fill = gray!15] (1,1) rectangle (7,8);
	\draw (7.3,8) node {$ I$};	
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (8,0);
	\draw[thick] (1,0.2) -- (1,-0.2);
	\draw[thick] (7,0.2) -- (7,-0.2);
	\draw[thick] (-0.2,1) -- (0.2,1);
	\draw[thick] (-0.2,8) -- (0.2,8);

	\draw[thick, dashed] (1,8) -- (7,1) ;

	\draw (4,4) node {$ \delta $};
	\draw[thick] (-0.2,4.5) -- (0.2,4.5);
	\draw[thick] (4,0.2) -- (4,-0.2);
	\filldraw[thick, fill = gray!15] (1,4.5) rectangle (4,8);	
	\draw[thick, dashed, red] (1,8) -- (4,4.5) ;
	\filldraw[thick, fill = gray!15] (1,6.25) rectangle (2.5,8);
	\draw[thick, dashed, blue] (1,8) -- (2.5,6.25) ;
	\filldraw[thick, fill = gray!15](1,7.125) rectangle (1.75,8);
	\draw[thick, dashed, green] (1,8) -- (1.75,7.125) ;
	\draw (4.3,7.7) node {$ I_1$};
	\draw (2.8,7.7) node {$ I_2$};
	\draw (2.05,7.7) node {$ I_3$};
	\draw[red] (3,5) node {$ \delta/2 $};
\draw[blue] (1.5,6.7) node {$ \delta/4 $};
\draw[green] (1.25,7.3) node {\scriptsize$ \delta/8 $};
	\end{tikzpicture}
	\caption{The first three $ 2 $-cells in the sequence $ \{I_n\} $.}
\end{figure}
This sequence of $ k $-cells was constructed to have three properties:
\begin{enumerate}
	\item $ I\supset I_1\supset I_2\supset I_3\supset\cdots $
	\item $ I_n $ cannot be covered by any finite subcollection of $ \{G_\alpha\} $, otherwise $ K $ would not be compact.\footnote{This follows from the same reasoning applied to the $ 2^k $ $ k $-cells $ Q_i $ we initially chose $ I_1 $ from.}
	\item If $ \x,\y\in I_n $, then $ |\x-\y|\le \delta/2^n $.\footnote{When we divided $ I $, the length of the diagonal for $ I_1 $ became half of $ \delta $. When we divide $ I_1 $, the length of the diagonal of $ I_2 $ became half of that of $ I_1 $. This means we can always write the diagonal of $ I_n $ in terms of powers of $ 1/2 $ and $ \delta $.  }
\end{enumerate}
By the first property of this sequence, we can invoke Lemma 2.3 to determine $ \cap_{n\in\N} I_n\neq\emptyset $. This means there exists some $ \x^*\in\R^k $ such that $ \x^*\in I_n $ \textit{for all} $ n\in\N $. There must be some $ \alpha $ such that $ \x^*\in G_\alpha $, otherwise $ \{G_\alpha\} $ would not be an open cover of $ I $. Since $ G_\alpha $ is open, there exists some $ r>0 $ such that $ N_r(x)\subseteq G_\alpha $. Alternatively, we could say that $ |\y-\x^*|<r $ implies $ y\in G_\alpha $ by the definition of $ N_r(x) $. If we take $ n $ to be so large that $ \delta/2^n<r $,\footnote{We can always find such an $ n $. If not, $ 2^n\le\delta /r $ for all $ n \in\N$. This can't be though because $ \R $ has the Archimedean property (Theorem 1.3).} then $ I_n\subseteq G_\alpha $, but this would mean $ I_n $ has a finite subcover. This contradicts property $ 2 $ of our sequence $ \{I_n\} $, thereby contradicting the assumption that $ K $ is not compact. 
\end{proof}
\begin{example}
	In order to make this proof a bit more concrete we'll walk through it with the $ 2 $-cell define by $ I=[0,1]\times[0,1] $, and a specific open cover.\footnote{In order to give such an example, we need to specify an open cover to work with. In doing so, we're sort of shooting ourselves in the foot. The whole point of compactness is that \textit{every} open cover has a finite subcover. What we're really proving in this example, is this specific open cover has no finite subcover.} Let our open cover $ G_\alpha=N_{0.01}(\alpha)  $ for $ \alpha\in I $. $$ I\subseteq \bigcup_{\alpha\in I} G_\alpha=\bigcup_{\alpha\in I}N_{0.01}(\alpha)$$  Assume that this open cover has no finite subcover. We have $$ \delta=((1-0)^2+(1-0)^2)^{1/2}=\sqrt{2} .$$ We divide $ I $ into four $ 2 $-cells: $ Q_1=[0,1/2]\times[0,1/2] $, $Q_2=[0,1/2]\times[1/2,1]$, $ Q_3=[0.5,1]\times[0,1/2] $, and $ Q_4=[1/2,1]\times[1/2,1] $. If $ \{G_\alpha\} $ has no finite subcover for $ I $, then the same can be said for one of these $ Q_i $. Suppose this is the case for $ Q_1 $, and let $ I_1=Q_1=[0,1/2]\times[0,1/2] $. Repeat this process seven times until we arrive at $ I_7=[0,1/256]\times[0,1/256] $. The maximum distance between any two points in $ I_7 $ is $$ \left((1/256-0)^2+(1/256-0)^2\right)^{1/2}\approx0.0055<0.01.$$ Therefore, we can cover $ I_7 $ with a single $ N_{0.01}(\alpha) \in\{G_\alpha\}$ for any $ \alpha\in I_7 $. But this means we can cover $ I_6 $ with four elements in $ \{G_\alpha\} $,\footnote{I'm playing a little fast and loose here with which exact elements, because I'm not specifying where the neighborhoods are centered.}  and cover $ I_5 $ with $ 4^2 $ elements in $ \{G_\alpha\}  $, etc. We can cover $ I $ with $ 4^7 $ elements in $ \{G_\alpha\} $. This contradicts the assumption that $ \{G_\alpha\} $ has no finite subcover. 
	
	One of the subtler, but nevertheless important, parts of this process is that we could repeatedly divide the cells until we found a cell that fit in a neighborhood of radius $ 0.01 $. We will always be able to do this because of the Archimedean property as discussed in Footnote 22. It is important, that you do not associate this particular ``trick'' with the fact that $ I $ is a subset of $ \R^2 $. We are discussing the Archimedean property in the context of distances and radii, so we're actually using the fact that $ \R^2  $ is equipped with a distance function $ d:\R^2\times\R^2\to[0,\infty] $. We're using the fact that the codomain of $ d $ has the Archimedean property!  
\end{example}
We have now shown that every $ k $-cell is compact. The proof is made more manageable with illustrations, but is still rather technical. Our contradiction came from the fact that as $ n $ becomes large, $ I_n $ becomes small. Eventually $ I_n $ will be so small, that it must have a finite open subcover. This contradicts the assumption that $ I $ is not compact. 
\begin{theorem}[Heine-Borel Theorem]
	A set $ E $ in $ \R^n $ is compact \textit{if and only if} it is closed and bounded.
\end{theorem} 
\begin{proof}{\color{white}space}
\begin{enumerate}
	\item [$ (\Longrightarrow) $] All compact sets are closed and bounded by Theorems 2.3 and 2.4. 
	\item [$ (\Longleftarrow) $] If $ E $ is closed and bounded then $ E\subseteq I $ for some $ n -$cell. Any closed subset of a compact set is compact by Proposition 2.4, so $ E $ is compact.
\end{enumerate}
\end{proof}
\begin{example}
	Any closed interval $ [a,b]\subseteq \R$ is compact. 
\end{example}
\begin{theorem}[Bolzano–Weierstrass Theorem]
	Every bounded infinite subset of $ \R^n $ has a limit point in $ R^k $. 
\end{theorem}
\begin{proof}
	Let $ E\subseteq\R^n $ be bounded and infinite. There is some $ n $-cell $ I\subseteq\R^k $, such that $ E\subseteq I $. By Lemma 2.4 $ I $ is compact. We know apply Proposition 2.6 to conclude that $ E $ has a limit point in $ I $, which is also in $ \R^k $.
\end{proof}
\begin{example}
	The set $ (a,b)\subseteq\R^n $ is infinite and bounded. It has an infinite number of limit points in $ \R^n $,\footnote{In fact, the set of limit points is $ [a,b]\subseteq\R^n $} including $ a $ and $ b $.
\end{example}
\subsection{Exercises}
\begin{ex}
	Show that the set of all algebraic numbers is countably infinite.
\end{ex}
\begin{ex}
	Show that the set of all binary numbers with infinite digits is uncountably infinite.
\end{ex}
\begin{ex}
	Verify that the taxi-cab metric on $ \R^n $ is a valid metric space.
\end{ex}
\begin{ex}
	Let $ X $ be an infinite set with the metric, $$d(x,y)=\begin{cases}
		1\text{ if }x\neq y\\
	0\text{ if }x=y
	\end{cases} .$$ Prove that $ (X,d) $ is a metric space. Which subsets of $ X $ are open? Which are closed? 
\end{ex}
\begin{ex}
	Prove that $ E^\circ $ is open.
\end{ex}
\begin{ex}
	Prove that $ E $ is open \textit{if and only if} $ E=E^\circ $.
\end{ex}
\begin{ex}
	If $ G\subseteq E $ and $ G $ is open, prove that $ G\subseteq E^\circ $
\end{ex}
\begin{ex}
	Prove that $ (E^\circ)^c=\overline{E^c} $.
\end{ex}
\begin{ex}
	Find an example of a set $ E  $ in a metric space such that $ E^\circ\neq(\bar{E} )^\circ$.
\end{ex}
\begin{ex}
	Find an example of a set $ E  $ in a metric space such that $ \bar{E}\neq\overline{E^\circ}$.
\end{ex}
\begin{ex}
	Prove that $ \partial E $ is closed.
\end{ex}
\begin{ex}
	Prove that $ \partial(E^\circ)\subseteq \partial E  $, and $ \partial(\bar{E})\subseteq \partial E  $
\end{ex}
\begin{ex}
	Prove that $ \partial E=\partial (E^c)  $.
\end{ex}
\begin{ex}
	Suppose $ E $ is closed. Show that $ (\partial E)^\circ=\emptyset $. 
\end{ex}
\begin{ex}
	Prove that $ \partial (\partial E)\subseteq \partial E $. When will the sets be equal? 
\end{ex}
\begin{ex}
	Prove that $ E $ is closed \textit{if and only if} $ E\cap \partial E=\emptyset $.
\end{ex}
\begin{ex}
	Prove that $ \partial E=\emptyset$ \textit{if and only if} $ E $ is closed and open.  
\end{ex}
\begin{ex}
	Prove that $ \bar{E}=E\cup\partial E $.
\end{ex}
\begin{ex}
	Prove that $ (\partial \bar{E})^\circ=\emptyset $. 
\end{ex}
\newpage
\section{Sequences and Series}
Now that we are intimately familiar with the behavior of metric spaces, we can discuss a topic that may be familiar from calculus -- sequences and series. Metric spaces will allow us to rigorously define convergence, and the properties related to the convergence os sequences and series. While we will derive some results and examples in general metric spaces, we will also start introducing results specific to $ \R $.       
\subsection{Convergence}
\begin{definition}
	Let $ X $ be a metric space. A \textit{\color{red}sequence $ \{x_n\} $} is an function from $ f:\N\to X $. We will sometimes refer to an entire sequence as $  x_n=f(n)$. 
\end{definition}
Using an arbitrary metric space $ X $ in this definition means that we, once again, always need to pay attention to what metric space we are in. We saw this with open sets, closed sets, and compact sets, and we will see it again. It will become especially relevant when determining if sequences converge. 
\begin{example}
	Let $ x_n=1/n$ be a sequence in $ \R $. The first several terms in this sequence are $$ 1,\frac{1}{2},\frac{1}{3},\frac{1}{4},\ldots.$$
	This set is also a sequence in $ \Q $. This sequence is not defined in $ \Z $ or $ \N $, as neither of these sets has fractions.  
\end{example}
\begin{example}
 Let $ x_n=2 $ be a sequence in $ \R $. This constant sequence always takes on the value $ 2$. This sequence is also a sequence in $ \N,\Z $, and $ \Q $.
\end{example}
\begin{example}
Let $ x_n=(-1)^n $ be a sequence in $ \R $. This sequence alternates between $ -1 $ and $ 1 $ for all values in $ \N $.  
\end{example}

Now we are ready to formalize what it means for a sequence to converge. When the idea of convergence is first introduced, you often hear phrases like ``arbitrarily close''. If a sequence converges to some point $ x\in X $, we can \textit{always} get closer to $ x $. For any value in $ \{x_n\}$, we can find other points ``later'' in the sequence $ \{x_n\} $ that is even closer. If convergence is a recipe, then these are the ingredients:  
\begin{enumerate}
	\item No matter how ``close'' we get, we can always get closer with another point in $ \{x_n\} $. Fortunately, we're in a metric space $ (X,d) $, so we can use $ d $ to determine how close we are.
	\item Well actually, it cannot be \textit{any} other points ``later'' in $ \{x_n\} $. For instance, suppose we have the following sequence: $$ 1, \frac{1}{2}, 2, \frac{1}{3}, 3, \frac{1}{4}, 4,\ldots . $$ The even terms of this series are getting closer to $ 0 $, while the odd terms are growing. The latter fact means this sequence doesn't converge. This happens because not \textit{all} the points ``later'' in $ \{x_n\} $ are closer.  
\end{enumerate} 
These two ingredients will correspond to the $ \varepsilon $ and $ N $ in our definition.
\begin{definition}
	A sequence $ \{x_n\} $ in a metric space $ X $ \textit{\color{red}converges (in $ X $)} if there exists an $ x\in X $ such that \textit{for all }$ \varepsilon>0 $, there is an $ N\in\N $ such that $ d(x_n,x)<\varepsilon $ \textit{for all} $ n\ge N $. We will call $ x $ the \textit{\color{red}limit} of $ \{x_n\} $, and write either $ x_n\to x $, or $$\lim\limits_{n\to\infty} x_n=x. $$
\end{definition}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \N$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick,dashed] (0,4.5) -- (16,4.5);
	\draw (-.3,4.5) node {$ x$};
		\draw[red] (5,1) node {$ n<N$};
		\draw[green] (12,1) node {$ n\ge N$};
	\draw[thick,dashed] (0,5) -- (16,5);
	\draw[thick,dashed] (0,4) -- (16,4);
	\draw (-.5,4) node {$ x-\varepsilon$};
	\draw (-.5,5) node {$ x+\varepsilon$};
	\filldraw[red] (0.4,1) circle (2pt);
	\filldraw[red] (0.8,8) circle (2pt);
	\filldraw[red] (1.2,1.5) circle (2pt);
	\filldraw[red] (1.6,7.5) circle (2pt);
	\filldraw[red] (2.0,2) circle (2pt);
	\filldraw[red] (2.4,7) circle (2pt);
	\filldraw[red] (2.8,2.2) circle (2pt);
	\filldraw[red] (3.2,6.8) circle (2pt);
	\filldraw[red] (3.6,2.4) circle (2pt);
	
	\filldraw[red] (4,6.6) circle (2pt);
	\filldraw[red] (4.4,2.6) circle (2pt);
	\filldraw[red] (4.8,6.4) circle (2pt);
	\filldraw[red] (5.2,2.8) circle (2pt);
	\filldraw[red] (5.6,6.2) circle (2pt);
	
	\filldraw[red] (6,3.0) circle (2pt);
	\filldraw[red] (6.4,6) circle (2pt);
	\filldraw[red] (6.8,3.2) circle (2pt);
	\filldraw[red] (7.2,5.8) circle (2pt);
	\filldraw[red] (7.6,3.4) circle (2pt);
	\filldraw[red] (8,5.6) circle (2pt);
	
	\filldraw[red] (8.4,3.6) circle (2pt);
	\filldraw[red] (8.8,5.4) circle (2pt);
	\filldraw[red] (9.2,3.8) circle (2pt);
	\filldraw[red] (9.6,5.2) circle (2pt);
	\filldraw[red] (10,4) circle (2pt);
	\filldraw[red] (10.4,5) circle (2pt);				
	\draw[thick,dashed] (10.8,0) -- (10.8,9);
	\draw (10.8,-0.3) node {$ N$};			
	\filldraw[green] (10.8,4.2) circle (2pt);
	\filldraw[green] (11.2,4.8) circle (2pt);
	\filldraw[green] (11.6,4.4) circle (2pt);
	\filldraw[green] (12,4.6) circle (2pt);
	\filldraw[green] (12.4,4.6) circle (2pt);
	\filldraw[green] (12.8,4.4) circle (2pt);
	\filldraw[green] (13.2,4.4) circle (2pt);
	\filldraw[green] (13.6,4.6) circle (2pt);
	\filldraw[green] (14,4.6) circle (2pt);
	\filldraw[green] (14.4,4.4) circle (2pt);	
	\filldraw[green] (14.8,4.45) circle (2pt);
	\filldraw[green] (15.2,4.55) circle (2pt);
	\filldraw[green] (15.6,4.475) circle (2pt);
	\filldraw[green] (16,4.525) circle (2pt);


	\end{tikzpicture}
	\caption{A convergent sequence $ \{x_n\} $ in $ \R $. No matter how small we take $ \varepsilon $ to be, we can always find some $ N $ such that all $ d(x_n,x)=|x_n-x|<\varepsilon $ for all $ n\ge N $. We could also write $ x_n\in N_\varepsilon(x)=(x-\varepsilon,x+\varepsilon) $. }
\end{figure}
We can think of the convergence of a sequence in the context of a hypothetical game. Suppose you and a friend have some convergent sequence $ \{x_n\} $ in $ X $. Your friend says some small number $ \varepsilon $, and challenges you to find an $ N $ such that $ d(x_n,x)<\varepsilon $ for all $ n\ge N $. By the definition of convergence, you can do this. This frustrates your friend, so he demands you do it for an even smaller value of $ \varepsilon $. Unfortunately for him, you will always be able to find such an $ N $. No matter how small $ \varepsilon $ is, you will be able to do this. 
\begin{remark}
	We can formulate an equivalent definition of convergence using neighborhoods. If for all  $ \varepsilon>0 $, $ d(x_n,x)<\varepsilon $ whenever $ n\ge N $, then we could also say $ x_n\in N_\varepsilon(x) $ for all $ n\ge N $. 
\end{remark}

\begin{example}
	The sequence $ x_n=1/n $ converges to $ 0 $ in $ \R $. Suppose your friend lets $ \varepsilon=0.01 $, and asks you to find an $ N\in\N $ such that $$ d(1/n,0)=|1/n-0|=1/n<\varepsilon=0.01 $$ for all $ n\ge N $. If you let $ N=101 $, then you have done this. \begin{align*}
		1/101&<0.01\\1/102&<0.01\\1/103&<0.01\\&\vdots
	\end{align*} 
	Your friend then gives you $ \varepsilon=0.001 $. In this case, let $ N=1001 $.
	\begin{align*}
	1/1001&<0.01\\1/1002&<0.01\\1/1003&<0.01\\&\vdots
	\end{align*} 
	You're already bored of this game, so you get an idea. Maybe you can find some function of $ \varepsilon $ that will give you your value of $ N $. In order to do this, you just manipulate the inequality you must satisfy.
	\begin{align*}
		d(1/n,0)&<\varepsilon\\|1/n-0|&<\varepsilon\\1/n&<\varepsilon\\n&>\varepsilon^{-1}		
	\end{align*}
	We just let $ N=\varepsilon^{-1}+1 $. This way, for all $ n\ge N $, we have $ n>\varepsilon^{-1} $, which implies $ d(1/n,0)<\varepsilon $. 
\end{example}
\begin{remark}
	The definition of convergence has two inequalities. The inequality $ d(x_n,x)<\varepsilon $ is strict, while $ n\ge N $ is not. In the grand scheme of things, it doesn't matter if these are strict or not. If we instead had $ d(x_n,x)\le\varepsilon $, then we could just have taken $ N=\varepsilon^{-1} $ in the previous example. Alternatively, if we had $ d(x_n,x)<\varepsilon $ and $ n>N $, then $ N=\varepsilon^{-1} $ would work as well. I'm going to try very hard to stick with the inequalities in the definition, but I may make a mistake. Just know that it doesn't change the results of proofs at all. It does mean we need to be a little careful when using neighborhoods though, because they are open set.   
\end{remark}
\begin{example}
	We can verify that the sequence $ x_n=(n+1)/(n-1) $ converges to $ 1 $ in $ \R $. Let $ \varepsilon>0 $. We want to find the value of $ N $ in terms of $ \varepsilon $ that satisfies $ d(x_n,x)<\varepsilon $ for all $ n\ge N $. First notice that $$d(x_n,x)=\left\lvert\frac{n-1}{n+1}-1\right\rvert=\left\lvert\frac{-2}{n+1}\right\rvert=\frac{2}{n+1} .$$
	We can satisfy $ \frac{2}{n+1}<\varepsilon $ with $ N=2/\varepsilon-1 $.\footnote{You may be thinking that it isn't always true that this $ N $ will be in $ \N $. That's fine. We could just round the answer to get a whole number that satisfies the inequality. Generally, we're not too worried about this.} We have that $ d(x_n,x)<\varepsilon $ for all $ n\ge 2/\varepsilon-1 $. 
\end{example}
\begin{example}
	Let $ \Z $ be a metric space equipped with the $ p $-adic metric (see Example 2.4). The sequence $ x_n=p^n $ converges to $ 0 $ for any prime $ p $. We have $$d(x_n,0)=p^{-\max\{ m\in\N\ |\ p^m\mid (p^n-0) \}}=p^{-n} .$$ If we let $ N=\ln(2/\varepsilon)/\ln(p) $, then for all $ \varepsilon>0 $ we have $$ d(x_n,0)\le d(x_N,0)=p^{-\frac{\ln(2/\varepsilon)}{\ln(p)}}=p^{\frac{\ln(2/\varepsilon)}{\ln(p)}}=p^{\log_p(2/\varepsilon)}=\frac{2}{\varepsilon}<\varepsilon$$ for all $ n\ge N $. Let's let $ p=2 $. The sequence $ x_n=2^n $ will converge in $ \Z $ with the 2-adic metric. $$ \{2^n\}=\{2,4,8,16,32,64,128,\ldots\}$$  
	\begin{center}
		\begin{tabular}{ccc}
			$ n $ & $ x_n $ & $ d(x_n,0) $ \\ \hline
			1 & 2     &  $ 1/2 $         \\
			2&   4   &   $ 1/4 $        \\
			3&    8  &   $ 1/8 $        \\
			4&     16 &   $ 1/16 $        \\
			5&      32&  $ 1/32 $          \\
			6&      64&       $ 1/64 $   
		\end{tabular}
	\end{center}

\end{example}
\begin{example}
	The series $ x_n=(1+1/n)^n $ converges to $ e $ in $ \R $. This series does not converge in $ \Q $, because $ e\notin\Q $. 
\end{example}
\begin{example}
	Let $ X=(0,1] $ be equipped with the Euclidean metric. The sequence $ x_n=1/n $ does not converge in $ X $, because $ 0\notin X $. 
\end{example}
\begin{example}
	Let $ x_n=c $ for some constant $ c\in\R $. This type of sequence is often called a \textit{\color{red}constant sequence}. All constant sequences converge to $ c $ in $ \R $, as $$ |x_n-c|=|c-c|=0<\varepsilon $$ for all $ \varepsilon $. Constant sequences are the only sequences that converge in $ \Z $ equipped with the Euclidean metric.
\end{example}
\begin{remark}[Where's My Limit?!]
	These last two examples really emphasize the fact that the limit must be in the same metric space as our sequence. If our sequence is defined by some $ f:\N\to X $, we need $ x\in X $! Note that this is different from requiring that the value $ x $ is actually realized by our sequence. That is, it needn't be the case that $ f $ is in the image of $ X $. We just need it to be in the codomain of $ X $. In Example 3.4, $ 0\notin f(\N) $ (the image/range of $ \N $), but it is in $ \R $, and that's all that matters. 
\end{remark}
\subsection{Properties Related to Convergence}
Now we'll cover some basic properties of sequences and convergence. We will also prove several results that are specific to sequences in $ \R $, many of which should be familiar from calculus. 
\begin{proposition}
	Let $ \{x_n\} $ be a sequence in a metric space $ X $. The sequence $ \{x_n\} $ converges to $ x\in X $ \textit{if and only if} every neighborhood of $ p $ contains $ x_n $ for all but finitely many $ n $.  
\end{proposition}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ x_n\to\ x $, and let $ N_r(x) $ be some neighborhood of $ x $. For some $ \varepsilon=r>0 $, $ x_n\in N_r(x) $ for all $ n\ge N $ (see Remark 3.1). Therefore $ N_r(x) $ contains $ x_n $ for all but finitely $ n $, those being $ \{x_1,\cdots x_{N-1}\} $.  
		\item [$ (\Longleftarrow) $] Suppose every neighborhood of $ x $ contains all but finitely many $ x_n $. Fix $ \varepsilon >0 $, and observe $ N_\varepsilon(x) $. By our assumption, there exists an $ N $ such that $ x_n\in N_\varepsilon(x) $ for $ n\ge N $. Therefore we have $ d(x_n,x)<\varepsilon $ if $ n\ge N $, so $ x_n\to x $.   
	\end{enumerate}
\end{proof}
\begin{remark}[Limit vs. Limit Point]
	This result implies that if $ x_n\to x $, then $ x $ is a limit point of the range of $ x_n $. The converse is not necessarily true. Take the sequence $ x_n=(-1)^n(1+1/n) $ as an example. $$-2, \frac{3}{2}, -\frac{4}{3},\frac{5}{4},-\frac{6}{5},\ldots $$ The range of this sequence has two limit points, $ \{-1,1\} $. Neither of these are limits of the sequence, as this sequences fails to converge because it alternates. 
\end{remark}
\begin{example}
Let $ x_n=1/n $ be a sequence in $ \R $. Example 3.4 showed that $ x_n\to 0 $. For the neighborhood $ N_{0.01}(0) $ we have: $$ N_{0.01}(0)=\left\{\frac{1}{101},\frac{1}{102},\frac{1}{103},\ldots\right\}, $$ where our finite set of points not in $ N_{0.01}(0) $ is $$\{x_n\}\backslash N_{0.01}(0)=\left\{1,\frac{1}{2},\ldots, \frac{1}{100}\right\}.$$
\end{example}
\begin{proposition}[Uniqueness of Limits]
	Let $ \{x_n\} $ be a sequence in a metric space $ X $. If $ x,x'\in X $, and if $ \{x_n\} $ converges to $ x $ and $ x' $, then $ x=x' $.
\end{proposition}
\begin{remark}[Playing with $ \varepsilon $]
	Before we prove this, we should highlight a ``trick'' we will use. It is the most common technique used in proofs involving $ \varepsilon $. If we know for all $ \varepsilon>0 $, $ d(x_n,x)<\varepsilon $ for all $ n\ge N $, then we can use \textit{any} $ \varepsilon\in(0,\infty] $. This means we have $ d(x_n,x)<f(\varepsilon) $ for all $ n\ge N $, where $ f:(0,\infty]\to(0,\infty] $. Return to the hypothetical game you are playing with your friend. At first, your friend wants to let $ \varepsilon=0.01 $, but then he thinks ``no that's too big, let me cut it in half'', and he uses $ \varepsilon/2=0.005 $. The number $ \varepsilon/2>0 $ so it still is valid. He could even say ``let me square it, divide it by 4, and then add $ \pi $''. In this case $ f:(0,\infty]\to(0,\infty] $ is define as $ f(\varepsilon)=\varepsilon^2/4+\pi $, and is still valid because $ f(\varepsilon)>0 $. 
	
	Why would we want to do this? We may want to show something converges, and do so using inequalities we already know that involve $ \varepsilon $. We want our end result to show that $ d(x_n,x)<\varepsilon $, so we need to choose our initial inequalities involving $ \varepsilon $ such that they yield a single $ \varepsilon $. This probably isn't too clear right now, but this next proof, and that for Theorem 3.2 will hopefully make this more clear.  
\end{remark}
\begin{proof}
Assume that $ x_n\to x $, and $ x_n\to x' $. Let $ \varepsilon >0 $. There exists $ N',N''\in\N $ such that \begin{align*}
	n\ge N&\implies d(x_n,x)<\varepsilon/2,\\
	n\ge N'&\implies d(x_n,x')<\varepsilon/2.
\end{align*} 
If we let $ N=\max\{N',N''\} $, then using the triangle inequality gives $$d(x,x')\le d(x,x_n)+d(x_n,x')<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon,\footnote{Becuase we picked $ \varepsilon/2 $, they added to the desired $ \varepsilon $. If we hadn't done this, then we would have $ d(x,x')<2\varepsilon $. Sometimes, people are fine with this would just argue ``well $ 2\varepsilon\to0 $ as $ \varepsilon\to 0 $'' . This isn't wrong, but it's not exactly kosher. It's best to just satisfy the definition without having to use a limiting process with $ \varepsilon $. }$$ for all $ n\ge N $. If this holds for all $ \varepsilon>0 $, then it must be that $ d(x,x')=0 $, so $ x=x' $. 
\end{proof}
\begin{definition}
 A set $ \{x_n\} $ in a metric space $ X $ is \textit{\color{red} bounded} if its range is bounded. That is, if $ f:\N\to\R $ is the function corresponding to $ x_n $, the set $ f(\N) $ is bounded in $ X $. 
\end{definition}
\begin{example}
	The sequence $ x_n=1/n$ in $ \R $ is bounded. We have a range of $ f(\N)\subseteq(0,1] $, so the range is clearly bounded.
\end{example}
\begin{example}
	The sequence $ 2^n $ in the 2-adic metric (see Example 3.6) is bounded. The range of this sequence is $ \{2,4,8,16,\ldots\} $, but we have $ d(x,y)\le1/2 $ for all $ x,y\in\{2,4,8,16,\ldots\} $. 
\end{example}
\begin{proposition}
	Let $ \{x_n\} $ be a sequence in a metric space $ X $. If $ \{x_n\} $ converges, then $ \{x_n\} $ is bounded
\end{proposition}
\begin{proof}
	Suppose $ x_n\to x $. For all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $ d(x_n,x)<\varepsilon $ for all $ n>N $. We have $ 1>0 $, so we can let $ \varepsilon=1 $. There is some $ N $ such that $ d(x_n,x)<1 $ for all $ n>N $. Let $$r=\max\{1,d(p_1,p),\ldots,d(p_N,p)\} .$$ We have $ d(p_n,p)\le r $ for all $ n\in\N $, so the sequence is bounded.
\end{proof}
\begin{example}
	Let's work through this proof with an actual example. Let $ x_n=1/n $ in $ \R $.\footnote{Yes, I will keep using this very trite example.} We know $ x_n\to 0 $, so there exists an $ N $ such that $d(x_n,0)<1  $ for all $ n\ge N $. In this case $ N=2 $. We let $ r=\max\{1,d(p_1,p)\}=\max\{1,1\}=1 .$ We have $ d(x_n,0)\le 1 $ for all $ n\in\N $. 
	
	This proof is short, but interesting. We know every neighborhood of our limit $ 0 $ contains all but finitely many $ n $ (Proposition 3.1). The maximum distance becomes that between the limit and the finite points excluded from some arbitrary $ N_\varepsilon(0) $.\footnote{In this case we just took $ \varepsilon=1 $, but any other number would work just fine.}  	
\end{example}
\begin{example}
	The converse of Proposition 3.3 \textit{is not true}. Take $ x_n=(-1)^n $ in $ \R $. The sequence is bounded, as the range of the sequence is $ \{-1,1\} $. Despite being bounded, the sequence does not converge as it alternates between $ 1 $ and $ -1 $. 
\end{example}
Recall that in Section 2, we sometimes discussed approximating a limit point of some set $ E $ with elements in $ E $. This was not the most formal of discussions, but our next theorem will make this fact explicit. The theorem is a statement about existence, and does not provide an actual construction of the sequence in claims exists. It's \textit{very} important to be able to distinguish when a result does one of these, but not the other. It can often have practical implications for problem solving. 
\begin{theorem}
	Let $ \{x_n\} $ be a sequence in a metric space $ X $. If $ E\subseteq X $ and if $ x $ is a limit point of $ E $, then there is a sequence $ \{x_n\} $ in $ E $ such that $ x_n\to x $ in $ X $. 
\end{theorem}
\begin{proof}
	The point $ x\in X $ is a limit point of $ E $, so all $ N_r(x) $ contain some points of $ E $ that are not $ x $. If we let $ r=1/n $, then we have that there exists some point $ x_n\in N_{1/n}(x) $. Equivelently, $ d(x_n,x)<1/n $. For all $ \varepsilon>0 $, choose $ N $ such that $ N\varepsilon >1 $.\footnote{This gives $ N>1\varepsilon $.} This way, we have $$ d(x_n,x)<\frac{1}{1/\varepsilon}=\varepsilon $$ for all $ n>N $. Therefor $x_n\to x$.
\end{proof}
\begin{example}
	Let $ E=(0,1]\subseteq\R $ and, $ x_n=1/n $ in $ E $. The point $ 0\in\R $ is a limit point of $ E $. The sequence $ \{x_n\} $ converges to $ 0 $ in $ X $. Note that $ \{x_n\} $ does not converge in $ E $ (Example 3.7).   
\end{example}
\begin{corollary}
	Let $ E $ be a subset of a metric space $ X $. If $ E $ is dense in $ X $, then there for all $ x\in X $, there exists some sequence $ \{x_n\} $ in $ E $ such that $ x_n\to x $. 
\end{corollary}
Corollary 3.1 is an amazingly useful result once we become more comfortable with limits (and continuity). We may want to prove that some set $ X $ has a certain property, which could require we verify some condition for each $ x\in X $. If $ X $ has some dense subset $ Y $, then we could just prove that the property exists for a limit of a sequence in $ E $, because each point in $ X $ is a limit of such a sequence! This sounds like it would be more a more complicated method of proof, but that is because we are just starting to build the toolkit required to work with limits. If points in $ E $ are easier to work with than those in $ X $,\footnote{Nealy every time it's actually the points in $ X\backslash E $ that are the ones that are harder to work with.} then it may just be easier to take limits of them. 
\begin{example}
	The set $ \Q $ is dense in $ \R $. This means that every number in $ \R $ is the limit of a sequence in $ \Q $, including irrational numbers. We saw this already in Example 3.6. In this specific case, we actually can write down the sequence. Even if we do not know the explicit form of the sequence, we still know it at least exists.\footnote{This could be considered a drawback of this proof. It is not a proof via construction, so we don't have some blueprint that tells us how to find the sequence.} You probably don't know any sequence in $ \Q $ that converges to $ \pi $, but you do know that such a sequence exists because of Corollary 3.1.\footnote{I cannot think of a sequence that does this off the top of my head, but the series $ \sum_{n=0}^{\infty}\frac{4(-1)^k}{2k+1} $ will. We'll prove this in Section 7.} 
\end{example}
The next two results pertain only to sequences in $ \R^n $. The first is a set of familiar results from calculus, and the second gives us the means to determine if sequences of real vectors converge. 
\begin{theorem}
	Suppose $ \{x_n\} $ and $ \{y_n\} $ are sequences in $ \R $, $ \lim_{n\to\infty}x_n=x $, and $ \lim_{n\to\infty}y_n=y $. Then
	\begin{enumerate}
		\item $ \lim\limits_{n\to\infty}(x_n+y_n)=x+y $;  
		\item $ \lim\limits_{n\to\infty}cx_n=cx $, for all $ c\in\R $; 
		\item $ \lim\limits_{n\to\infty}(x_ny_n)=xy $, for all $ c\in\R $;
		\item $ \lim\limits_{n\to\infty}1/x_n=1/x$, provided $ s_n\neq 0 $ for all $ n\in \N $, and $ s\neq 0 $.
	\end{enumerate}
\end{theorem}
\begin{proof}{\color{white}w}
\begin{enumerate}
	\item There exists $ N_1,N_2\in\N $ such that  
	\begin{align*}
	n\ge N_1&\implies |x_n-x|<\frac{\varepsilon}{2},\\
	n\ge N_2&\implies |y_n-y|<\frac{\varepsilon}{2}.
	\end{align*} 
	If we set $ N=\max\{N_1,N_2\} $, then for all $ n\ge N $ we have $$ |(s_n+t_n)-(s+t)|=|(s_n-s)+(t_n-t)|\le|(s_n-s)|+|(t_n-t)|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon.$$ Therefore $ x_n+y_n\to x+y $.
	\item Given $ \varepsilon>0 $, there exists a $ N\in\N $ such that $ d(x_n,x)<\varepsilon/c$ for all $ n\ge N $. This means for all $ n\ge N $, we have $$ |cx_n-cx|=c|x_n-x|<c\cdot\frac{\varepsilon}{c}=\varepsilon ,$$ so $ cx_n\to cx $. 
	\item There exists $ N_1,N_2\in\N $ such that  
	\begin{align*}
	n\ge N_1&\implies |x_n-x|<\frac{\sqrt{2}}{\varepsilon},\\
	n\ge N_2&\implies |y_n-y|<\frac{\sqrt{2}}{\varepsilon}.
	\end{align*} 
	If we let $ N=\max\{N_1,N_2\} $, then \begin{align*}
		|x_ny_n-xy|&=|x_ny_n-xy+0+0+0|\\&=|x_ny_n-xy+(xy-yx)+(x_ny-yx_n)+(y_nx-xy_n)|\\&=|(x_ny_n-xy_n-x_ny+xy)+(yx_n-yx)+(xy_n-xy)|\\&=|(x_n-x)(y_n-y)+y(x_n-x)+x(y_n-y)|\\&\le|(x_n-x)(y_n-y)|+|y(x_n-x)+x(y_n-y)|\\&\le|(x_n-x)(y_n-y)|\\&<\sqrt{\varepsilon}\sqrt{\varepsilon}\\&=\varepsilon,
	\end{align*}
	for all $ n\ge N $. This gives $ x_ny_n\to xy $.\footnote{A very common ``trick'' used in almost all nontrivial proofs involving $ \varepsilon $ is to add and subtract the same term within an absolute value, in effect adding zero. You can then rearrange the terms and use the triangle inequality to ``separate'' terms in the absolute value. I call is a ``trick'', because when I first saw someone do it I thought ``well how the hell was I ever supposed to know to do that without ever seeing it done!''}
	\item There exists an $ N_1,N_2\in\N $ such that \begin{align*}
	n\ge N_1&\implies |x_n-x|<\frac{|x|}{2},\\
	n\ge N_2&\implies |x-x_n|<\frac{x^2}{2}\varepsilon.
	\end{align*}  
	Note that we can manipulate one of this inequalities using the triangle inequality:
	\begin{align*}
	 |x_n-x|&<\frac{|x|}{2}\\
	 |x|-|x_n|&<\frac{|x|}{2}\\
	 ||x|-|x_n||&<\frac{|x|}{2}\\
	 -\frac{|x|}{2}<|x|&-|x_n|<\frac{|x|}{2}\\
	 \frac{|x|}{2}<&|x_n|<\frac{3|x|}{2}\\ \frac{1}{|x_n|}&<\frac{2}{|x|}
	\end{align*}
	If we let $ N=\max\{N_1,N_2\} $, then
	\begin{align*}
		\left\lvert\frac{1}{x_n}-\frac{1}{x}\right\rvert &=\left\lvert\frac{1}{x}\frac{1}{x_n}(x-x_n) \right\rvert\\
		&=\frac{1}{|x|}\frac{1}{|x_n|}|x-x_n|\\&<\frac{1}{|x|}\frac{1}{|x|}\frac{|x|^2\varepsilon}{2}
		\\&=\varepsilon,
	\end{align*}
	for all $ n\ge N $. This gives $ 1/x_n\to 1/x $. 
\end{enumerate}
\end{proof}
\begin{remark}
	Part 1 and 2 of Theorem 3.2  allow us immediately to perform subtraction using limits. Parts 3 and 4 allow us to do the same with division. 
\end{remark}
\begin{remark}[Linearity of Limits]
	The first two parts of Theorem 3.2 allow us to conclude that limits are linear. This shouldn't be news, as it comes up in any standard calculus course, but it has powerful implication. We will use limiting processes to define differentiation and integration, so the linearity of limits will give rise to the linearity of these two operations as well.
\end{remark}
\begin{proposition}
	Suppose $ \{\x_n\} $ is a sequence in $ \R^k $ where $ \x_n=(x_{1,n},\ldots,x_{k,n}) $. The sequence $ \{\x_n\} $ converges to $ \x=(x_1,\ldots, x_n) $ \textit{if and only if} $ x_{j,n}\to x_j $ for $ j=1,\ldots,k $.  
\end{proposition}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ \x_n\to \x $. For all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $$|\x_n-\x|<\varepsilon $$ for all $ n\ge N $. This gives that  $ x_{j,n}\to x_j $ , as
		\begin{align*}
	|x_{j,n}- x_j|&<\left[(x_{1,n}- x_1)^2+\ldots+(x_{k,n}- x_k)^2\right]^{1/2}\\&=|\x_n-\x|\\&<\varepsilon
		\end{align*}  for $ j=1,\ldots,k $. 
		\item [$ (\Longleftarrow) $]  Suppose $ x_{j,n}\to x_j  $ for $ j=1,\ldots,k $. For all $ \varepsilon>0 $, there is some $ N\in\N $ such that $$ |x_{j,n}-x_j|<\frac{\varepsilon}{\sqrt{k}}$$ for all $ n\ge N $. For all $ n\ge N $ we have 
		\begin{align*}
			|\x_n-\x|&=\left[(x_{1,n}- x_1)^2+\ldots+(x_{k,n}- x_k)^2\right]^{1/2}\\&<\left[(\varepsilon/\sqrt{k})^2+\ldots+(\varepsilon/\sqrt{k})^2\right]^{1/2}\\&=\left(\frac{k\varepsilon^2}{k}\right)^{1/2}\\&=\varepsilon.
		\end{align*}
	Therefore $ \x_n\to \x $.
	\end{enumerate}
\end{proof}
Proposition 3.4 establishes that a sequence of vectors converge if and only if each component converges. This is not terribly surprising. 
\subsection{Subsequences}
Given some sequence $ \{x_n\} $ in $ X $, it's possible to restrict our attention to only certain terms of $ \{x_n\} $. For example, if we have $ x_n=(-1)^n $ in $ \R $, we could look at every other term and in effect have a sequence of all $ 1 $'s or all $ -1 $'s. Clearly, both these sequences that are ``contained'' in $ x_n=(-1)^n $ would converge. This case is particularly interesting because the sequence $ x_n=(-1)^n $, despite us being able to ``throw away'' certain terms and have a convergent sequence as a result. As we'll see, this is not some fluke occurrence, and it's related to the fact that this sequence lives in a compact space! This will be the first of many nice/cool results that will follow from compactness. 
\begin{definition}
	Given a sequence $ \{x_n\} $ in $ X $, consider a sequence $ \{n_k\} $ such that $ n_1<n_2<\cdots $. The sequence $ \{x_{n_k}\} $ is called a \textit{\color{red}subsequence} of $ \{x_n\} $. If $ \{x_{n_k}\} $ converges, its limit is called a \textit{\color{red} subsequential limit} of $ \{x_n\} $. 
\end{definition}
\begin{remark}
	If $ \{x_n \}$ corresponds to the function $ f:\N\to X $, then we can think of a subsequence  $ \{x_{n_k}\} $ corresponding to some function $ g:\N\to f(\N) $, where $ f(\N)\subseteq X $. In this sense, a subsequence of $ \{x_n\} $ is a sequence in the range/image of $ \{x_n\} $.
\end{remark}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \N$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[red] (5,2) node {$ \{x_n\}$};
	\draw[blue] (5,7) node {$ \{x_{n_k}\}$};

	
	\filldraw[red] (0.4,1) circle (2pt);
	\filldraw[blue,fill=red,thick] (0.8,8) circle (2pt);
	\filldraw[red] (1.2,1.5) circle (2pt);
	\filldraw[blue,fill=red,thick] (1.6,7.5) circle (2pt);
	\filldraw[red] (2.0,2) circle (2pt);
	\filldraw[blue,fill=red,thick] (2.4,7) circle (2pt);
	\filldraw[red] (2.8,2.2) circle (2pt);
	\filldraw[blue,fill=red,thick] (3.2,6.8) circle (2pt);
	\filldraw[red] (3.6,2.4) circle (2pt);
	
	\filldraw[blue,fill=red,thick] (4,6.6) circle (2pt);
	\filldraw[red] (4.4,2.6) circle (2pt);
	\filldraw[blue,fill=red,thick] (4.8,6.4) circle (2pt);
	\filldraw[red] (5.2,2.8) circle (2pt);
	\filldraw[blue,fill=red,thick] (5.6,6.2) circle (2pt);
	
	\filldraw[red] (6,3.0) circle (2pt);
	\filldraw[blue,fill=red,thick] (6.4,6) circle (2pt);
	\filldraw[red] (6.8,3.2) circle (2pt);
	\filldraw[blue,fill=red,thick] (7.2,5.8) circle (2pt);
	\filldraw[red] (7.6,3.4) circle (2pt);
	\filldraw[blue,fill=red,thick] (8,5.6) circle (2pt);
	
	\filldraw[red] (8.4,3.6) circle (2pt);
	\filldraw[blue,fill=red,thick] (8.8,5.4) circle (2pt);
	\filldraw[red] (9.2,3.8) circle (2pt);
	\filldraw[blue,fill=red,thick] (9.6,5.2) circle (2pt);
	\filldraw[red] (10,4) circle (2pt);
	\filldraw[blue,fill=red,thick] (10.4,5) circle (2pt);				
		
	\filldraw[red] (10.8,4.2) circle (2pt);
	\filldraw[blue,fill=red,thick] (11.2,4.8) circle (2pt);
	\filldraw[red] (11.6,4.4) circle (2pt);
	\filldraw[blue,fill=red,thick] (12,4.6) circle (2pt);
	\filldraw[red] (12.4,4.6) circle (2pt);
	\filldraw[blue,fill=red,thick] (12.8,4.4) circle (2pt);
	\filldraw[red] (13.2,4.4) circle (2pt);
	\filldraw[blue,fill=red,thick] (13.6,4.6) circle (2pt);
	\filldraw[red] (14,4.6) circle (2pt);
	\filldraw[blue,fill=red,thick] (14.4,4.4) circle (2pt);	
	\filldraw[red] (14.8,4.45) circle (2pt);
	\filldraw[blue,fill=red,thick] (15.2,4.55) circle (2pt);
	\filldraw[red] (15.6,4.475) circle (2pt);
	\filldraw[blue,fill=red,thick] (16,4.525) circle (2pt);
	
	
	\end{tikzpicture}
	\caption{A sequence $ \{x_n\} $ in $ \R $, with a subsequence $ \{x_{n_k}\} $. }
\end{figure}
A logical question to ask is how do we relate the limit of some converget sequence with the its subsequential limits? Is there at least one subsequential limit of a sequence that is the same as the limit of a sequence? Is the even stronger result that every subsequential limit of a sequence the same as the limit of the sequence possible? 
\begin{theorem}
	A sequence $ \{x_n\} $ converges to $ x $ \textit{if and only if} every subsequence of $ \{x_n\} $ converges to $ x $.
\end{theorem} 
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ x_n\to x $. For all $ \varepsilon $, there exists an $N $ such that $ |x_n-x|<\varepsilon $ for all $ n\ge N $. If $ x_{n_k} $ is some arbitrary subsequence of $ \{x_n\} $, then $ |x_{n_k}-x|<\varepsilon $ for all $ n_k\ge N $. Therefore $ x_{n_k}\to x $. 
		\item [$ (\Longleftarrow) $]  Suppose every subsequence of $ \{x_n\} $ converges to $ x $. This means that the trivial subsequence of $ \{x_{n_k}\}=\{x_n\} $ converges.\footnote{This trivial subsequence comes from taking $ \{n_k\}=\N $.} 
	\end{enumerate}
\end{proof}
\begin{corollary}
	A sequence $ x_n $ in a metric space $ X $ diverges \textit{if and only if} it has a divergent subsequence, \textit{or} more than one subsequential limit. 
\end{corollary}
Theorem 3.3 and it's corollary relate the limit of a convergent sequence to its subsequential limits, and says they are the all the same. The proof of this is very brief, but the result is sweeping. \textit{Every} single possible subsequence, a collection which is uncountably infinite, has the same limit.    
\begin{example}
	Let $ \{x_n\}=\{1,2,3,1,2,3,1,2,3,\ldots\} $ be a bounded sequence in $ \R $. This sequence has subsequences that converge to 1, $2 $, and 3, so $ \{x_n\} $ diverges. Alternatively, we could say that $ \{x_n\} $ diverges because it has a divergent subsequence in $ \{x_{n_k}\}=\{1,2,1,2,1,2,\ldots\} $.  
\end{example}
A more interesting case to consider than that of a convergent sequence, is a divergent sequence. How do subsequences of divergent sequences behave. Do divergent sequences has subsequential limits? This question is answered with one of the most important theorems in analysis.\footnote{How can you tell if a result is important? A good rule of thumb is that something is important if it has some specific name.}
\begin{theorem}
	If $ \{x_n\} $ is a sequence in a compact metric space $ X $, then some subsequence of $ \{x_n\} $ converges in $ X $. 
\end{theorem}
\begin{proof}
	Let $ E $ be the range of $ \{x_n\} $. If $ E $ is finite,\footnote{Remember that a sequence is infinite even if it ranges is finite. If it's range is finite, that just means it takes on at least one of these values an infinite number of times.} then we can construct a constant subsequence $ \{x_{n_k}\} $ such that $$x_{n_1}=x_{n_2}=\cdots= x ,$$ where $ x\in E $. This constant sequence converges.
	
	If $ E $ is infinite, then $ E $ has some limit point $ x\in X $ by the Bolzano-Weierstrass Property (Proposition 2.6). By Theorem 3.1, there exists a sequence $ \{y_n\} $ in $ E $ such that $ y_n\to x $. But $ E $ is just the range of $ \{x_n\} $, so any sequence $ \{y_n\} $ in $ E $ can be expressed as a subsequence $ \{x_{n_k}\} $ (see Remark 3.8). Therefore $ x $ is a subsequential limit of $ {x_n} $.  
\end{proof}
\begin{corollary}[Bolzano-Weierstrass Theorem]
	Every bounded sequence in $ \R^k $ has a convergent subsequence. 
\end{corollary}
\begin{proof}
	A bounded subset of $ \R^k $ is compact, so any such bounded sequence converges. 
\end{proof}
\begin{remark}[Equivalence Forms of Compactness in Metric Space]
	Recall Proposition 2.6 and Remark 2.10. These pertained to the fact that in a compact metric space, any subset has a limit point in the compact set. It was then discussed that there are more general settings in point-set topology where this may not be true for compact sets, and for that reason the property given by Proposition 2.6 is often called limit point compactness. A similar situation holds for Theorem 3.4. If every convergent sequence in some space has at least one subsequential limit, the space is sometimes called \textit{\color{red}sequentially compact}. In metric spaces, sequential compactness is equivalent to compactness, so knowing the difference isn't important.\footnote{This is why no introductory analysis course distinguishes them.} Many times, the definition of sequential compactness is given as the definition of compactness, as it is easier to digest than the actual definition. In sum: a \textit{\textbf{metric space}} is compact \textit{if and only if} it is sequentially compact \textit{if and only if} it is limit point compact. All of this will be made formal in Section 15 
\end{remark}
\begin{example}
	The sequence $ x_n=1/n $ in $(0,1] $ diverges (see Example 3.8). It also has no convergent subsequence, so by Theorem 3.4, $ (0,1] $ is not compact. We already knew this though, as $ (0,1] $ is not closed.
\end{example}
\begin{example}
	The sequence $ x_n=(-1)^n $ in $ \R $ is bounded (see Example 3.14). Therefore it has at least one subsequential limit. Those limits are $ 1 $ and $ -1 $.  
\end{example}
\begin{remark}
 Much like the proof of Theorem 3.1, we do not find an explicit formula for a convergent subsequence when proving The Bolzano-Weierstrass Theorem. As of now, we don't have a blueprint for finding some convergent subsequence for a sequence in $ \R^k $, we only know it is out there. Later on, we'll introduce the concept of $ \limsup $ and $ \liminf $, and show a specific type of subsequence that will \textit{always} converge. 
\end{remark}
\subsection{Cauchy Sequences}
Up until now, showing a sequence converges has been purely a theoretical exercise. Proving $ x_n $ converges to $ x $ using the definition of convergence requires we know $ x $, but how would we know the limit of a sequence if we didn't know it converged?\footnote{This is similar to something you may have seen in an intro stats course. You can use a $ z $-test  for the population mean, but that requires you know the population standard deviation. In what world would you know the population standard deviation but not the population mean?!}
\begin{example}
	Let $ x_n=(1+1/n)^n $ be a sequence in $ \R $. In Example $ 3.7 $, I claimed this converged to the constant $ e $, but this is not clear at all from looking at the sequence. 
\end{example}
In this subsection we'll develop a way of verifying of a sequence converges without knowing the limit. It's hard to emphasize just how useful this is in actual applications of analysis, as it's rarely clear what some relevant sequence would converge to. The method we develop will not work in every metric space, but will work in the most common spaces. We start by introducing a second type of convergence. 
\begin{definition}
	A sequence $ \{x_n\} $ in a metric space $ X $ is a \textit{\color{red}Cauchy sequence (in $ X $)} if for all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $ d(x_n,x_m)<\varepsilon $ if $ n\ge N $ and $ m\ge N $.
\end{definition}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \N$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);

	\draw[red] (5,1) node {$ n,m<N$};
	\draw[green] (12,1) node {$ n,m\ge N$};
	\draw[black] (16.3,4.5) node {$ \varepsilon$};
	\draw[black] (16.3, 4.7) -- (16.3, 5);
	\draw[black] (16.3, 4.3) -- (16.3, 4);
	\draw[black] (16.2, 4) -- (16.4, 4);
	\draw[black] (16.2, 5) -- (16.4, 5);
	\draw[thick,dashed] (10.8,5) -- (16,5);
	\draw[thick,dashed] (10.8,4) -- (16,4);
	\filldraw[red] (0.4,1) circle (2pt);
	\filldraw[red] (0.8,8) circle (2pt);
	\filldraw[red] (1.2,1.5) circle (2pt);
	\filldraw[red] (1.6,7.5) circle (2pt);
	\filldraw[red] (2.0,2) circle (2pt);
	\filldraw[red] (2.4,7) circle (2pt);
	\filldraw[red] (2.8,2.2) circle (2pt);
	\filldraw[red] (3.2,6.8) circle (2pt);
	\filldraw[red] (3.6,2.4) circle (2pt);
	
	\filldraw[red] (4,6.6) circle (2pt);
	\filldraw[red] (4.4,2.6) circle (2pt);
	\filldraw[red] (4.8,6.4) circle (2pt);
	\filldraw[red] (5.2,2.8) circle (2pt);
	\filldraw[red] (5.6,6.2) circle (2pt);
	
	\filldraw[red] (6,3.0) circle (2pt);
	\filldraw[red] (6.4,6) circle (2pt);
	\filldraw[red] (6.8,3.2) circle (2pt);
	\filldraw[red] (7.2,5.8) circle (2pt);
	\filldraw[red] (7.6,3.4) circle (2pt);
	\filldraw[red] (8,5.6) circle (2pt);
	
	\filldraw[red] (8.4,3.6) circle (2pt);
	\filldraw[red] (8.8,5.4) circle (2pt);
	\filldraw[red] (9.2,3.8) circle (2pt);
	\filldraw[red] (9.6,5.2) circle (2pt);
	\filldraw[red] (10,4) circle (2pt);
	\filldraw[red] (10.4,5) circle (2pt);				
	\draw[thick,dashed] (10.8,0) -- (10.8,9);
	\draw (10.8,-0.3) node {$ N$};			
	\filldraw[green] (10.8,4.2) circle (2pt);
	\filldraw[green] (11.2,4.8) circle (2pt);
	\filldraw[green] (11.6,4.4) circle (2pt);
	\filldraw[green] (12,4.6) circle (2pt);
	\filldraw[green] (12.4,4.6) circle (2pt);
	\filldraw[green] (12.8,4.4) circle (2pt);
	\filldraw[green] (13.2,4.4) circle (2pt);
	\filldraw[green] (13.6,4.6) circle (2pt);
	\filldraw[green] (14,4.6) circle (2pt);
	\filldraw[green] (14.4,4.4) circle (2pt);	
	\filldraw[green] (14.8,4.45) circle (2pt);
	\filldraw[green] (15.2,4.55) circle (2pt);
	\filldraw[green] (15.6,4.475) circle (2pt);
	\filldraw[green] (16,4.525) circle (2pt);
	
	
	\end{tikzpicture}
	\caption{A Cauchy sequence in $ \R $.}
\end{figure}
The terms in a Cauchy sequence get arbitrarily closer to each other over time. This is opposed to a convergent sequence, where the terms get arbitrarily close to some limit. This difference is the reason we never refer to a Cauchy sequence having a limit.\footnote{Unless of course it also happens to be convergent. No matter how small we let $ \varepsilon $ be, we can always find an $ N\in\N $ such that $ d(x_n,x_m)<\varepsilon $ for all $ n,m\ge N $. }  
\begin{example}
	Let $ x_n=1/2^n $ be a sequence in $ \R $. We can show that this sequence is a Cauchy sequence. If we let $ N=-\ln(\varepsilon/4)/\ln(2) $, then we have $$|x_n-x_m|=\left\lvert\frac{1}{2^n}- \frac{1}{2^m}\right\rvert\le\frac{1}{2^N}+\frac{1}{2^N}=\frac{1}{2^{-\ln(\varepsilon/4)/\ln(2)}}+\frac{1}{2^{-\ln(\varepsilon/4)/\ln(2)}}=2\cdot 2^{\log_2(\varepsilon/4)}=\frac{\varepsilon}{2}<\varepsilon $$ for all $ n,m\ge N $. Therefore $ x_n=1/2^n $ is a Cauchy sequence. 
\end{example}
While a Cauchy sequence does not have a limit, we can formulate a convergent sequence using a Cauchy sequence. If the terms in a Cauchy sequence are getting arbitrarily close, then one may suspect that if we look at the ``tail'' of such a sequence, the maximum distance between them is shrinking. This idea gives rise to the next definition and theorem, which will prove an alternate definition for a Cauchy sequence that uses the definition of a convergent sequence.
\begin{definition}
	Let $ E $ be a nonempty subset of a metric space $ X $. The \textit{\color{red}diameter} of $ E $ is the supremum of the set of $ d(x,y) $ for all $ x,y\in E $. $$\diam E=\sup\{d(x,y)\mid x,y\in E\} $$
\end{definition} 
\begin{remark}
	Whenever you see a supremum, it's worth asking yourself ``does this necessarily exist? If so, how do I know this?" In this case, the answer is yes. In Definition 3.6, the set $ E $ is a subset of $ [0,\infty] $, because $ d(x,y)\in[0,\infty] $ for all $ d(x,y) $ by the definition of $ d $. This in turn gives $ E\subseteq\R $, and any subset of $ \R $ has a supremum. 
\end{remark}
\begin{example}
	Let $ E=[a,b]\subseteq\R $. We have $ \diam E=b-a $. If we instead have $ E=(b-a) $, then we still have $ \diam E=b-a $, even though this diameter is never realized, because a supremum of a set needn't be in the set. 
\end{example}  
\begin{lemma}
	If $ x_n $ is a sequence in $ X $, and $ E_n=\{x_N,x_{N+1},x_{N+2},\ldots\} $, then $ \{x_n\} $ is a cauchy sequence \textit{if and only if} $ \lim_{N\to\infty}\diam E_n=0 $. 
\end{lemma}
\begin{proof}{\color{white}space}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ \{x_n\} $ is a Cauchy sequence.  For all $ \varepsilon>0 $, there exists some $ N\in\N $ such that $$d(x_n,x_m)<\frac{\varepsilon}{2} $$ for all $ n,m\ge N $. We use the supremum to define $ \diam E_n $, so we know there exists $ x',y'\in E_n $ such that $$0\le\diam E_N=\sup\limits_{x,y\in E_N}d(x,y)<d(x',y')+\frac{\varepsilon}{2} .$$ By the definition of $ E_n $, $ x' $ and $ y' $ take the form $ x_n $ and $ p_m $ for some $ n,m\ge N'\ge N $. Therefore $$\diam E_n<d(x_n,x_m)+\frac{\varepsilon}{2}=\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon .$$ This means that  $ \lim_{N\to\infty}\diam E_n=0 $.
		\item [$ (\Longleftarrow) $] Suppose $ \lim_{N\to\infty}\diam E_n=0 $. For all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $$ d(\diam E_n,0)=\diam E_n<\varepsilon$$ for all $ n\ge N $. If we have $ n,m\ge N $, then $ x_n,x_m\in E_N $. Therefore $$ d(x_n,x_m)\le\sup\limits_{x,y\in E_n}d(x,y)=\diam E_N<\varepsilon$$ for all $ n,m\ge N $. This gives that $ \{x_n\} $ is a Cauchy Sequence   
	\end{enumerate}
\end{proof}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \N$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);

	\filldraw[red] (0.4,1) circle (2pt);
	\filldraw[red] (0.8,8) circle (2pt);
	\filldraw[red] (1.2,1.5) circle (2pt);
	\filldraw[red] (1.6,7.5) circle (2pt);
	\filldraw[red] (2.0,2) circle (2pt);
	\filldraw[red] (2.4,7) circle (2pt);
	\filldraw[red] (2.8,2.2) circle (2pt);
	\filldraw[red] (3.2,6.8) circle (2pt);
	\filldraw[red] (3.6,2.4) circle (2pt);
	\filldraw[red] (4,6.6) circle (2pt);
	\filldraw[red] (4.4,2.6) circle (2pt);
	\filldraw[red] (4.8,6.4) circle (2pt);
	\filldraw[red] (5.2,2.8) circle (2pt);
	\filldraw[red] (5.6,6.2) circle (2pt);
	\filldraw[red] (6,3.0) circle (2pt);
	\filldraw[red] (6.4,6) circle (2pt);
	\filldraw[red] (6.8,3.2) circle (2pt);
	\filldraw[red] (7.2,5.8) circle (2pt);
	\filldraw[red] (7.6,3.4) circle (2pt);
	\filldraw[red] (8,5.6) circle (2pt);
	\filldraw[red] (8.4,3.6) circle (2pt);
	\filldraw[red] (8.8,5.4) circle (2pt);
	\filldraw[red] (9.2,3.8) circle (2pt);
	\filldraw[red] (9.6,5.2) circle (2pt);
	\filldraw[red] (10,4) circle (2pt);
	\filldraw[red] (10.4,5) circle (2pt);						
	\filldraw[red] (10.8,4.2) circle (2pt);
	\filldraw[red] (11.2,4.8) circle (2pt);
	\filldraw[red] (11.6,4.4) circle (2pt);
	\filldraw[red] (12,4.6) circle (2pt);
	\filldraw[red] (12.4,4.6) circle (2pt);
	\filldraw[red] (12.8,4.4) circle (2pt);
	\filldraw[red] (13.2,4.4) circle (2pt);
	\filldraw[red] (13.6,4.6) circle (2pt);
	\filldraw[red] (14,4.6) circle (2pt);
	\filldraw[red] (14.4,4.4) circle (2pt);	
	\filldraw[red] (14.8,4.45) circle (2pt);
	\filldraw[red] (15.2,4.55) circle (2pt);
	\filldraw[red] (15.6,4.475) circle (2pt);
	\filldraw[red] (16,4.525) circle (2pt);
	\draw[red] (5,7) node {$ \{x_n\}$};
	\draw[blue] (5,4.4) node {$ \{\diam E_n\} $};
	\filldraw[blue] (0.4,7) circle (2pt);
	\filldraw[blue] (0.8,6.5) circle (2pt);
	\filldraw[blue] (1.2,6) circle (2pt);
	\filldraw[blue] (1.6,5.5) circle (2pt);
	\filldraw[blue] (2,5) circle (2pt);
	\filldraw[blue] (2.4,4.8) circle (2pt);
	\filldraw[blue] (2.8,4.6) circle (2pt);
	\filldraw[blue] (3.2,4.4) circle (2pt);
	\filldraw[blue] (3.6,4.2) circle (2pt);
	\filldraw[blue] (4,4) circle (2pt);
	\filldraw[blue] (4.4,3.8) circle (2pt);
	\filldraw[blue] (4.8,3.6) circle (2pt);
	\filldraw[blue] (5.2,3.4) circle (2pt);
	\filldraw[blue] (5.6,3.2) circle (2pt);
	\filldraw[blue] (6,3) circle (2pt);
	\filldraw[blue] (6.4,2.8) circle (2pt);
	\filldraw[blue] (6.8,2.6) circle (2pt);
	\filldraw[blue] (7.2,2.4) circle (2pt);
	\filldraw[blue] (7.6,2.2) circle (2pt);
	\filldraw[blue] (8,2) circle (2pt);
	\filldraw[blue] (8.4,1.8) circle (2pt);
	\filldraw[blue] (8.8,1.6) circle (2pt);
	\filldraw[blue] (9.2,1.6) circle (2pt);
	\filldraw[blue] (9.6,1.2) circle (2pt);
	\filldraw[blue] (10,1) circle (2pt);
	\filldraw[blue] (10.4,0.8) circle (2pt);
	\filldraw[blue] (10.8,0.6) circle (2pt);
		\filldraw[blue] (11.2,0.4) circle (2pt);
	\filldraw[blue] (11.6,0.2) circle (2pt);
	\filldraw[blue] (12,0.2) circle (2pt);
	\filldraw[blue] (12.4,0.2) circle (2pt);
	\filldraw[blue] (12.8,0.2) circle (2pt);
	\filldraw[blue] (13.2,0.2) circle (2pt);
	\filldraw[blue] (13.6,0.2) circle (2pt);
	\filldraw[blue] (14,0.2) circle (2pt);
	\filldraw[blue] (14.4,0.15) circle (2pt);
	\filldraw[blue] (14.8,0.1) circle (2pt);
	\filldraw[blue] (15.2,0.075) circle (2pt);
	\filldraw[blue] (15.6,0.05) circle (2pt);
	\end{tikzpicture}
	\caption{A Cauchy sequence in $ \{x_n\} $ in $ \R $, and the sequence $ \{\diam E_n\} $ as defined in Theorem 3.5}
\end{figure}
\begin{example}
	Let $ x_n=1/n $ in $ \R $. For $ E_n=\{1/N,1/(N+1), 1/(N+2),\ldots\} $, we have $$\diam E_n=\sup\{d(x,y)\mid x,y\in E_n \}=\sup\{d(1/n,y)\mid y\in E_n \}=1/N. $$This means $ \lim_{N\to\infty}\diam E_n=0 $, so $ \{x_n\} $ is a Cauchy sequence by Theorem 3.5. 
\end{example}

Considering the diameter of a Cauchy sequence may seem redundant and overly complicated, but it allows us to prove many results relating to Cauchy sequences. Their introduction is more a means to and end than a definition that is important in its own right. That being said, we do need to introduce a couple of properties of the diameter in order to get mileage out of the concept when proving results.
\begin{lemma}[Properties of Diameter]
	{\color{white}d}
	\begin{enumerate}
		\item If $ \bar{E} $ is the closure of a set $ E $ in a metric space $ X $, then $\diam E=\diam \bar{E} $.
		\item If $ K_n $ is a sequence of compact sets in $ X $ such that $ K_{n+1}\subseteq K_n $ for all $ n\in\N $ and if $$\lim_{N\to\infty}\diam K_n=0, $$ then $ \cap_{n=1}^\infty K_n $ consists of exactly one point. 
	\end{enumerate}
\end{lemma}
\begin{proof}	{\color{white}d}
	\begin{enumerate}
		\item We have that $ E\subseteq \bar{E} $, so $ \diam E\le \diam \bar{E} $.\footnote{As much as I hate to borrow condescending adjectives from \cite{rudin1964principles}, this is ``clear''.} Now we will show that $ \diam E\ge\diam\bar{E} $, which gives the equality. Fix a specific $ \varepsilon>0 $, and let $ x,y\in\bar{E} $. The point $ x $ and $y$ are limit points of $ E $, so there exists $ x',y'\in E $ such that $ d(x,x')<\varepsilon $ and $ d(y,y')<\varepsilon $. The triangle inequality gives $$ d(x,y)\le d(x,x')+d(x',y')+d(y',y)<2\varepsilon+d(x',y')\le 2\varepsilon+\diam E.$$ Our selection of $ x,y\in\bar{E} $ and $ \varepsilon $ were arbitrary, so $ \diam\bar{E}\le \diam E $. 
		\item By Lemma $ 2.3 $, $ \cap_{i=1}^\infty K_n\neq\emptyset $. For the sake of contradiction, suppose $ \cap_{i=1}^\infty K_n\neq $ contains more than one point. If this is the case, then $ \diam \cap_{i=1}^\infty K_n\neq>0 $. For all $n\in\N $, $ \cap_{i=1}^\infty K_n\neq\subseteq K_n $, so $ \diam K_n\ge\diam \cap_{i=1}^\infty K_n\neq $. This contradicts the assumption that $ \lim_{N\to\infty}\diam K_n\to 0 $. 
	\end{enumerate}
\end{proof}
With the introduction of Cauchy sequences comes the million dollar question that underlies this whole subsection. Do Cauchy sequences converge? The answer is in fact, no. Most of the time, Cauchy sequences do converge, in general this is not true. What is true however, is that all convergent sequences are Cauchy sequences. The task of finding a method to prove convergence without knowing a limit becomes a matter of finding a general class of functions for which all Cauchy sequences converge. Once we know this, we can simply verify such a sequence is Cauchy and know it converges. 
\begin{example}
	Recall the sequence $ x_n=1/n $ in the space $ (0,1] $. This sequence does not converge because $ 0\notin(0,1] $. Nevertheless, it is a Cauchy sequence. Cauchy sequences do not have an explicit limit, so excluding $ 0 $ from the space $ \{x_n\} $ is in does not affect whether or not $ \{x_n\} $ is Cauchy.
\end{example}
Example 3.24 shows that a Cauchy sequence need not be convergent, but we can show that any convergent sequence is a Cauchy sequence.
\begin{theorem}
	Let $ \{x_n\} $ be a sequence in a metric space $ X $. If $ x_n $ converges to some limit $ x\in X $, then $ \{x_n\} $ is a Cauchy sequence. 
\end{theorem} 
\begin{proof}
	For all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $ d(x_n,x)<\varepsilon/2 $ for all $ n\ge N $. The triangle inequality gives $$d(x_n,x_m)\le d(x_n,x)+d(x,x_m)<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon $$ for all $ m,n\ge N $. Thus $ \{x_n\} $ is Cauchy. 
\end{proof}
\begin{example}
	Example 3.4, 3.5, 3.6, 3.7, and 3.9 also double as examples of Cauchy sequences by Theorem 3.6.
\end{example}
We now turn to one of the most important results involving sequences. If we restrict our attention to compact spaces, then all Cauchy sequences converge. This allows us to prove a sequence in a compact space converges without having any knowledge of its limit! 
\begin{theorem}
	If $ X $ is a compact metric space and if $ \{x_n\} $ is a Cauchy sequence in $ X $, then $ \{x_n\} $ converges to some point of $ X $.
\end{theorem}
\begin{proof}
Let $ \{x_n\} $ be a Cauchy sequence in a compact space $ X $. For $ n\N $, define $ E_n=\{x_N,x_{N+1},x_{N+2},\ldots\} $. By Lemma 3.1 and Lemma 3.2, $$ \lim_{N\to\infty} \diam E=\lim_{N\to\infty} \diam\bar{E}=0. $$ The set $ \bar{E}_n $ is a closed subset (Lemma 3.2) of a compact space, so it is compact (Proposition 2.4). We also have $ E_{n+1}\subseteq E_n $ for all $ n\in\N $ so $ \bar{E}_{n+1}\subseteq \bar{E}_n $.

 If  $ \bar{E}_{n+1}\subseteq \bar{E}_n $ for all $ n\in\N $, where $ \bar{E}_n $ is compact and 	$ \lim_{N\to\infty} \diam\bar{E}=0 $, there is a unique $ x\in X $ such that $ x\in\cap_{n=1}^\infty $  (Lemma 3.2). 

Let $ \varepsilon>0 $. By $ \lim_{N\to\infty} \diam\bar{E}=0 $, There exists some $ N'\in\N $ such that $ \diam\bar{E}_N<\varepsilon $ for all $ N\ge N' $. Our unique point $ x $ is in $ \bar{E_n} $, so $ d(x,y)\le\varepsilon $ for all $ y\in\bar{E} $, and hence for every $ y\in E_n $. This is equivalent to saying $ x_n\in N_\varepsilon(x) $ for all $ n\ge N_0 $. This gives $ x_n\to x $. 
\end{proof}
\begin{theorem}[Cauchy Criterion]
	In $ \R^k $, every Cauchy sequence converges.
\end{theorem}
To show this result, we will show that a Cauchy sequence in $ \R^k $ is bounded. 
\begin{proof}
{\color{red}FINISH}
\end{proof}
The Cauchy Criterion is what we have been building to. To prove a sequence converges in $ \R^k $, we just need to show it is a Cauchy sequence. This is often much easier and not as time consuming. It's important to remember that Theorem 3.6 is a more general result. It's so useful, we even have a name for metric spaces where Cauchy sequences always converge.
\begin{definition}
	A metric space is \textit{\color{red}complete} is every Cauchy sequence converges. 
\end{definition}
\begin{example}
	The real line $ \R $ is complete by the Cauchy Criterion. This example also shows that the converse of Theorem 3.6 does not hold. The real line is not compact, but is nevertheless complete. 
\end{example}
\begin{example}
	The rationals $ \Q $ are not complete. For example, the the sequence $ x_n=(1+1/n)^n $ does not converge in $ \Q $, but it is Cauchy.
\end{example}
\begin{example}
	The set of all real valued continuous functions defined on the domain $ [a,b]\subseteq\R $, denoted $ C([a,b]) $, is complete. Right now, this example seems very abstract, as we haven't even discussed sequences of functions. We will return to this later on.  
\end{example}
\begin{remark}
	Early on, we referred to $ \R $ as ``complete'' in the sense that it had no gaps. While this is related to Definition 3.7, they're not the same. Formally defining the first and distinguishing it from Definition 3.7 would be a long detour. If it is ever unclear which one is being referred to, then I'll try to specify. 
\end{remark}
\subsection{Monotonic Sequences}
While the Cauchy Criterion simplifies showing convergence in $ \R^n $, is it possible to arrive at even stronger results by restricting our attention to a smaller class of real sequences? The answer is yes. 

Many of the sequences in $ \R $ we are first introduced to share a common element in the ordering of the elements. Many basic sequences either ``grow'' or ``shrink'' at each step. We will refer to this behavior as monotonicity.
\begin{definition}
	A sequence $ \{x_n\} $ in $ \R $ is \textit{\color{red} monotonically increasing} if $ x_n\le x_{n+1} $ for all $ n\in\N $.
\end{definition} 
\begin{example}
	The sequence $ x_n=n $ in $ \R $ is not only monotonically increasing, but also divergent.
\end{example}
\begin{example}
	The sequence $ x_n=1-1/n $  in $ \R $ is monotonically increasing and converges to $ 1 $. 
\end{example}
\begin{definition}
	A sequence $ \{x_n\} $ in $ \R $ is \textit{\color{red} monotonically decreasing} if $ x_n\ge x_{n+1} $ for all $ n\in\N $.
\end{definition} 
\begin{example}
	The sequence $ x_n=1/n $  in $ \R $ is monotonically decreasing and converges to $ 0 $.
\end{example}
\begin{example}
	The sequence $ x_n=-n $ in $ \R $ is monotonically decreasing and diverges. 
\end{example}

If you take some time to think about the examples presented, it may become clear what the convergent sequences have in common. They are bounded! This is not coincidence. If a monotonic sequence is bounded, it ``moves in one direction'' towards its bound. It can neither ``overcome'' this bound, nor ``change direction'', so the only option is that it converges to it! As it turns out, the converse also happens to hold -- Any convergent monotonic sequence is bounded.  
\begin{theorem}[Monotone Convergence Theorem]
	Suppose $ \{x_n\} $ in $ \R $ is monotonic. Then $ \{x_n\} $ converges \textit{if and only if} it is bounded.
\end{theorem}
\begin{proof}{\color{white}aa}
	\begin{enumerate}
	\item [$ (\Longrightarrow) $] Suppose $ \{x_n\} $ is monotonically increasing and bounded.\footnote{If $ \{x_n\} $ is monotonically decreasing, then the proof is analogous.} We have $ x_n\le x_{n+1} $, and $ x_n\le x $ for all $ n\in\N $, where $ x\in\R $ is the supremum of the range of $ \{x_n\} $.  For all $ \varepsilon>0 $, there exists an $ N\in\N $ such that $$x-\varepsilon<x_N\le x, $$ otherwise $ s-\varepsilon $ would be the supremum of the range of $ \{x_n\} $. Since $ \{x_n\} $ is monotonically increasing, $$ x-\varepsilon<x_n\le x $$ for all $ n\ge N $. But this gives that $$|x_n-x|<\varepsilon $$ for all $ n\ge N $. Therefore $ x_n\to x $. 
	\item [$ (\Longleftarrow) $] Suppose $ \{x_n\} $ is bounded and monotonic. All bounded sequences converge (Proposition 3.3).
	\end{enumerate}
\end{proof}
Not only have we shown our result, but in doing so, we showed that a bounded and monotonic sequence will converge to its supremum/infimum. 

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw (-.3,9) node {$ \R$};
\draw (16,-0.3) node {$ \N$};	
\draw[red] (5,2) node { $ \{x_n\} $ };	
\draw (-.3,1) node {$ x$};		
\draw[thick,->] (0,0) -- (0,9);
\draw[thick,->] (0,0) -- (16,0);	
\filldraw[red] (0.4,1+2.5/0.4) circle (2pt);
\filldraw[red] (0.8,1+2.5/0.8) circle (2pt);
\filldraw[red] (1.2,1+2.5/1.2) circle (2pt);
\filldraw[red] (1.6,1+2.5/1.6) circle (2pt);
\filldraw[red] (2.0,1+2.5/2) circle (2pt);
\filldraw[red] (2.4,1+2.5/2.4) circle (2pt);
\filldraw[red] (2.8,1+2.5/2.8) circle (2pt);
\filldraw[red] (3.2,1+2.5/3.2) circle (2pt);
\filldraw[red] (3.6,1+2.5/3.6) circle (2pt);

\filldraw[red] (4,1+2.5/4) circle (2pt);
\filldraw[red] (4.4,1+2.5/4.4) circle (2pt);
\filldraw[red] (4.8,1+2.5/4.8) circle (2pt);
\filldraw[red] (5.2,1+2.5/5.2) circle (2pt);
\filldraw[red] (5.6,1+2.5/5.6) circle (2pt);

\filldraw[red] (6,1+2.5/6) circle (2pt);
\filldraw[red] (6.4,1+2.5/6.4) circle (2pt);
\filldraw[red] (6.8,1+2.5/6.8) circle (2pt);
\filldraw[red] (7.2,1+2.5/7.2) circle (2pt);
\filldraw[red] (7.6,1+2.5/7.6) circle (2pt);
\filldraw[red] (8,1+2.5/8) circle (2pt);

\filldraw[red] (8.4,1+2.5/8.4) circle (2pt);
\filldraw[red] (8.8,1+2.5/8.8) circle (2pt);
\filldraw[red] (9.2,1+2.5/9.2) circle (2pt);
\filldraw[red] (9.6,1+2.5/9.6) circle (2pt);
\filldraw[red] (10,1+2.5/10) circle (2pt);	
\filldraw[red] (10.4,1+2.5/10.4) circle (2pt);
\filldraw[red] (10.8,1+2.5/10.8) circle (2pt);
\filldraw[red] (11.2,1+2.5/11.2) circle (2pt);
\filldraw[red] (11.6,1+2.5/11.6) circle (2pt);
\filldraw[red] (12,1+2.5/12) circle (2pt);
\filldraw[red] (12.4,1+2.5/12.4) circle (2pt);
\filldraw[red] (12.8,1+2.5/12.8) circle (2pt);
\filldraw[red] (13.2,1+2.5/13.2) circle (2pt);
\filldraw[red] (13.6,1+2.5/13.6) circle (2pt);
\filldraw[red] (14,1+2.5/14) circle (2pt);
\filldraw[red] (14.4,1+2.5/14.4) circle (2pt);	
\filldraw[red] (14.8,1+2.5/14.8) circle (2pt);
\filldraw[red] (15.2,1+2.5/15.2) circle (2pt);
\filldraw[red] (15.6,1+2.5/15.6) circle (2pt);
\filldraw[red] (16,1+2.5/16) circle (2pt);
\draw[thick,dashed] (0,1) -- (16,1);
\end{tikzpicture}
\caption{A monotonic and bounded sequence $ \{x_n\} $ in $ \R $. The sequence converges to the infimum of the sequences range, $ x $.}
\end{figure}
\begin{example}
	We can verify that $ x_n=(1+1/n)^n $ converges by showing it is monotonically increasing and bounded. We can show it is bounded by using the inequality $ \ln(1+x)\le x $. $$\left(1+\frac{1}{n}\right)^n=\exp\left(n\ln\left(1+\frac{1}{n} \right)\right)\le \exp\left(n\cdot\frac{1}{n}\right)=e. $$ To show the sequence is monotonic, we can use the inequality relating the arithmetic mean to the geometric mean (AM-GM inequality)\footnote{This is a very useful inequality to know when working with proves involving boundedness and monotonicity.} which gives, $$ \frac{x_1+x_2+\cdots+x_n}{n}\ge(x_1x_2\ldots x_n)^{1/n}.$$ If we let $ x_1=1 $, $ x_2=x_3=\cdots=x_{n+1}=1+1/n $, then 
	\begin{align*}
(x_1x_2\ldots x_{n+1})^{\frac{1}{n+1}}&\le \frac{1}{n+1}(x_1+x_2+\cdots+x_{n+1})\\\left[1\left(1+\frac{1}{n}\right)\cdots \left(1+\frac{1}{n}\right)\right]^\frac{1}{n+1}&\le\frac{1}{n+1}\left[1+\left(1+\frac{1}{n}\right)+\ldots + \left(1+\frac{1}{n}\right)\right]\\\left(1+\frac{1}{n}\right)^\frac{n}{n+1}&\le \frac{1+n\left(1+\frac{1}{n}\right)}{n+1}\\&=1+\frac{1}{n+1}\\
\left(1+\frac{1}{n}\right)^n&\le\left(1+\frac{1}{n+1}\right)^{n+1}.
	\end{align*}
	Hence $ x_n\le x_{n+1} $. We've only shown that $ \{x_n\} $ converges. In order to show that $ x_n\to e $, we would need to show that $ e $ is a least upper bound. 
\end{example}
\begin{remark}[Inequalities]
	Proofs involving inequalities can be an acquired taste. They often rely on really clever algebraic manipulations and using a set of common inequalities such as the AM-GM inequality. For this reason, I think they're a complete pain in the ass. Instead of relying on mathematical intuition and insight, you just need to make some guess about how to manipulate an equation. 
\end{remark}
\begin{remark}
	It's worth recapping what we've done with sequences up until this point, and the motivation behind our results. One of the main goals when working with sequences is proving/disproving convergence. Unfortunately, showing a sequence converges only using the definition of convergence can be hard, as it requires us to know the limit of a function. If we know our sequence is in a specific type of metric space, or has a certain property, then you can use Theorem 3.6, Theorem 3.7, and/or Theorem 3.8 to verify convergence! 
	\begin{figure}[h!]
		\begin{tikzpicture}
			\draw[->] (0,0.3)--(2,1.2);
			\draw[->] (2.5,1.2) -- (5.5,0.3);
			\draw[->] (0,-0.3)--(2,-1.2);
			\draw[->] (2,1.8) -- (4,2.7);
			\draw[->] (5.5,3)--(7.5,3);
			\draw[->] (5.5,3)--(8,1.8);
			\node[draw] at (0,0) {Is $ X $ compact?};
			\node[draw] at (2,1.5) {Does $ X=\R $?};
			\node[draw] at (2,-1.5) {Definition of Convergence};
			\node[draw] at (4,3) {Is $ X $ monotonic?};
			\node[draw] at (5.5,0) {Theorem 3.6, Definition of Cauchy};
			\node[draw] at (12,3) {Montone Convergence Theorem, Definition of Bounded};
			\node[draw] at (8,1.5) {Cauchy Criterion, Definition of Cauchy};
			\draw (0.5,0.8) node {\scriptsize Yes};
			\draw (0.5,-0.8) node {\scriptsize No};
			\draw (3.7,1) node {\scriptsize No};
			\draw (2.9,2.4) node {\scriptsize Yes};
			\draw (7.2,2.4) node {\scriptsize No};
			\draw (6.8,3.2) node {\scriptsize Yes};
		\end{tikzpicture}
		\caption{If $ \{x_n\} $ is a convergent sequence in $ X $, how do we prove that $ \{x_n\} $ converges?}
	\end{figure}
\end{remark}
\subsection{Sequences and Infinity}
We have yet to discuss what happens when divergent sequences ``go off'' to infinity. 
\begin{definition}
	Let $ \{x_n\} $ be a sequence in $ \R $. If for all $ M\in\R $ there exists an $ N\in\N $ such that $ x_n\ge M $, we write $$\lim\limits_{n\to\infty}x_n=\infty .$$ Similarly, if for every $ M\in\R $ there exists an $ N\in\N $ such that $ n\ge N $ implies $ x_n\le M $, we write $$\lim\limits_{n\to\infty}x_n=-\infty .$$
\end{definition}
These definition make a slight abuse of notation, as they refer to the limit of a sequence despite the sequence in question diverging. 
\begin{example}
	Let $ x_n=n $ be a sequence in $ \R $. For all $ M\in\R $, let $ N=M+1 $. We have $$s_n\ge s_{N}=N=M+1>M $$ for all $ n\ge N $. Therefore $ x_n\to\infty $.
\end{example}
\begin{remark}[Extended Real Numbers]
If we were instead in the extended real numbers (Definition 1.19) $ \overline{\R} $, then the type of sequences given in Definition 3.10 would converge, because $ \{-\infty,\infty\}\subset\overline{\R} $. In fact, because every set in $ \overline{\R} $ is bounded, many of the results pertaining to sequences would be more general in $ \overline{\R} $. Any proposition or theorem that requires a sequence to be bounded in $ \R $, would not make this requirement in $ \overline{\R} $, as it would be implicitly met. 
\end{remark}
\subsection{$ \limsup $ and $ \liminf $}
It can be insightful to study how the bounds of sequences change as $ n $ grows. For sequences in $ \R $, doing this illuminates interesting relationships between converge, bounds, limits of bounds, and subsequential limits. 

A sequence is bounded if its range is a bounded set. If this sequence is in $ \R $, then we know something very powerful about the range of the sequence -- its supremum and infimum exist in $ \R $ (Theorem 1.2). The same can be said for the supremum and infimum of the set of all elements in the range of $ x_n $ that have yet to be realized, $\{x_m\mid m\ge n\} $. The set $ \{x_m\mid m\ge n\}$ is bounded above or below by $ x_n $, so the supremum or the infimum (or both) of $\{x_m\mid m\ge n\} $ exist. We will define the supremum and infimum of a set using this set of values $ \{x_m\mid m\ge n\} $. 
\begin{definition}
	Let $ x_n $ be a sequence in $ X $. The \textit{\color{red}infimum of a sequence } is $$ \inf x_n=\inf\limits_{m\ge n}x_m=\inf\{x_m\mid m\ge n\}.$$
\end{definition}  
\begin{definition}
	Let $ x_n $ be a sequence in $ X $. The \textit{\color{red}supremum of a sequence} is $$ \sup x_n=\sup\limits_{m\ge n}x_m=\sup\{x_m\mid m\ge n\}.$$
\end{definition}  
The supremum and infimum of a sequence form their own sequences as the set $ \{x_m\mid m\ge n\} $ depends on the value of $ n $. Figure 36 illustrate the supremum and infimum of a sequence in $ \R $.
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw (-.3,10) node {$ \R$};
\draw (16,-0.3) node {$ \N$};		
\draw[thick,->] (0,0) -- (0,10);
\draw[thick,->] (0,0) -- (16,0);
\draw [loosely dotted, ultra thick] (0,5) sin (0.5,10) cos (1,5) sin (1.5,1) cos (2,5) sin (2.5,9) cos (3,5) sin (3.5,2) cos (4,5) sin (4.5,8) cos (5,5) sin (5.5,3) cos (6,5) sin (6.5,7) cos (7,5) sin(7.5,4) cos (8,5) sin (8.5,6) cos (9,5) sin(9.5,4.5) cos (10,5) sin (10.5,5.25) cos(10.75,5) sin (11,4.75) cos (11.25,5) sin (11.5,5.25) cos(11.75,5) sin(12,4.75) cos(12.25,5) sin(12.5,5.25) cos(12.75,5)  sin(13,4.75) cos(13.25,5) sin(13.5,5.25) cos(13.75,5)  sin(14,4.75) cos(14.25,5) sin(14.5,5.25) cos(14.75,5);	
\draw[loosely dotted, ultra thick, red] (0.5,10) -- (2.5,10);
\draw[loosely dotted, ultra thick,red] (2.5,9) -- (4.5,9);
\draw[loosely dotted, ultra thick,red] (4.5,8) -- (6.5,8);
\draw[loosely dotted, ultra thick,red] (6.5,7) -- (8.5,7);
\draw[loosely dotted, ultra thick,red] (8.5,6) -- (10.25,6);
\draw[loosely dotted, ultra thick,red] (10.25,5.25) -- (14.75,5.25);
\draw[loosely dotted, ultra thick, blue] (1.5,1) -- (3.5,1);
\draw[loosely dotted, ultra thick,blue] (3.5,2) -- (5.5,2);
\draw[loosely dotted, ultra thick,blue] (5.5,3) -- (7.5,3);
\draw[loosely dotted, ultra thick,blue] (7.5,4) -- (9.5,4);
\draw[loosely dotted, ultra thick,blue] (9.5,4.5) -- (10.5,4.5);
\draw[loosely dotted, ultra thick,blue] (10.5,4.75) -- (15,4.75);
\draw[loosely dotted, ultra thick,blue] (0,5) -- (1,5);
\draw[loosely dotted,ultra thick,blue] (0.9,5) sin (1.4, 1);
\draw[loosely dotted,ultra thick,red] (-0.1,5) sin (0.4, 10);
\draw(5.5,5) node {$\{ x_n\}$};
\draw[red] (5.5,8.3) node {$  \{\sup x_n\}$};
\draw[blue] (6.5,2.6) node {$ \{\inf x_n\}$};
\end{tikzpicture}
\caption{A sequence $ \{x_n\} $ in $ \R $, with the sequence $ \{\sup x_n\} $ and $ \{\inf x_n\} $ shown in red and blue, respectively. For any $ n\in\N $, we take the supremum and infimum of all the points of $ \{x_m\mid m\ge n\}$. Graphically this means we take the supremum and infimum of all the points to the right of a certain value of $ \N $.  }
\end{figure}
\begin{example}
	Let $ x_n=1/n $ be a sequence in $ \R $. We have $ \{x_m\mid m\ge n\}=\{1/m \mid n\ge m\} $. This gives $ \sup x_n=1/n $ and $ \inf x_n=0 $, each being its own sequence in $ \R $.  
\end{example}
\begin{example}
	If $ x_n $ be a monotonically increasing sequence, then $ \inf x_n = x_n$ as $ x_n\le x_m $ for all $ m\ge n $. Similarly, if $ x_n $ is monotonically decreasing, then $ \sup x_n=x_n $.  
\end{example}
\begin{example}
	If $ x_n=\sin x $, then $ \inf x_n=-1 $ and $ \sup x_n=1 $ for all $ n\in\N $. 
\end{example}
Now we will define the limits of $ \sup x_n $ and $ \inf x_n$.
\begin{definition}
	Let $ \{x_n\} $ be a sequence in $ X $. The \textit{\color{red}limit inferior} of $ \{x_n\} $ is the limit of $ \{\inf x_n\} $, and is written as  $\liminf\limits_{n\to\infty} x_n$ or $\varliminf\limits_{n\to\infty} x_n. $ 
\end{definition}
\begin{definition}
	Let $ \{x_n\} $ be a sequence in $ X $. The \textit{\color{red}limit superior} of $ \{x_n\} $ is the limit of $ \{\sup x_n\} $, and is written as  $\limsup\limits_{n\to\infty} x_n$ or $\varlimsup\limits_{n\to\infty} x_n. $ 
\end{definition}
\begin{example}
The sequence $ x_n=1/n $ has a limit inferior of 0, and a limit superior of 0. 
\end{example}
\begin{example}
The sequence $ x_n=\sin x $ has a limit inferior of -1, and a limit superior of 1. 
\end{example}

You may have noticed two things: the sequences $ \{\sup x_n\} $ and $ \{\inf x_n\} $ seem to always converge, and sometimes the limit inferior and limit superior are equal. The observations are consequences of our next results. 
\begin{proposition}
	If $ \{x_n\} $ is a bounded sequence in $ \R $, then $ \liminf\limits_{n\to\infty} x_n $ and $ \limsup\limits_{n\to\infty} x_n $ exist. That is, the sequences $ \{\inf x_n\} $ and $ \{\sup x_n\} $ converge.
\end{proposition} 
\begin{proof}
	We will prove the result for the limit superior.\footnote{The proof for the limit inferior is very similar.} Let $ \{x_n\} $ be a bounded sequence in $ \R $. By the least-upper-bound property, $ \sup x_n $ exists for all $ n\in\N $, as the range of $ x_n $ is bounded. The sequence $ \sup x_n $ is bounded because $ x_n $ is bounded. By the Monotone Convergence Theorem, is suffices to show that $ \sup x_n $ is monotonically decreasing. We have $ \{x_n\mid m\ge n+1\}\subset \{x_n\mid m\ge n\} $, so any upper bound of $ \{x_n\mid m\ge n\} $ is an upper bound of  $ \{x_n\mid m\ge n+1\} $. This includes the least-upper-bound, which in this case is $ \sup x_n $. By the definition of the least-upper-bound, $ \sup x_{n+1} $ is less than all other upper-bounds of $ \{x_n\mid m\ge n+1\} $, therefore $ \sup x_n\ge \sup x_{n+1} $. This gives that $ \{\sup x_n\} $ is monotonically decreasing.   
\end{proof}
\begin{remark}[Monotonicity and Limits of Bounds]
	The proof of the last result took advantage of the fact that $ {\sup x_n} $ was monotonically decreasing and bounded, allowing the use of the Monotonic Convergence Theorem (Theorem 3.8). When we proved this result, we showed that a monotonically decreasing sequence will converge to its infimum. This means that $ \{\sup x_n\} $ converges to its infimum. For this reason the limit superior is sometimes written as $ \inf\sup x_n $. Similarly, the limit inferior can be written as $ \sup\inf x_n $. 
\end{remark}
\begin{proposition}
	Let $ \{x_n\} $ be a bound subsequence in $ \R $. Then 
	\begin{enumerate}
		\item There exist subsequences which converges to $ \limsup x_n $ and $ \liminf x_n $.
		\item For all $ c\in\R $, if there exists a subsequence which converges to $ c $, then $ \liminf x_n \le c\le \limsup x_n $. 
\end{enumerate}
\end{proposition}
If we let $ E $ be the set of subsequential limits corresponding to a bounded $ \{x_n\} $ in $ \R $, then Proposition 3.6 concludes that $ \{\liminf x_n,\limsup x_n\}\subset E $, $ \liminf x_n=\inf E $, and $ \limsup x_n=\sup E $. 
\begin{example}
	Let $ x_n=1/n $ be a sequence in $ \R $. It is convergent and bounded. We know from Example 3.37 that $ \liminf x_n=\limsup x_n=0 $. By Proposition 3.6, $ 0 $ is the only subsequential limit of $ \{x_n\} $. We already knew this though, because that is precisely what Theorem 3.3 asserts.
\end{example}
\begin{example}
	For $ x_n=\sin x $ in $ \R $, the set of subsequential limits is a subset of $ [-1,1] $. 
\end{example}
\begin{proposition}
	Let $ \{x_n\} $ be a bounded sequence in $ \R $. The sequence converges \textit{if and only if} $$ \liminf\limits_{n\to\infty} x_n=\limsup\limits_{n\to\infty} x_n.$$
\end{proposition}
\begin{proof}{\color{white}a}
	\begin{enumerate}
	\item [$ (\Longrightarrow) $] Suppose $ \{x_n\} $ converges to $ x\in \R $. By Theorem 3.3, every subsequence of $ \{x_n\} $ converges to $ x $. By Proposition 3.6, $ \lim\inf x_n $ and $ \lim\sup x_n $ are subsequential limits of $ \{x_n\} $. This means they both are $ x $, as $ x $ is the only subsequential limit of $ \{x_n\} $. Therefore $ \liminf x_n=\limsup x_n $.  
	\item [$ (\Longleftarrow) $] Assume that $ \limsup x_n=\liminf x_n=x $. For all $ \varepsilon>0 $, 
	\begin{align*}
		x-\varepsilon < \liminf x_n \le \limsup x_n< x+\varepsilon.
	\end{align*}
	This allows us to choose an $ N_1,N_2\in\N $ such that
	 \begin{align*}
		s-\varepsilon<x_n<s+\varepsilon
	\end{align*}
	for all $ n\ge\max\{N_1,N_2\} $. This gives that $ |x_n-x|<\varepsilon $ for all such $ n $, so $ x_n\to x $. 
\end{enumerate}
\end{proof}
\subsection{Series}
Now we consider series. As we'll see, everything we did with sequences caries over nicely to series. In fact, working with series will be even simpler.

A series is simply a special type of sequence which results from summing each element of a different sequence.
\begin{definition}
	Given a sequence $ \{x_n\} $ in a metric space $ X $, we have a \textit{\color{red}series} in the form of $$ \sum_{n=1}^{\infty}x_n. $$
\end{definition}
\begin{example}[Geometric Series]
	If we let $ x_n=1/2^n $, then $$ \sum_{n=1}^{\infty} \frac{1}{2^n}=\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\cdots .$$
\end{example}
In order to define the convergence of a series, we will look at the behavior of the first $ n $ terms. That is, we are interested in \begin{align*}
	s_1=\sum_{k=1}^{1}x_k&=x_1\\s_2=\sum_{k=1}^{2}x_k&=x_1+x_2\\s_3=\sum_{k=1}^{3}x_k&=x_1+x_2+x_3\\s_4=\sum_{k=1}^{4}x_k&=x_1+x_2+x_3+x_4\\&\vdots
\end{align*} 
If the sequence $ \{s_n\} $ converges, then we will say the series $ \sum_{n=1}^{\infty}x_n $ converges. 
\begin{definition}
	Let $ \sum_{n=1}^{\infty} x_n $ be a series in $ X $. Define the \textit{\color{red}partial sums} of the series to be the sequence $$ s_n=\sum_{k=1}^{n}x_k .$$ A series \textit{\color{red}converges} to $ x\in X $ if the sequence $ \{s_n\} $ converges to $ x\in X $. In this case, we write $$\sum_{n=1}^{\infty}x_n=x .$$
\end{definition}

\begin{remark}[Everything from Sequences Carries Over]
	Every single result we established for sequences can be formulated in terms of series, because every single series can be expressed as a sequence of partial sums. 
\end{remark}
Showing a series converges using this definition is unnecessarily difficult. In $ \R $ we can use a reformulation of the Cauchy Criterion
\begin{theorem}[Cauchy Criterion for Series]
	Let $ \sum_{n=1}^{\infty}x_n $ be a sequence in $ \R $. The series converges \textit{if and only if} for all $ \varepsilon>0 $ there exists an $ N\in\N $ such that $$ \left\lvert\sum_{k=n}^{m}x_k \right\rvert\le\varepsilon $$ for all $ m\ge n \ge N $.
\end{theorem}
\begin{proof}
This follows from the fact that $$ |s_n-s_m|=\left\lvert\sum_{k=1}^nx_n-\sum_{k=1}^mx_m  \right\rvert=\left\lvert\sum_{k=n}^{m}x_k \right\rvert .$$
\end{proof}
\begin{corollary}
	If $ \sum_{n=1}^\infty x_n $ converges, then $ x_n\to 0 $.
\end{corollary}
\begin{proof}
	Let $ m=n $ and use Theorem 3.9
\end{proof}

\begin{remark}[No Limit Needed!]
Theorem 3.9 is a direct consequence of the Cauchy Criterion, so we don't need to know the limit of a convergent sequence to prove it converges. 
\end{remark}
\begin{example}
	The converse of Corollary 3.4 is not true! The harmonic series $ \sum_{n=1}^{\infty} 1/n$ diverges despite the fact that $ 1/n\to 0 $. 
\end{example}


\begin{example}
	The series $ \sum_{n}^{\infty}1/x! $ converges to $ e $ in $ \R $. This sequence does not converge in $ \Q $, as $ e\notin \Q $. 
\end{example}
\subsection{Tests for Convergent Series}
Whenever we work with a series $ \sum_{n=1}^{\infty}x_n $, we want to find some way to relate a sequence involving $ x_n $ to the convergence of the series. There are three tests we can use to verify that a sequence in $ \R $ converges by doing just this: the comparison test, the ratio test, and the root test. Another benefit of these tests, is none require us to know the limit of a series to prove it converges! 
\begin{theorem}[The Comparison Test]
	Let $ \sum_{n=1}^{\infty}x_n  $ be a series in $ \R $. If $ |x_n|\le y_n $ for $ n\ge N_0 $, where $ N_0 $ is some fixed integer, and if $ \sum_{n=1}^{\infty}c_n $ converges, then $ \sum_{n=1}^\infty x_n $ converges.
\end{theorem}
\begin{proof}
	For all $ \varepsilon>0 $, there exists an $ N\ge N_0 $ such that $ m\ge n\ge N $ implies $$ \sum_{k=n}^{m}y_k\le \varepsilon  ,$$ according to The Cauchy Criterion. Therefore we have $$ \left\lvert\sum_{k=n}^{m}x_k \right\rvert\le\sum_{k=n}^{m}|x_k|\le\sum_{k=n}^{m}y_k\le \varepsilon,  $$ so again by the Cauchy criterion, $\sum_{k=n}^{m}x_k  $ converges. 
\end{proof}
\begin{example}
	The geometric series $ \sum_{n=1}^{\infty}1/3^n $ converges to $ 3/2 $. We have $$\frac{1}{3^n+n}<\frac{1}{3^n} $$ for all $ n\in\N $, so the series $ \sum_{n=1}^{\infty}1/(3^n+n) $ converges by the Comparison Test.
\end{example}
\begin{example}
	The harmonic series $ \sum_{n=1}^{\infty}1/n  $ diverges. We have $$\frac{n}{n^2-\cos^2 n}>\frac{n^2}{n}=\frac{1}{n}.$$	Therefore by the converse of Theorem 3.10, the series $ \sum_{n=1}^{\infty}\frac{n}{n^2-\cos^2 n} $ diverges.
\end{example}

\begin{theorem}[The Ratio Test]
	If $ \sum_{n=1}^{\infty}x_n  $ is a series in $ \R $, then
	\begin{enumerate}
		\item the series converges if $ \lim_{n\to\infty}|x_{n+1}/x_n|<1 $;
		\item the series diverse if $ |x_{n+1}/x_n|\ge 1 $ for all $ n\ge N_0 $ for a fixed $ N_0\in\N $.
	\end{enumerate} 
\end{theorem}
\begin{proof}
	Suppose $ \lim |x_{n+1}/x_n|=\beta<1 $. For $ \varepsilon $ This means we can find an integer $ N\in\N $ such that $|x_{n+1}/x_n|<\beta $ for all $ n\ge N $. This can be rewritten as $ |x_{n+1}|<\beta|x_n| $. If we do this for $ n=N,N+1,N+2,\ldots ,N+k $, then \begin{align*}
		|x_{N+1}|&<\beta|x_N|,\\
		|x_{N+2}|&<\beta|x_{N+1}|<\beta^2|x_{N}|,\\
		&\vdots\\
		|x_{N+k}|&<\beta^p|a_N|.
	\end{align*}
This gives $$ \sum_{k=N+1}^{\infty}|x_k|=\sum_{k=1}^{\infty}|x_{N+k}|<\sum_{k=1}^{\infty}\beta^k|x_N|=|x_N|\sum_{k=1}^\infty \beta ^k=|x_N|\frac{\beta}{1-\beta}.$$	Therefore by the Comparison Test, $ \sum_{n=1}^{\infty}x_k $ converges, because $$ \sum_{n=1}^{\infty}|x_n|\le\sum_{k=N+1}^{\infty}|x_k|.$$

Now suppose that $ \limsup|x_{n+1}/x_n|=\beta>1 $. This means that $ |x_{n+1}|>|x_n| $ for a sufficiently large $ n $, meaning $ \lim\limits_{n\to\infty}x_n\neq 0 $. By the converse of Corollary 3.4, the series diverges.  
\end{proof}
\begin{example}
	Does $ \sum_{n=0}^{\infty}n!/5^n $ converge of diverge? 
	$$\lim\limits_{n\to\infty}\left\lvert \frac{x_{n+1}}{x_n}\right\rvert=\lim\limits_{n\to\infty}\left\lvert \frac{(n+1)!}{5^{n+1}}\frac{5^n}{n!}\right\rvert=\lim\limits_{n\to\infty}\frac{(n+1)!}{5n!}=\lim\limits_{n\to\infty}\frac{(n+1)n!}{5n!}=\lim\limits_{n\to\infty}\frac{n+1}{5}=\infty$$ By the Ratio Test, the series diverges. 
\end{example}
\begin{example}
	If we have $ \lim_{n\to\infty}|x_{n+1}/x_n|=1 $, then we aren't able to conclude anyhting about the convergence of a series. For example $ \sum_{n=1}^{\infty}1/n $ diverges, whereas  $ \sum_{n=1}^{\infty}1/n^2 $ converges. In both cases $ \lim_{n\to\infty}|x_{n+1}/x_n|=1 $. 	
\end{example}
\begin{theorem}[The Root Test]
	If $ \sum_{n=1}^{\infty}x_n  $ is a series in $ \R $, then
	\begin{enumerate}
		\item the series converges if $ \lim_{n\to\infty}\sqrt[n]{|x_n|}<1 $;
		\item the series diverges if $ \lim_{n\to\infty}\sqrt[n]{|x_n|}>1 $;
		\item we cannot determine anything if $ \lim_{n\to\infty}\sqrt[n]{|x_n|}=1 $.
	\end{enumerate} 
\end{theorem}
\begin{proof}
Let $ \alpha=\lim_{n\to\infty}\sqrt[n]{|x_n|} $. If $ \alpha <1 $, we can find a $ \beta\in(\alpha,1) $, and an $ N\in\N $ such that $ \sqrt[n]{|x_n|}<\beta$ for $ n\ge N $. This inequality can be expressed as $ |x_n|<\beta^n $. Because $ \sum_{n=0}^{\infty}\beta^n $ converges, the Comparison Test gives that $\sum_{n=N}^\infty{|x_n|}$ converges. Since $$ \sum_{n=1}^\infty{|x_n|}=\sum_{n=N}^{N-1}{|x_n|}+\sum_{n=N}^\infty{|x_n|},$$ the series $ \sum_{n=1}^\infty{|x_n|} $ must converge, as the first sum is finite. 

If $ \alpha>1 $, then $ |x_n|>1^n=1 $, so $ \lim\limits_{n\to\infty}x_n\neq 0 $. By Corollary 3.4, the series diverges.

The series $ \sum_{n=1}^{\infty}1/n $ and $ \sum_{n=1}^{\infty}1/n^2 $ both have $ \alpha=1 $, despite the prior diverging and the latter converging. Therefore if $ \alpha=1 $, we cannot make any statement about convergence. 
\end{proof}
\begin{example}
	Does the series $\sum_{n=0}^{\infty}\left(\frac{5n-3n^3}{7n^3+3}\right)^n $ converge or diverge? 
	$$\lim\limits_{n\to\infty}\sqrt[n]{|x_n|}=\lim\limits_{n\to\infty}\left\lvert\left(\frac{5n-3n^2}{7n^3+2}\right)^n\right\rvert^{1/n} =\lim\limits_{n\to\infty}\left\lvert\frac{5n-3n^2}{7n^3+2}\right\rvert=\frac{3}{7}<1$$ By the Root Test, the series converges.
\end{example}
\begin{remark}[Slightly More General Tests]
 We can make the Ratio Test and Root Test a bit more powerful by using $ \limsup $ instead of just the limit. The proofs would be modified slightly. This will rarely make a difference. 
\end{remark}
\begin{remark}[Ratio vs. Root]
	The Ratio Test is much easier to use than the Root Test. Calculating ratios tends to be way easier than the $ n $th root of a value. That being said, the Root Test is more powerful in the sense that the Ratio Test will always agree with the Root Test. Furthermore, there are cases when the Ratio Test is inconclusive, but the Root Test is not. A good example of this is found in Example 3.35 in \cite{rudin1964principles}. 
\end{remark}

\subsection{Exercises}
rudin, 3.4 part b

constant sequences in $ \Z $

subsequential limits closed

series of nonnegative terms converges iff partial sums bounded
\newpage
\section{Continuity}
We spent a fair amount of time studying what it means for a sequence or series to get ``arbitrarily close''to some point. We now want to do something similar with more general functions, and rigorously define continuity. Continuity makes nearly all of analysis possible, and without it we would be hopeless. Knowing the properties related to continuity will prove useful when proving many results, as many results will pertain exclusively to continuous functions. 
\subsection{Limits of Functions}
We begin by defining what a limit of a function is.
\begin{definition}
Let $ X $ and $ Y $ be metric spaces, and $ E\subset X $. Suppose there is some function $ f:E\to Y $, and a limit point $ p $ of $ E $ and some $ L\in Y $ satisfying: for all $ \varepsilon>0 $ there exists a corresponding $ \delta>0 $ such that $d_Y(f(x),L)<\varepsilon $ for all points $ x\in E $ which satisfy $ d_X(x,p)<\delta $. We say that $ L $ is \textit{\color{red}the limit of $ f $ as $ x $ approaches p}, and write $$\lim\limits_{x\to p}f(x)=L .$$
\end{definition} 
This definition seems much more complicated than that given for convergence, but the concepts conveyed are fairly similar. It may be helpful to return to the somewhat lame example of playing a game with your friend involving $ \varepsilon $. Instead of a sequence you now play with some function $ f:E\to Y $, and a fixed limit point $ p\in E $.\footnote{It's important that $ p $ is a limit point. This means every neighborhood of $ p $ has infinitely many points, so we can always get arbitrarily close to it.} Your friend gives you a value of $ \varepsilon $, and he challanges you to find a $ \delta $ such that $ d_Y(f(x),L)<\varepsilon $ whenever $ d_X(p,x)<\delta $. Note that we never require that $ f(x)=L $ (see Figure 37). Our next several examples will make this more concrete.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9) };
	\draw[thick, dashed] (6,0) -- (6,6);
	\draw[thick, dashed] (5,0) -- (5,5.35);
	\draw[thick, dashed] (7,0) -- (7,6.55);
	\draw[thick, dashed] (0,6) -- (6,6);
	\draw[thick, dashed] (0,5.35) -- (5,5.35);
	\draw[thick, dashed] (0,6.55) -- (7,6.55);
	\filldraw[color=blue, fill=white] (6,6) circle (2pt);
	\filldraw[color=white, fill=blue] (6,2) circle (2pt);
	\draw (6,-0.3) node {$ x_0$};
	\draw (5,-0.3) node {$ x_0-\delta$};
	\draw (7,-0.3) node {$ x_0+\delta$};
	\draw[blue] (10,8.3) node {$ f(x)$};
	\draw (-0.9,5.35) node {$ L-\varepsilon$};
	\draw (-0.9,6.55) node {$ L+\varepsilon$};
	\draw (-0.8,6) node {$ L$};
	\end{tikzpicture}
	\caption{We have $ \lim_{x\to x_0}f(x)=L $ for a real function. No matter how small $\varepsilon $ gets, we can find a $ \delta $ such that $ |f(x)-L|<\varepsilon $ for all $ x $ satisfying $ |x-x_0|<\delta $. }
\end{figure}
\begin{example}
	Suppose we define $ f:\R\to\R $ as some linear function $ f(x)=mx+b $. For $ L=mx_0+b $ and $ x_0\in\R $, your friend gives you $ \varepsilon=0.1 $. You need to find a $ \delta $ such that $$d_\R(f(x),L)=|mx+b-(mx_0+b)|=|m(x-x_0)|=|m||x_0-x|<0.1.$$ Therfore you let $ \delta=0.1/|m| $. Instead of letting him keep naming $ \varepsilon $, you tell him that you'll just set $ \delta=\varepsilon/|m| $. This way whenever $ d_\R(f(x),L)<\varepsilon $ we have 
	$$ d_\R(f(x),L)=|mx+b-(mx_0+b)|=|m(x-x_0)|=|m||x_0-x|=|m|\cdot d_\R(x,x_0)<|m|\cdot\frac{\varepsilon}{|m|}=\varepsilon $$ whenever $ d_\R(x,x_0)<\delta $. Therefore $ \lim_{x\to x_0}f(x)=L $.  
\end{example}
\begin{example}
	Define $ f:\R\to\R $ as 
	$$ \begin{cases}
	1\text{ if }x>0\\-1\text{ if }x<0
	\end{cases} .$$
	We can show that the limit at $ x_0=0 $ fails to exist by finding one such $ \varepsilon $ where Definition 4.1 does not holds. Let $ \varepsilon=1/2 $, and suppose for contradiction $ \lim_{x\to x_0}f(x)=L $. We have that $ |f(x)-L|<\varepsilon=1/2 $ for all $ x $ such that $ |x-0|<\delta $. But we also have that 
	\begin{align*}
		2&=|1-(-1)|\\
		&=\left\lvert f(\delta/2)-f(-\delta/2) \right\rvert\\
		&=\left\lvert f(\delta/2)-L+L-f(-\delta/2) \right\rvert\\
		&\le\left\lvert f(\delta/2)-L|+|L-f(-\delta/2) \right\rvert\\
		&\le\frac{1}{2}+\frac{1}{2}\\&=1,
	\end{align*}
	which is a clear contradiction.
\end{example}
\begin{example}
	Suppose we define $ f:\R\to\R $ as $ f(x)=\sin x $. We can verify that $ \lim_{x\to 0}f(x)=1 $ despite the fact that $ f(x) $ is undefined at $ 0 $. If we let $ \delta=\sqrt{\varepsilon} $ and restrict our attention to the interval $ (-\pi/2,\pi/2) $,\footnote{This insures that $ 0<\cos x<\sin x/x <1 $.} then for all $ \varepsilon>0 $ we have $$d_\R(f(x),L)=\left\lvert \frac{\sin x}{x}-1\right\rvert<1-\cos x<2\sin^2\frac{x}{2}<\frac{x^2}{2}=\frac{|x-0|^2}{2}=\frac{d_\R(x,0)^2}{2}<\frac{(\sqrt{\varepsilon})^2}{2}<\varepsilon$$ whenever $ d_\R(x,0)<\delta $. 
\end{example}
As you may suspect, their is a very strong link between the convergence of a sequence and the limit of a function. If we have $ \lim_{x\to p}f(x)=L $, then $ p $ is a limit point of $ E\subset X $ as defined in Definition 4.1. This limiting process lets $ x $ become arbitrarily close to $ p $, much like a sequence would to its limit. As it turns out, there is a way to reformulate limits in terms of sequences if we use a sequence $ \{p_n\} $ in $ E $ which converges to $ p $.\footnote{Such a sequence is guaranteed to exist because $ p $ is a limit point of $ E $ (see Theorem 3.1).}
\begin{theorem}
	Let $ X $ and $ Y $ be metric spaces, and $ E\subset X $. Suppose there is some function $ f:E\to Y $, and a limit point $ p $ of $ E $. Then $ \lim_{x\to p}f(x)=L $ \textit{if and only if} $ f(p_n)\to L $ (alternatively written as $ \lim_{n\to\infty}f(p_n)=L $) for every non-constant sequence $ \{p_n\} $ in $ E $ such that $ p_n\to p $.\footnote{The use of the same limit notation here may obscure the fact that $ \{f(p_n)\} $ is a sequence such that $ f(p_n)\to L $. } 
\end{theorem} 
\begin{proof}
	{\color{white}a}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ \lim_{x\to p}f(x)=L $. By Theorem 3.1, we can choose some arbitrary sequence $ \{p_n\} $ in $ E $ such that $ p_n\to p $. For all $ \varepsilon $ there exists a $ \delta>0 $ such that $ d_Y(f(x),L)<\varepsilon$ for all $ x\in E $ satisfying $ d_X(x,p)<\delta $. By the convergence of $ p_n $, there exists an $ N\in\N $ such that $ d_X(p_n,p)<\delta $ for all $ n>N $.\footnote{We take the $ \delta $ from the definition of a limit of a function and we use it as the $ \varepsilon $ in the definition of a limit of a sequence.} Therefore, for all $ n>N $, we have $d_Y(f(p_n),L)<\varepsilon$. This gives $$\lim_{n\to\infty}f(p_n)=L .$$
		\item [$ (\Longleftarrow) $] We will prove the contrapositive. Suppose that $ \lim_{x\to p}f(x)\neq L $. There exists some $ \varepsilon>0 $ such that for all $ \delta>0 $, there exists some point $ x\in E $ (which depends on $ \delta $) for which $ d_Y(f(x),L)\ge\varepsilon $ but $ d_X(x,p)<\delta $.\footnote{This is just the negation of Definition 4.1.} If we define $ \delta_n=1/n $ for all $ n\in\N $, then there is sequence $ \{p_n\} $ in $ E $ such that $ d(p_n,p)<\delta_n=1/n $. We have $ p_n\to p $ despite the fact that $ \lim_{n\to\infty}f(p_n)\neq L $. 
	\end{enumerate}
\end{proof}
\begin{example}
	For the real function $ f(x)=x $, it's clear that $ \lim_{x\to 0}f(x)=0 $. By Theorem 4.1 we know that $$ \lim\limits_{n\to\infty}f(1/n)=0,$$ as $ 1/n\to0 $. Furthermore we know this is the case for any $ \{x_n\} $ which converges to $ 0 $.  
\end{example}
So what's the big deal with writing limits in terms of sequences? This just seems like another complex relationship to remember. Theorem 4.1 is important because it allows us to prove many results involving limits (and later continuity) using properties we already know from sequences. For example, when using Theorem 4.1 we  know right away that limits of functions are unique because the limits of sequences are unique. 
\begin{corollary}
	If $ f $ has a limit at $ p $, the limit is unique.
\end{corollary}
\begin{proof}
If $ \lim_{x\to p}f(x)=L $, then by Theorem 4.1 the sequence $ \{f(p_n)\} $ converges to $ L $ for all $ \{p_n\} $ which converge to $ p $. By Proposition 3.2, $ L $ is unique. 
\end{proof}
The proof really amounts to nothing more than saying ``combine Theorem 4.1 and Proposition 3.2". We can combine Theorem 4.1 with Theorem 3.2 to arrive at familiar properties of functions. 
\begin{theorem}
	Let $ X $ and $ Y $ be metric spaces, and $ E\subset X $. Suppose there are some functions $ f:E\to Y $ and $ g:E\to Y $, and a limit point $ p $ of $ E $. If $ \lim_{x\to p}f(x)=A $ and $ \lim_{x\to p}f(x)=B $, then
	\begin{enumerate}
		\item $ \lim_{x\to p}(f+g)(x)=A+B $;
		\item $ \lim_{x\to p}(fg)(x)=AB $;
		\item $ \lim_{x\to p}(f/g)(x)=A/B $ if $ B\neq 0 $.
	\end{enumerate}
\end{theorem}
\begin{proof}
	This follows from Theorem 4.1 and Theorem 3.2.
\end{proof}
\subsection{Continuous Functions}
The definition of a limit of a function as given in Definition 4.1 has two quirks. The first, which has been acknowledged twice, is that a function needn't actually take on the value of its limit at a point. We never require $ f(p)=L $. Secondly, we do not require that $ p\in E $, we instead require that $ p $ is a limit point of $ p $. This means that not only can $ f $ be undefined at $ p $, but we shouldn't even be suprised that it is undefined at $ p $ when $ p\notin E $, as $ f:E\to Y $. We want to rule both of these out. Essentially, we want to define ``nice'' functions at a point $ p $ to be those where $ \lim_{x\to p}f(p)=L $. This is how we get our definition of continuity. 
\begin{definition}
	Let $ X $ and $ Y $ be metric spaces,  $ E\subset X $, $ p\in E $, and $ f:E\to Y $. Then $ f $ is \textit{\color{red}continuous at p} if for all $ \varepsilon>0 $, there exists a corresponding $ \delta>0 $ such that $$ d_Y(f(x),f(p))<\varepsilon $$ for all points $ x\in E $ which satisfy $ d_X(x,p)<\delta $. If $ f $ is continuous at every point of $ E $, then $ f $ is {\color{red}continuous (on $ E $)}. 
\end{definition}
This definition is illustrated in Figure 38. Just like in calculus, a function is continuous at a point $ x $ if and only if $$\lim\limits_{t\to x}f(t)=f(x) $$
\begin{remark}
Go back and review Remark 2.4. This issue came up with sequences. We saw sequences that converged in one metric space, but not another. Similarly, a function can be continuous in one metric space but not another. It also can be continuous on one subset of a metric space, but not another. If a subset $ E\subset X $ is never specified, then we assume that $ E=X $. This is why we usually consider all of $ \R $ when determining if a real function is continuous.
\end{remark}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (-.5,8.8) node {$ f(x_1)$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw[fill=green!15, thick, dashed, opacity=0.8] (5,5.35) rectangle (7,6.55);
		\draw[fill=green!15, thick, dashed, opacity=0.8] (12,8.2) rectangle (14,9.4);
		\draw[fill=green!15, thick, dashed, opacity=0.8] (2.5,3) rectangle (3.5,4.2);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9) };
	\draw[thin, dashed] (6,0) -- (6,6);
	\draw[thin, dashed] (5,0) -- (5,5.35);
	\draw[thin, dashed] (7,0) -- (7,6.55);
		\draw (13,-0.3) node {$ x_1$};
	\draw[thin, dashed] (0,6) -- (6,6);
	\draw[thin, dashed] (0,5.35) -- (5,5.35);
	\draw[thin, dashed] (0,6.55) -- (7,6.55);
	\draw (6,-0.3) node {$ x_0$};
	\draw (5,-0.3) node {$ x_0-\delta$};
	\draw (7,-0.3) node {$ x_0+\delta$};
	\draw[blue] (11,8.5) node {$ f(x)$};
	\draw (-0.9,5.35) node {$ f(x_0)-\varepsilon$};
	\draw (-0.9,6.55) node {$ f(x_0)+\varepsilon$};
	\draw (-0.5,6) node {$ f(x_0)$};
	\draw[thin, dashed] (13,0) -- (13,8.75);
		\draw[thin, dashed] (0,8.8) -- (13,8.8);
			\draw[thin, dashed] (0,8.8) -- (13,8.8);
			\draw[thin, dashed] (3,0) -- (3,3.6);
			\draw[thin, dashed] (0,3.6) -- (3,3.6);
			\draw (-.5,3.6) node {$ f(x_2)$};
			\draw (3,-0.3) node {$ x_2$};
	\end{tikzpicture}
	\caption{The function $ f $ is continuous at $ x_0 $, $ x_1 $, and $ x_2 $. Given some $ \varepsilon>0 $, there exist a delta such that $ |f(x)-f(y)|<\varepsilon $ for all $ |x-y|<\delta $. Each green box has height of $ 2\varepsilon $, and the width for the $ 2\delta $ at the corresponding point. Note that the value of $ \delta $ is the same for $ x_0 $ and $ x_1 $, but different for $ x_2 $. For a fixed $ \varepsilon $, the delta for one point may differ depending on the value of $ x $. If some value of $ \delta $ is too large, then the box would be too wide, and the function $ f(x) $ would ``escape'' from the top and/or bottom of the box. This graphical feature would imply a violation of continuity, so we need to shrink $ \delta $ until the function ``escapes'' from the sides (which is what we did for $ x_2 $).   }
\end{figure}
\begin{example}
	Suppose we define $ f:\R\to\R $ as some linear function $ f(x)=mx+b $. Let $ p\in\R $. We can mimic our work in Example 4.1 to show this function is continuous on all of $ \R $. For $ p\in\R $ satisfying $ |x-p|<\varepsilon/|m| $, we have$$ d_\R(f(x),f(p)=|mx+b-(my+b)|=|m(x-p)|=|m||x-p|=|m|\cdot d_\R(x,p)<|m|\cdot\frac{\varepsilon}{|m|}=\varepsilon .$$ Therefore $ f $ is continuous at $ p $, for all $ p\in\R $
\end{example}
\begin{example}
	Let $ f:\R\to\R $ be define as $$ \begin{cases}
	1\text{ if }x>0\\-1\text{ if }x\le0
	\end{cases} .$$
This function is continuous on the set $ \R\backslash\{0\} $. It fails to be continuous on all of $ \R $, as there is a ``jump'' from $ -1 $ to $ 1 $ at $ x=0 $. 	
\end{example}
\begin{example}
	The function $ f:\R\to\R $ defined as $ f(x)=x^2 $ is continuous on $ \R $. Let $ p\in\R $. For all $ \varepsilon>0 $, define $ \delta=\min\{1,\varepsilon/(2|p|+1)\} $. We have $$|f(x)-f(p)|=|x^2-p^2|=|x-p|\cdot|x+p|<\delta(2|p|+1)=\varepsilon ,$$ for $ x\in \R $ which satisfy $ |x-p|<\delta $.  Therefore $ f $ is continuous at $ p $, for all $ p\in\R $. 
\end{example}
\begin{example}[Thomae's function]
	Define the real function $ f:[0,1]\to[0,1] $ as $$f(x)=\begin{cases}
	\frac{1}{q} \text{ if }x=\frac{p}{q}\text{ for }p,q\in\Z\\
	0\text{ if }x\neq\Q
	\end{cases} ,$$ where we assume $ p/q\in\Q $ is in simplest terms. The function $ f $ is continuous for the irrational numbers in $ [0,1] $ but discontinues for the rational numbers in $ [0,1] $. 
	
	Suppose $ r\in[0,1] $ is irrational, giving $ f(r)=0 $. Fix $ \varepsilon>0 $. By the Archimedean Property, there is some $ m\in\N $ such that $ 1/m \varepsilon$, and some $ k_m\in $ such that $$ r\in\left(\frac{k_m}{m},\frac{k+1}{m}\right) .$$ Define the following values:
	\begin{align*}
		d_m&=\min\left\{\left\lvert r-\frac{k}{m}\right\rvert,\left\lvert r-\frac{k+1}{m}\right\rvert\right\},\\
		&\delta=\min\{d_1,\ldots,d_m\}.
	\end{align*}
	 If $ x\in[0,1] $ is a rational number with $ |x-r|<\delta $, then $ x=p/q $ and $ q>m $, so $$|f(x)-f(r)|=\left\lvert \frac{1}{q}-0\right\rvert=\frac{1}{q}<\frac{1}{m}<\varepsilon ,$$ whenever $ |x-r|<\delta $. If $ x\in[0,1] $ is irrational, then $$ |f(x)-f(r)|=|0-0|=0<\varepsilon ,$$ regardless of $ \delta $. Thus, for any $ x\in[0,1] $ satisfying $ |x-r|<\delta $, $ |f(x)-f(r)|<\varepsilon $. The function is continuous at any irrational $ r $.
	 
	 Now suppose $ r=p/q\in[0,1] $ is rational. We have $ f(r)=1/q $. Let $ \varepsilon=1/2q $ and define $ r_k=r+\frac{1}{k\sqrt{2}} $. The number $ r_k $ is irrational, so $ f(r_k)=0 $. Can we find a $ \delta $ such that $$ |f(x)-f(r)|=\left\lvert f(x)-\frac{1}{q}\right\rvert<\varepsilon=\frac{1}{2q}?$$ No we cannot. We can always find and $ x_k $ such that $$|f(x_k)-f(r)|=\frac{1}{q}>\frac{1}{2q}. $$  
\end{example}
\begin{remark}[Pathological Examples]
	Thomae's function is constructed for the express purpose of being an unintuitive example. Such examples are known as pathological in math, and are particularly common in analysis. While such examples would never arise in the real world, they often give insight into just what is possible. We'll see several more: a function that is continuous everywhere but differentiable nowhere, a set that we can not possible know the size of, a set that is uncountably infinite but has no size, etc. 
\end{remark}
\begin{remark}[Can $\delta  $ depend on $p $?]
	Yes! In Example 4.5, our selection of $ \delta $ was independent of $ x $, and works for every point in $ \R $. This was not the case for Example 4.7, but that is fine. Our selection of $ \delta $ will change for each value of $ p\in\R $. All that matters is \textit{for each} $ \varepsilon>0 $, we can find a correspond $ \delta $. We'll come back to this idea shortly.  
\end{remark}
\begin{remark}[It Only Takes One $ \varepsilon $, Discontinuity]
Showing a function is \textit{not} continuous only requires we find a single $ \varepsilon $ such that $ |f(x)-f(y)|\ge\varepsilon $ for all $ |x-y|<\delta $ for $ y\in E $. This follows from the negation of Definition 4.1. If we let $ x\to0 $, then we end up with $ \delta=0 $, a clear contradiction. 
\end{remark}
Our next makes explicit the fact that if $ f $ is continuous at a point, then the limit of the function exists at that point.
\begin{theorem}
	Let $ X $ and $ Y $ be metric spaces,  $ E\subset X $, $ p \in E $ be a limit point of $ E $, and $ f:E\to Y $. Then $ f $ is continuous \textit{if and only if} $ \lim_{x\to p}f(x)=f(p) $.
\end{theorem}
\begin{proof}
	Simply compare the definition of a limit (Definition 4.1), with the definition of continuity (Definition 4.2).
\end{proof}
\begin{corollary}[Continuous Functions Preserve Limits]
		Let $ X $ and $ Y $ be metric spaces, and $ E\subset X $. Suppose there is some function $ f:E\to Y $, and a $ p\in E $ which is a limit point of $ E $. Then $ f $ is continuous at $ p $ \textit{if and only if} $ f(p_n)\to f(p) $ for every non-constant sequence $ \{p_n\} $ in $ E $ such that $ p_n\to p $. That is $$\lim\limits_{n\to \infty}f(p_n)=f\left(\lim\limits_{n\to\infty} p_n\right)=f(p) .$$
\end{corollary}
\begin{proof}
This follows from Theorem 4.1 and Theorem 4.3. 
\end{proof}
\begin{corollary}[Properties of Continuous Functions] 
Let $ X $ and $ Y $ be metric spaces, and $ E\subset X $. Suppose there are some functions $ f:E\to Y $ and $ g:E\to Y $, and a point $ p \in E $. If $ f $ and $ g $ are continuous at $ p $, then 
\begin{enumerate}
	\item $ f+g $ is continuous at $ p $;
	\item $ fg $ is continuous at $ p $;
	\item $ f/g $ is continuous at $ p $ if $ p\neq0 $.
\end{enumerate}
\end{corollary}
\begin{proof}
	This follows from Theorem 4.2 and Theorem 4.3.
\end{proof}
\begin{remark}[Moving Around Limits]
Corollary 4.2 is the first time we've encountered a type of question analysis will provide answers for again and again -- when can I move a limit into a function/operation? The majority of Section 7 will be dedicated to these type of questions. Corollary 4.2 tells us that a function is continuous if and only if the limit of a function is the function of the limits.   
\end{remark}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (-.5,8.23) node {$ f(p)$};
	\draw (14,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (14,0);
	\draw[thick,dashed, ->] (1,-1) -- (11,-1);	
	\draw[thick,dashed, ->] (-1.3,1) -- (-1.3,8.23);
	\draw[blue] (12,9) node {$ f(x)$};
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9) };
	\draw[thick, dashed] (11,0) -- (11,8.23);
	\draw[thick, dashed] (0,8.23) -- (11,8.23);
	\draw[ultra thin, dashed] (0,1) -- (1,1) -- (1,0);
	\draw[ultra thin, dashed] (0,2.5) -- (2,2.5) -- (2,0);
	\draw[ultra thin, dashed] (0,3.6) -- (3,3.6) -- (3,0);
	\draw[ultra thin, dashed] (0,4.5) -- (4,4.5) -- (4,0);
	\draw[ultra thin, dashed] (0,5.3) -- (5,5.3) -- (5,0);
	\draw[ultra thin, dashed] (0,6) -- (6,6) -- (6,0);
	\draw[ultra thin, dashed] (0,6.6) -- (7,6.6) -- (7,0);
	\draw[ultra thin, dashed] (0,7) -- (8,7) -- (8,0);
	\draw[ultra thin, dashed] (0,7.5) -- (9,7.5) -- (9,0);
	\draw[ultra thin, dashed] (0,7.9) -- (10,7.9) -- (10,0);
	\draw (11,-0.3) node {$ p$};
	\draw (6,-1.3) node {$ p_n\to p$};
	\draw (1,-0.3) node {\tiny $ p_1$};
	\draw (2,-0.3) node {\tiny $ p_2$};
	\draw (3,-0.3) node {\tiny $ p_4$};
	\draw (4,-0.3) node {\tiny $ p_8$};
	\draw (5,-0.3) node {\tiny $ p_{16}$};
	\draw (6,-0.3) node {\tiny $ p_{32}$};
	\draw (7,-0.3) node {\tiny $ p_{64}$};
	\draw (8,-0.3) node {\tiny $ p_{128}$};
	\draw (9,-0.3) node {\tiny $ p_{256}$};
	\draw (10,-0.3) node {\tiny $ p_{512}$};
	\draw (-.5,1) node {\tiny$ f(p_1)$};
	\draw (-.5,2.5) node {\tiny$ f(p_{2})$};
	\draw (-.5,3.6) node {\tiny$ f(p_{4})$};
	\draw (-.5,4.5) node {\tiny$ f(p_{8})$};
	\draw (-.5,5.3) node {\tiny$ f(p_{16})$};
	\draw (-.5,6) node {\tiny$ f(p_{32})$};
	\draw (-.5,6.6) node {\tiny$ f(p_{64})$};
	\draw (-.5,7) node {\tiny$ f(p_{128})$};
	\draw (-.5,7.5) node {\tiny$ f(p_{256})$};
	\draw (-.5,7.9) node {\tiny$ f(p_{512})$};
	\draw (-2.5,4.165) node {$ f(p_{n})\to f(p)$};
	\end{tikzpicture}
	\caption{For $ p_n\to p $, we have $ f(p_n)\to f(p) $, therefore $ f $ is continuous at $ p $. }
\end{figure}
\begin{example}
	Define $ f:\R\to\R $ as $$\begin{cases}x\text{ if }x\in\Q\\ 0\text{ if }x\notin\Q\end{cases}. $$ This function is continuous at $ 0 $, because for all real $ x_n\to 0 $, $ f(x_n)\to f(0)=0 $. This happens to be the only point in $ \R $ at which $ f $ is continuous. Let $ c\in\Q\backslash\{0\} $. There is a sequence of irrational numbers $ \{x_n\} $ such that $ x_n\to c $. Despite this, $ f(x_n)=0\to0\neq c $, so $ f $ is not continuous at $ c $. Similarly, if $ c\in\R\backslash\Q $, then there exists a sequence of rationals $ x_n $ such that $ x_n\to c $. Despite this, $ f(x_n)=x_n\to c\neq f(c)=0 $, so $ f $ is not continuous at $ c $. 
\end{example}
While Corollary 4.3 proves valuable when showing a function is continuous, when combined with our next result, we can show that almost any function we can write down is continuous.  
\begin{theorem}[Composition Preserves Continuity]
	Let $ X $, $ Y $, and $ Z $ be metric spaces, and $ E\subset X $. If $ f:E\to Y $ and $ g:f(E)\to Y $ are continuous at $ p\in E $ and $ f(p)\in f(E) $ respectively, then the function $ h:g(f(E))\to Z $ defined as  $$ h(x)=g(f(x))=(g\circ f)(x)$$ is continuous at $ p $. 
\end{theorem}
\begin{proof}
	Fix $ \varepsilon>0 $. Since $ g $ is continuous at $ f(p) $, there exists a $ \delta' $ such that $$d_Z(g(y),g(f(p))<\varepsilon $$ if $ d_Y(y,f(p))<\delta' $ for $ y\in f(E) $. Since $ f $ is continuous at $ p $, there exists $ \delta>0 $ such that $$d_Y(f(x),f(p))<\delta' $$ if $ d_X(x,p)<\delta $ and $ x\in E $.\footnote{We took the $ \delta' $ from the definition of $ g $'s continuity, and let $ \varepsilon=\delta' $ for the definition of $ f $'s continuity. This means that any $ f(x) $ which satisfy $ d_Y(f(x),f(p))<\delta'  $ will satisfy $ d_Y(y,f(p))<\delta' $.} These inequalities give $$d_Z(g(f(x)),g(f(p)))=d_Z(h(x),h(p))<\varepsilon $$ if $ d_X(x,p)<\delta $ and $ x\in E $. Therefore $ h=g\circ f $ is continuous at $ p $. 
\end{proof}
If $ h=g\circ f $ is continuous, then any composition of $ h $ with another continuous function will also be continuous. If we keep composing the result with continuous functions, we will always have a continuous function. For this reason, nearly every function encountered in actual applications will be continuous.   
\begin{example}
	Let $ f:\R\to\R $ be defined as $$ f(x)=\left(\frac{e^{\sin x}}{\cos x}\right)^4 .$$ If we let $ g(x)=\cos x $, $ h(x)=\sin  x $, $ j(x)=e^x $, $ k(x)=x^4 $ (all continuous functions from $ \R $ to $ \R $), then $$f(x)=k\circ((j\circ h)/g).$$ By Theorem 4.4 and Corollary 4.3, $ f(x) $ is continuous. 
\end{example}
If a continuous function is a ``nice'' mapping from some metric space $ E\subset X $ to another metric space $ Y $, it's worth asking what happens to sets when they mapped using $ f $? If $ E $ is open, will the image $ f(E) $ be open? Does this hold for closed sets? What about our favorite sets, compact sets? We will now begin to put continuity in conversation with Section 2, and the point-set topology of metric spaces. We'll end this subsection by introducing a \textit{very important} theorem related to the topology of a metric space. We will then introduce a special type of continuity, before seeing how continuity interacts with compactness.
\begin{theorem}
	Let $ X $ and $ Y $ be metric spaces. The function $ f:X\to Y $ is continuous on $ X $ \textit{if and only if} $ f^{-1}(V) $ is open in $ X $ for every open set $ V $ in $ Y $.
\end{theorem}
\begin{proof}
	{\color{white}a}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ f $ is continuous on $ X $ and $ V$ is an open set in $ Y $. We need to show that $ f^{-1}(V) $ is open by showing every point of the set is an interior point. Let $ p\in f^{-1}(V)\subset X $, and $ f(p)\in V $. The set $ V $ is open, so there exists an $ r=\varepsilon>0 $ such that $ N_\varepsilon(f(p))\subset V $. Alternatively we may write, $ y\in V $ for all $ d_Y(f(p),y)<\varepsilon $. The function $ f $ is continuous at $ p $ because $ p\in X $, so there exists a $ \delta>0 $ such that $ d_Y(f(x),f(p))<\varepsilon $ is $ d_X(x,p)<\delta $. In terms of sets, this means that $ N_\delta(p)\subset f^{-1}(V) $, so $ p\in f^{-1}(V) $ is an interior point. Therefore $ f^{-1}(V) $ is open.
		\item [$ (\Longleftarrow) $] Suppose $ f^{-1}(V) $ is open in $ X $ for all open sets $ V $ in $ Y $. For $ p\subset X $ and $ \varepsilon>0 $, let $ V=N_\varepsilon(f(p)) $. Alternatively, $ V $ is the set of all $ y $ such that $ d_Y(y,f(p))<\varepsilon $. The set $ V $ is a neighborhood, so it is open, hence $ f^{-1}(V) $ is open. If $ f^{-1}(V) $ is open, then there exists a $ \delta $ such that $ N_\delta(p)\subset f^{-1}(V) $. In other words, as soon as $ d_X(p,x)<\delta $, $ x\in f^{-1}(V) $. Because $ x\in f^{-1}(V) $, $ f(x)\in f(V) $, so we have $ d_Y(f(x),f(p))<\varepsilon $ as soon as $ d_X(p,x)<\delta $. This means $ f $ is continuous on $ X $. =
	\end{enumerate}
\end{proof}
\begin{corollary}
	Let $ X $ and $ Y $ be metric spaces. The function $ f:X\to Y $ is continuous on $ X $ \textit{if and only if} $ f^{-1}(V) $ is closed in $ X $ for every closed set $ V $ in $ Y $.
\end{corollary}
\begin{proof}
	The complement of an open set is closed, and $ f^{-1}(V^c)=(f^{-1}(V))^c $. 
\end{proof}
Theorem 4.5 is illustrated in Figure 40 for a real function. 
\begin{remark}[Topological Definition of Continuity]
We've already referenced several times the fact that we are restricting our attention to metric spaces, but that point-set topology can be studied in a more general setting. As it turns out, when working in a topological space that is not a metric space, we define a continuous function as one with the property established in Theorem 4.5. If you were to ever take a proper topology course, continuity is never discussed in terms of $ \varepsilon-\delta $, but is instead related to open sets.
\end{remark}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (14,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (14,0);
	\draw[blue] (12,9) node {$ f(x)$};
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9) };
	\draw[ultra thin, dashed] (0,2.5) -- (2,2.5) -- (2,0);
	\draw[ultra thin, dashed] (0,6.6) -- (7,6.6) -- (7,0);
	\draw[ultra thick,red] (2,0) -- (7,0);
	\draw[ultra thick,red] (0,2.5) -- (0,6.6);
	\filldraw[color=red, fill=white] (2,0) circle (2pt);
	\filldraw[color=red, fill=white] (7,0) circle (2pt);
	\filldraw[color=red, fill=white] (0,2.5) circle (2pt);
	\filldraw[color=red, fill=white] (0,6.6) circle (2pt);
	\draw[red] (-0.4,4.55) node {$ V $};
	\draw[red] (4.5,-0.4) node {$ f^{-1}(V) $};
	\end{tikzpicture}
	\caption{A continuous function $ f:\R\to\R $, and the open interval $ V\subset \R $. Because $ f $ is continuous, $ f^{-1}(V) $ is open in $ \R $. }
\end{figure}
\begin{example}
Theorem 4.5 only says that the preimage of an open set will be open when a function is continuous. It never says that the image of and open set is open. If we define $ f:\R\to\R $ as $ f(x)^2 $, and let $ E=(-1,1) $ be an open set, then $$f(E)=[0,1) .$$ Therefore $ f(E) $ is not open, despite $ f $ being continuous on $ \R $. 
\end{example}
\begin{example}
	Theorem 4.5 greatly simplifies the proof of Theorem 4.4. For an open $ E\subset Z $, $ g^{-1}(E)\subset Y $ is open by the continuity of $ g $. The set $ f^{-1}(g^{-1}(E))\subset X $ is open because $ f $ is continuous. Therefore, for $ h=g(f(x)) $, $ h^{-1}(E)\subset X $ is open, making $ h $ continuous. 
\end{example}
\subsection{Uniform Continuity}
Before exploring compactness and continuity, we need to address an earlier remark, namely Remark 4.3. We noticed that the value of $ \delta $ which corresponds to $ \varepsilon $ sometimes depends on where we are in the domain of a function. While does not violate continuity, it is a bit odd. If $ f $ is continuous on a set $ E $, then we can always find a corresponding $ \delta $ for each $ \varepsilon $, but that $ \delta $ may not work for all values of $ E $. What we are really doing is find a corresponding $ \delta $ for each $ \varepsilon $ \textit{and} for each $ p\in E $. However, Example 4.5 showed us that sometimes our corresponding $ \delta $ works \textit{for all} values on the domain. This is a special case that merits its own definition. 
\begin{definition}
	Let $ X $ and $ Y $ be metric spaces. The function $ f:X\to Y $ is \textit{\color{red}uniformly continuous (on $ X $)} if for every $ \varepsilon>0 $, there exists a $ \delta>0 $ such that $d_X(x,p)<\delta $ implies $  d_Y(f(x),f(p))<\varepsilon$ \textit{for all} $ x,p\in X $. 
\end{definition}
There are two major differences between uniform continuity and continuity. We already discussed the first -- we find a single value $ \delta $ which works for all elements of $ f $'s domain. This difference arises from the fact that in Definition 4.2, we only looked at $ p\in E $ which satisfy $ d_X(x,p)<\delta $. We don't do this in definition 4.3, as we look at \textit{all} $ p\in E $. The second is that uniform continuity pertains to sets, not points. While a function can be continuous at a single point, saying $ f $ is uniformly continuous at a point it vacuously true and meaningless, as the concept deals with multiple points in a domain. It also should be clear that any function which is uniformly continuous is continuous. 

It cannot be emphasized enough how much stronger uniform continuity is than continuity. If $ f:\R\to\R $ is continuous, then for every $ \varepsilon>0 $ (of which there are an uncountably infinite amount), we may need to find a corresponding $ \delta $ for each point in $ \R $! It is feasible that we have an uncountably infinite amount of $ \delta $ for each $ \varepsilon $. If instead $ f $ is uniformly continuous, for each fixed $ \varepsilon>0 $ (of which there are still an uncountably infinite amount), then we know there is one ``silver bullet'' $ \delta $ that works for all of $ \R $. This difference gives rise to an interesting geometric interpretation presented in Figure 41 and Figure 42. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (1.5,1.68) rectangle (2.5,3.28);
	\draw[thick, dashed, opacity=0.8] (13.5,1.68) rectangle (14.5,3.28);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (3.5,3.78) rectangle (4.5,5.38);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (5.5,5.2) rectangle (6.5,6.8);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (7.5,6.27) rectangle (8.5,7.87);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (9.5,7.08) rectangle (10.5,8.68);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (11.5,7.72) rectangle (12.5,9.32);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9)};
	\filldraw (4,4.55) circle (1pt);
	\filldraw (2,2.48) circle (1pt);
	\filldraw (6,6) circle (1pt);
	\filldraw (8,7.07) circle (1pt);
	\filldraw (10,7.88) circle (1pt);
	\filldraw (12,8.52) circle (1pt);
	\draw[blue] (14,9.3) node {$ f(x)=\sqrt{x}$};
	\draw[black] (13.3,2.48) node {$ 2\varepsilon$};
	\draw[black] (14,1.38) node {$ 2\delta$};
	\end{tikzpicture}
	\caption{Let $ f:(0,1]\to\R $ be defined as $ f(x)=\sqrt{x} $. The function is uniformly continuous. For each fixed $ \varepsilon $, we can center a box of height $ \varepsilon $ at a point $ (x,f(x)) $. We can find some width of the box $ \delta $ such that the function will never ``escape'' from the top or bottom of the box, no matter the point $ (x,f(x)) $.}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[fill=red!15, thick, dashed, opacity=0.8] (0.5,5.66) rectangle (1.5,7.06);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (2.5,3.03) rectangle (3.5,4.43);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (4.5,1.73) rectangle (5.5,3.13);
	\draw[fill=green!15, thick, dashed, opacity=0.8] (6.5,.92) rectangle (7.5,2.32);
		\draw[fill=green!15, thick, dashed, opacity=0.8] (8.5,.4) rectangle (9.5,1.8);
		\draw[thick, dashed] (0.5,5.66) -- (0.5,9);
		\draw[thick, dashed] (1.5,5.66) -- (1.5,5.4);
	\draw[ thick, blue]   (0.5,9) to[out=270,in=180] (15,0.5);
	\draw[blue] (14,0.8) node {$ f(x)=1/x$};
	\filldraw (1,6.36) circle (1pt);
	\filldraw (3,3.73) circle (1pt);
	\filldraw (5,2.43) circle (1pt);
	\filldraw (7,1.62) circle (1pt);
	\filldraw (9,1.1) circle (1pt);
	\draw[thick, dashed, opacity=0.8] (13.5,5.66) rectangle (14.5,7.06);
	\draw[black] (13.3,6.36) node {$ \varepsilon$};
	\draw[black] (14,5.36) node {$ \delta$};
	\filldraw[red] (0.63,7.6) circle (1.3pt) node [anchor= south west] {\tiny$ f(a) $};
	\filldraw[red] (1.425,5.53) circle (1.3pt) node [anchor= north east] {\tiny$ f(b) $};
	\end{tikzpicture}
	\caption{The function $ f:\R\to\R $ defined as $ f(x)=1/x $ is not uniformly continuous. For our set value of $ \varepsilon $ and $ \delta $ the function ``escapes'' from the top and bottom of the red rectangle. This is because there exists $ a,b\in\R $ such that $ |f(a)-f(b)|\ge\varepsilon $ despite the fact that $ |a-b|<\delta $. We could make the corresponding $ \delta $ smaller, but the function eventually ``escape'' as we move to the left. }
\end{figure}
\begin{example}
Let $ f:[0,\infty)\to\R $ be defined as $ f(x)=\sqrt{x} $. The function is uniformly continuous on $ [0,\infty) $. If we let $ \delta=\varepsilon^2 $, then for all $ |x-p|<\delta $, we have $$|f(x)-f(p)|^2=|\sqrt{x}-\sqrt{p}|^2\le|\sqrt{x}-\sqrt{p}|\cdot|\sqrt{x}+\sqrt{p}|=|x-p|<\varepsilon^2 .$$ Taking the square root of this inequality gives $ |f(x)-f(p)|<\varepsilon $ for all $ |x-p|<\delta $. Our choice of $ \delta $ does not depend on $ p\in\R $, so $ f $ is uniformly continuous. 
\end{example}
\begin{example}
	The function $ f:\R\to\R $ defined as $ f(x)=1/x $ is not uniformly continuous on $ (0,1] $. Suppose for contradiction that $ f $ is uniformly continuous on $ (0,1) $. Let $ \varepsilon=1/n $. There exists a $ \delta $ such that $$ |f(x)-f(p)|=\left\lvert\frac{1}{x}-\frac{1}{p}\right\rvert=\left\lvert\frac{x-p}{xp}\right\rvert<\frac{\delta}{xp}<\varepsilon=\frac{1}{n} $$ for all $ x,p\in(0,1) $ satisfying $ |x-p|<\delta $. Therefore we need $ \delta<|xp|/n $ for all $ x,y\in(0,1) $ satisfying $ |x-p|<\delta $. If we let $ y=x+\delta/2 $, then $ \delta<|xp|/n $ becomes $$\delta <\frac{2x^2}{2n-x} $$ for all $ x\in(0,1) $. 
\end{example}
\subsection{Continuity and Compactness}
If continuous functions are ``nice'', and compact sets are ``nice'', then it should not come as a surprise that continuous functions behave very well with compact sets. We will first define what it means for a function to be bounded, and then we will jump into a series of results about compactness.
\begin{definition}
	Let $ E\subset\R $. The real function $ f:E\to\R $ is \textit{\color{red}bounded} if there exists an $ M\in f(x) $ such that $ |f(x)|\le M $ for all $ x\in E $.
\end{definition}
Figure 43 shows a bounded real function.
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw (-.3,10) node {$ \R$};
\draw (16,4.7) node {$ \R$};		
\draw[thick,->] (0,0) -- (0,10);
\draw[thick,dashed] (0,9.5) -- (15,9.5);
\draw[thick,dashed] (0,0.5) -- (15,0.5);
\draw[thick,->] (0,5) -- (16,5);	
\draw [thick, blue] (1,6) sin (5,9) cos (9,5) sin (14,1);
\draw[thick] (1,4.8) -- (1,5.2);
\draw[thick] (14,4.8) -- (14,5.2);
\draw[black] (1,4.5) node {$ a $};
\draw[black] (-.3,9.5) node {$ M $};
\draw[black] (-.4,0.5) node {$ -M $};
\draw[black] (14,4.5) node {$ b $};
\filldraw[blue] (1,6) circle (2pt);
\filldraw[blue] (14,1) circle (2pt);
\end{tikzpicture}
\caption{A bounded function $ f:[a,b]\to\R $ }
\end{figure}
\begin{example}
	The function $ \sin (x) $ is bounded on all of $ \R $, as $ |\sin x|\le 1 $ for all $ x\in\R $.
\end{example}
\begin{example}
	The function $ f(x)=x^2 $ on $ \R $ is continuous but not bounded. 
\end{example}
\begin{theorem}[Continuity Preserves Compactness]
	Let $ f:X\to Y $ be a continuous function where $ X $ and $ Y $ are metric spaces. If $ X $ is compact, then $ f(X) $ is compact. 
\end{theorem}
\begin{proof}
	We will show that an arbitrary open cover $ \{V_\alpha\} $ of $ f(X) $ has a finite subcover. Since $ f $ is continuous, $ f^{-1}(V_\alpha) $ is open for all $ \alpha $ (Theorem 4.5). The set $ X $ is compact, so there exists finitely many indices $ \alpha_1,\ldots,\alpha_n $ such that $$X\subset f^{-1}(V_{\alpha_1})\cup\ldots\cup f^{-1}(V_{\alpha_n}) .$$ But we have $ f(f^{-1})(E)\subset E $ for any $ E\subset Y $, so we can take the image of the finite subcover of $ X $ and conclude $$ f(X)\subset V_{\alpha_1}\cup\ldots\cup V_{\alpha_n} .$$ Therefore $ f(X) $ is compact. 
\end{proof}
This result is similar to Theorem 4.5, but they ``go in different directions''. The preimage of an open set is open for a continuous $ f $. Now we are saying that the image of a compact set is compact for a continuous $ f $. It's tempting to say this holds for open sets, but it is not true. In general openness is not preserved by continuous functions (Example 4.11).
\begin{corollary}
	Let $ E\subset\R $. If the real function $ f:E\to\R $ is continuous and $ E $ is closed and bounded, then $ f(E) $ is closed and bounded.
\end{corollary}
\begin{example}
	Let $ f:(0,\infty)\to\R $ be the continuous function $ f(x)=1/x $. The set $ (1,\infty) $ is neither closed nor bounded, so $ f^{-1}((1,\infty))=(0,1) $ is neither closed nor bounded. 
\end{example}
Our next theorem proves useful in many applications, as it allows us to determine when a function has a maximum and minimum on an intervals. 
\begin{theorem}[Extreme Value Theorem]
	Suppose $ f $ is a continuous real function on a compact metric space $ X $, and \begin{align*}
		M&=\sup\limits_{x\in X}f(x),\\
		m&=\inf\limits_{x\in X}f(x).
	\end{align*}
	Then there exists points $ x,y\in X $ such that $ f(x)=M $ and $ f(y)=m $.
\end{theorem}
\begin{proof}
	Continuous functions preserve compactness (Theorem 4.6), so $ f(X)\subset\R $ is closed and bounded. A closed and bounded set contain their infimum and supremum, so $ f(X) $ contains $ M $ and $ m $. 
\end{proof}
\begin{remark}[$ \sup $ or $ \max $?]
	It may not be clear when we use maximum and when we use supremum. A maximum of some set or function is always attained. This is not always the case for a supremum. If we know that $ \sup E\in E $, then we are free to write $ \max E=\sup E $, but this won't always hold. For instance, $ (0,1) $ has a supremum of $ 1 $, but no well defined maximum. If you're ever unsure if the supremum is attained in the set or by the function, use $ \sup $. A maximum is always a supremum, so it technically is not incorrect. The same holds for $ \inf $ and $ \min $.
\end{remark}
Theorem 4.7 tells us that if a function is defined on a compact space, then it \textit{must} achieve a maximum and minimum on that domain. Just knowing that such points exist is a great deal of information. 
\begin{example}
	The function $ \sin(x) $ does not achieve a maximum or minimum on the interval $ (0,\pi/2) $, despite it being bounded. We can always find some larger value of $ f(x) $ as we get arbitrarily close to $ \pi/2 $, or some smaller value of $ f(x) $ as we get arbitraily close to $ 0 $. This is the exact type of behavior we ruled out when defining compactness! By defining $ \sin(x) $ on $ [0,\pi/2] $, we now achieve a maximum and minimum on the interval.  
\end{example}
\begin{example}
	The function $ f(x)=x^2 $ achieves a minimum on $ \R $, but it fails to achieve both a maximum and a minimum on all of $ \R $. This is because $ \R $ is neither closed nor bounded, rendering it not compact. 
\end{example}
Recall the fact that $ f(x)=1/x $ is not uniformly continuous on $ (0,1) $. In Figure 42 we argued that no matter the value of $ \delta $ given for a fixed $ \varepsilon $, we could move the rectangle of height $ \varepsilon $ and length $ \delta $ to the left until the function ``escaped'' from the top and bottom. Us being able to keep moving the rectangle to the left is a result of $ (0,1) $ not being closed. We can always get a little bit closer to $ 0 $ without leaving the domain. As we do this, the function will take on larger values indefinitely because $ f((0,1))=(1,\infty) $ is unbounded. These two observations seem to hint at the fact that if eliminate these behaviors, a function will always be uniformly continuous. As it turns out, compactness does just this, and the proof of this is one of the more elegant proofs in analysis.\footnote{Full disclosure: this is my favorite proof from basic real analysis.} 
\begin{theorem}
	Let $ f $ be a continuous function which maps a compact metric space $ X $ to a metric space $ Y $. Then $ f $ is uniformly continuous on $ X $.
\end{theorem}
\begin{proof}
	Fix $ \varepsilon>0 $. The function $ f $ is continuous on $ X $, so for each point $ p\in X $ there is a $ \delta(p) $ such that $$d_Y(f(p),f(x))<\varepsilon/2 $$ whenever $ x\in X $ and $ d_X(p,x)<\frac{1}{2}\delta(p) $.\footnote{We are writing $ \delta $ as a function of $ p\in X $ to emphasize that all we know now if $ f $ is continuous.} Let $$ J(p)=\left\{x\in X \mid d_X(x,p)<\frac{1}{2}\delta(p)\right\} .$$ We have $ p\in J(p) $ for all $ p\in X $, so $$X\subset\bigcup_{p\in X}J(p),$$ where $ J(p) $ is open for all $ p\in X $.\footnote{This follows from the fact that $ J(p) $ is a neighborhood of $ p $, and all neighborhoods are open sets. We could also write $ J(p)=N_{1/2\delta(p)}(p) $.} That is to say, $ \{J(p)\}_{p\in X} $ is an open cover of $ X $. The space $ X $ is compact, so there exists a finite set of points $ p_1,\ldots,p_n $ such that $$ X\subset J(p_1)\cup\cdots\cup J(p_n).$$ If we let $$ \delta=\frac{1}{2}\min\{\delta(p_1),\ldots,\delta(p_n)\},$$ then $ \delta>0 $.\footnote{This finite set achieves is infimum so we use $ \min $. A minimum of a finite set of positive numbers in positive. This is not necessarily the case with an infimum of an infinite set of positive numbers even if that set is compact. For us to be able to conclude $ \delta>0 $, it is crucial that the set $ \{\delta(p_1),\ldots,\delta(p_n)\} $ is finite, that way we can take the minimum over a finite set corresponding to the finite subcover. }
	
	Let $ x,p\in X $ such that $ d_X(x,p)<\delta $. The point $ p $ is in $ X $, so it must be ``covered'' by one of the open sets in the finite subcover $ \{J(p_1),\ldots,J(p_n)\} $. That is, there is an $ m\in\N $, where $ 1\le m\le n $, such that $ p\in J(p_m) $. Hence, $d_X(x,p_m)<\frac{1}{2}\delta(p_m) $. The triangle inequality gives $$d_X(x,p_m)\le d_X(x,p)+d_X(p,p_m)<\delta+\frac{1}{2}\delta(p_m)\le\delta(p_m) .$$ But then, we can use the triangle inequality and the continuity of $ f $ to conclude $$d_Y(f(x),f(p))\le d_Y(f(x),f(p_m))+d_Y(f(p_m),f(p))<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon ,$$ so $ f $ is uniformly continuous.
\end{proof}
This proof may look technical, but the underlying idea is much simpler. Think about the function $ f(x)=1/x $ on $ (0,1] $, and Figure 42. For a fixed $ \varepsilon $, we can always move the rectangles to the left until the function ``escapes'' from the top or bottom. The closer we get to $ 0 $ on the $ x -$axis, the smaller we need to make $ \delta $, and we can always move closer and closer to $ 0 $, so there isn't one $ \delta $ that will work at every point. Suppose instead we looked at $ f(x)=1/x $ on the interval $ [0.001,1] $. We may be able to get closer and closer to $ 0.001 $, making $ \delta $ smaller as we go to satisfy continuity, but because $ 0.001 $ is defined by $ f $, we could just take the $ \delta $ that works at $ 0.001 $ and use it for all of $ [0.001,1] $. If the $ \delta $ needs to become small as we move our rectangle to the left, then the ``leftmost'' point must have the smallest $ \delta $, a choice of $ \delta $ that will work for all of $ [0.001,1] $ and our fixed $ \varepsilon $. The next example will walk through this process, and the proof of Theorem 4.8, with an actual function. 
\begin{example}
	Suppose $ f:[-10,10]\to\R $ is defined as $ f(x)=x^2 $. Example 4.7 showed that $ f $ is continuous. In this case we had $ \delta=\min\{1,\varepsilon/(2|p|+1)\} $. Fix $ \varepsilon=1 $, so $ \varepsilon/2=1/2 $.\footnote{We use $ \varepsilon=1 $ as the final value of $ \varepsilon $ used to show uniform continuity. The value $ \varepsilon/2 $ is used in the definition of continuity of $ x^2 $.} We can let $ \delta(p)=1/(4|p|+1) $, and have $$|f(x)-f(p)|<\varepsilon=1/2 $$ for all $ p\in [-10,10] $ satisfying $ |x-p|<\frac{1}{2}\delta(p)<\delta(p) $. First, we should explicitly show that $ \delta(p) $ will change with $ p $. Suppose $ p=1 $, giving $ \delta(1)=1/5 $. For all $ x\in[-10,10] $ such that $ |x-1|<1/5 $, we do indeed have $ |x^2-1|<1/2 $. Now let's try to use $ \delta(1) $ for $ p=9 $. The set of all $ x\in[-10,10] $ which satisfy $ |x-9|<1/5 $ is $ (44/5,46/5) $. For $ 91/10\in(44/5,46/5) $, we have $$ |f(91/10)-f(9)|=\left(\frac{91}{10}\right)^2-9^2=1.81>\frac{1}{2}=\frac{\varepsilon}{2}.$$ This means that $ \delta(1) $ won't work for $ p=9 $! We now will mimic the proof of Theorem 4.8 and find a $ \delta $ that will work for all $ p\in[-10,10] $. 
	
	 Define $$J(p)=\left\{x\in[-10,10]\ \Big|\  |x-p|<\frac{1}{2}\delta(p)=\frac{1}{8|p|+2}\right\}=N_{1/2\delta(p)}=N_{1/(8|p|+2)}(p). $$ We have $ p\in J(p) $ for all $ p\in[-10,10] $, so $$ [-10,10]\subset\bigcup_{p\in [-10,10]}J(p) .$$ For the finite set of points $ P=\{-10,-9.9,\ldots,9.9,10\}=\{-10+(0.1)n\mid n=1,\ldots, 200\} $,\footnote{Where the heck do I get this set? Well the minimum value of $ 1/(8|p|+2) $ on the interval $ [-10,10] $ is about 0.012. If we round this down to 0.01, then all the $ J(p) $ centered at points in $ [-10,10] $ that are distance 0.01 apart will be guaranteed to cover $ [-10,10] $, as we took the distance between points to be less than the smallest radius of $ J(p) $. } we have $$ [-10,10]\subset\bigcup_{p\in P}J(p) ,$$ making $ \{J(p)\}_{p\in P} $ a finite subscover of $ [-10,10] $. Define $$ \delta=\min\limits_{p\in P}\{\delta(p)\}=\delta(10)=\delta(-10)=\frac{1}{81} .$$
	 
	 We will show that this value of $ \delta $ will work for a random point in $ [-10,10] $, say $ p=1.95 $. We could verify this right away, but we'll instead follow the steps of the proof, even though they become painfully redundant when working with actual numbers.  Now let $ x\in[-10,10] $ such that $ |x-1.95|<1/81 $.\footnote{For the rest of this paragraph, whenever we refer to $ x $, we mean these particular $ x $ satisfying  $ |x-1.95|<1/81 $} We have $ x\in[-10,10] $, so it must be in one of the elements of the finite subcover $ \{J(p)\}_{p\in P} $. We in fact have $ 1.95\in J(1.9) $, hence $$ |1.95-1.9|<\frac{1}{2}\delta(1.95)=0.0581 ,$$so the continuity of $ f $ at $ p=1.95 $ gives
	 \begin{align}
	 |f(1.95)-f(1.9)|=0.1925<\frac{\varepsilon}{2}=\frac{1}{2}.
	 \end{align}
	 We also have $$|x-1.9|\le|1.95-x|+|1.95-1.9|<\delta+\frac{1}{2}\delta(1.95)=\frac{1}{81}+0.0581=0.0704<\delta(1.95)=.104,  $$ so by the continuity of $ f $ at $ 1.9 $, 
	 \begin{align}
	 	|f(x)-1.9|<\frac{\varepsilon}{2}=\frac{1}{2}.
	 \end{align}
	 Combining Equation (5) and (6)	gives $$|f(x)-f(1.95)|\le|f(x)-f(1.9)|+|f(1.9)-f(1.95)|<\frac{1}{2}+\frac{1}{2}=1=\varepsilon $$ for all $ x\in[-10,10] $ such that $ |x-1.95|<\delta=1/81 $. Our choice of $ \delta $ does in fact work for $ \varepsilon=1 $ and $ p=1.95 $! 
	 
	 The choice of $ p=1.95 $ does not matter. We could have picked \textit{any} value in $ [-10,10] $, and $ \delta=1/81 $ still would have worked for $ \varepsilon=1 $. This should not come as a surprise, because we just picked \textit{one of} the smallest value of $ \delta(p) $ for $ p\in[-10,10] $. Why is it that we need compactness if we could have just let $ \delta=\min_{p\in[-10,10]}\delta(p) $? This would work in this specific case because the range $ \delta([-10,10]) $ is compact, due to $ \delta(p) $ being continuous for $ \varepsilon=1 $ (Theorem 4.6), and we the infimum of a compact set in $ \R $ is an element of the set. In general this won't work, because we never explicitly said that $ \delta(p) $ is a function, let alone a continuous function.  
\end{example}
\begin{example}
	\textit{Any} real continuous function $ f:[a,b]\to\R $ is uniformly continuous on $ [a,b] $, because $ [a,b]\subset\R $ is compact. 
\end{example}  
\begin{example}
	A compact domain is a sufficient condition for uniform continuity, but it is not a neccesary condition. Any linear real function $ y=mx+b $ is uniformly continuous on all of $ \R $, and $ \R $ is not compact. 
\end{example}
\subsection{Intermediate Value Theorem}
We now will turn out attention to one of the major theorems presented in a calculus course. The Intermediate Value Theorem is perhaps the quintessential result of continuity. In calculus, you may have been taught that a continuous function is any function you could draw without picking up your pencil. If this is the case, and the range of your function starts at $ f(a) $ and ends at $ f(b) $, then your function will of course take on every value between $ f(a) $ and $ f(b) $.  
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,9) };
	\draw[thick,dashed] (6,0) -- (6,6);
	\draw[thick,dashed] (0,6) -- (6,6);
	\draw[thick] (-0.2,9) -- (0.2,9);
	\draw[thick] (-0.2,1) -- (0.2,1);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (6,-.5) node {$ x $};
	\draw[black] (14,-.5) node {$ b $};
	\draw[black] (-.6,9) node {$ f(b) $};
	\draw[black] (-.6,1) node {$ f(a) $};
	\draw[blue] (12,9) node {$ f(x) $};
		\draw[black] (-0.8,6) node {$ x=f(c) $};	
	\filldraw[blue] (1,1) circle (2pt);
	\filldraw[blue] (14,9) circle (2pt);
	\end{tikzpicture}
	\caption{We have a real function $ f:[a,b]\to\R $. The Intermediate Value Theorem says for all $f(a)< c<f(b) $, there is a $ x\in(a,b) $ such that $ f(x)=c $.}
\end{figure}
\begin{theorem}
Let $ f $ be a real function continuous on the interval $ [a,b] $. If $ f(a)<f(b) $ and if $ c $ is a number satisfying $f(a)< c<f(b) $, then there is a $ x\in(a,b) $ such that $ f(x)=c $.
\end{theorem}
\begin{proof}
	Suppose $ f:[a,b]\to\R $ is continuous, and $ f(a)<f(b) $. Let $ c\in\R $ such that $f(a)<c<f(b)$. Define the set $$S=\{x\in[a,b]\mid f(x)\le c\}. $$ The set $ S\subset[a,b] $ is nonempty\footnote{$ a\in S $} and bounded above by $ b $, so $ s=\sup S $ exists by the least-upper-bound property. We claim that $ f(s)=c $. We will show that $ f(s)\not<c $ and $ f(s)\not>c $.
	
	Suppose $ f(s)>c $, and let $ \varepsilon=f(s)-c>0 $. By the continuity of $ f $, there exists a $ \delta>0 $ such that $$ |f(x)-f(s)|<f(s)-c=\varepsilon $$ for all $ x\in[a,b] $ which satisfy $ |x-s|<\delta $. For all such $ x $ we can conclude $ f(x)>c $,\footnote{$ |f(x)-f(s)|<f(s)-c$ implies $ f(x)-f(s)<f(s)-c $ or $ f(s)-f(x)<f(s)-c $. Either way, $ f(x)>c $.} so $ x\notin S $. But if this holds for any $x\in[a,b] $ such that $ |x-s|<\delta $, then $ s-\delta $ is an upper bound of $ S $. This contradicts $ s=\sup S $.
	
	Suppose $ f(s)>c $, and let $ \varepsilon=c-f(s)>0 $. By the continuity of $ f $, there exists a $ \delta>0 $ such that $$ |f(x)-f(s)|<c-f(s)=\varepsilon $$ for all $ x\in[a,b] $ which satisfy $ |x-s|<\delta $. For all such $ x $ we can conclude $ f(x)<y $, so $ x\in S $. This implies that $ s+\delta/2\in S $ is an upper bound, which contradicts $ s=\sup S $ being an upper bound. 
\end{proof}
The Intermediate Value Theorem may seem like a parlor trick, but it is useful in many proofs. There are many proofs that rely on us being able to find a certain value in an interval. If we have information about some function, and know the Intermediate Value Theorem holds, then we may be able to use if to find the value we're interested in. Many proofs are about ``converting'' information. We may have information about $ f $, but need information about points in the domain of $ f $. In a sense, Theorem 4.9 allows us to convert information about $ f $ \textit{into} the information about the domain.  
\begin{remark}[Bounded Intervals, Figures]
	Figure 44 is one of the first times where it's been important that a function is shown on a bounded interval $ [a,b] $. For all future figures, if a function is on some bounded interval, or we are restricting our attention to a bounded interval, I will try to denote that with solid dots at the ``beginning'' and ``end'' of the function, or mark $ a $ and $ b $ on the $ x -$axis. We saw this with Figure 43 as well. If no such markings are present, then I'm trying to convey that $ f $ is defined on all of $ \R $. 
\end{remark}
\begin{example}[Continuous Functions on $ \Q $]
	Let $ f:[a,b]\to\R $ be continuous such that $ f([a,b])\subset\Q $.\footnote{The function is still real, as $ \Q\subset\R $.} The function $ f $ must be constant, that is for all $ x\in[a,b] $, $ f(x)=c $ for some $ c\in\R $. Without loss of generality, assume $ f(a)<f(b) $. By the Intermediate Value Theorem, the function takes on every value in the interval $ (f(a),f(b)) $. This interval contains irrational numbers, as the irrational numbers are dense in $ \R $. This is a contradiction, so $ f $ is constant.   
\end{example}
\begin{example}[Roots of a Polynomial]
	Suppose $$ p(x)=a_0+a_1x+\ldots+a_kx^k $$ is an odd polynomial ($ k $ is an odd number) with real coefficients $ a_i\in\R $. Taking the limit of $ p(x) $ as $ x $ goes to infinity and negative infinity gives \begin{align*}
		\lim\limits_{x\to-\infty}f(x)=-\infty,\\
		\lim\limits_{x\to\infty}f(x)=\infty.
	\end{align*}
	This is the first time we've seen limits that go off to infinity, but if we combine what we know about sequences that diverge to infinity and Theorem 4.1, we can conclude that the limit does not exist, despite us using notation that would indicate it does. These attempts to take limits also lets us know that at some point in $ \R $, $ p(x) $ switches signs. Therefore by the Intermediate Value Theorem, there exists at least one root $ x_0\in\R $ such that $ p(x_0)=0 $.\footnote{We do not know for sure how many roots of $ p(x) $ are in $ \R $, but we know it's at least one. If we wanted to look for the other roots, we would find exactly $ k $ of them in $ \mathbb{C} $! This follows from the Fundamental Theorem of Algebra which says that any complex polynomial of degree $ k $ has $ k $ roots in $ \mathbb{C} $. If these sorts of facts interest you, see \cite{dummit2004abstract}.} 
\end{example}
\begin{example}
	The converse of the intermediate value theorem is not true. That is, just because for any two point $ x_1<x_2 $ and a number $ c $ in between $ f(x_1) $ and $ f(x_2) $ we are able to find a point $ x\in(x_1,x_2) $ such that $ f(x)=c $, that \textit{does not mean} $ f $ is continuous. Define $ f:\R\to\R $ to be $$f(x)=\begin{cases}
	\sin\left(\frac{1}{x}\right)\text{ if }x\neq 0\\
	0\text{ if }x=0
	\end{cases} .$$ This function satisfies the aforementioned property, but is not continuous on $ \R $, as there is a discontinuity at $ 0 $. 
\end{example}
\subsection{Discontinuities}
We've used the word discontinuous several times, but now we'll take the time to define it, and classify two types of discontinuities. A function that is discontinuous at a point is simply not continuous at the point. The formal definition we will present negates the statement given in Definition 4.2
\begin{definition}
	Let $ X $ and $ Y $ be metric spaces,  $ E\subset X $, $ p\in E $, and $ f:E\to Y $. Then $ f $ is \textit{\color{red}discontinuous at p} if for all $ \delta>0 $, there exists a single $ \varepsilon>0 $ such that $$ d_Y(f(x),f(p))\ge\varepsilon $$ for a single $ x\in E $ which satisfies $ d_X(x,p)<\delta $. If $ f $ is discontinuous at least one point in $ E $, then $ f $ is \textit{\color{red}discontinuous (on $ E $)}. 
\end{definition}
We can classify three different types of discontinuities if we introduce the notion of a right-hand limit and a left-hand limit for a real function. 
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $, and consider any point $ p $ such that $ a\le p< b $. If for all sequence $ \{p_n\} $ in $ (p,b) $ such that $ p_n\to p $, we have $ f(p_n)\to L $, then we write $f(p+)=L $, or $$\lim\limits_{x\to p^+}f(x)=p. $$ We say that $ L $ \textit{\color{red}is the right-hand limit} of $ f(x) $ as $ x\to p $.
\end{definition}
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $, and consider any point $ p $ such that $ a< p\le b $. If for all sequence $ \{p_n\} $ in $ (a,p) $ such that $ p_n\to p $, we have $ f(p_n)\to L $, then we write $f(p-)=L $, or $$\lim\limits_{x\to p^-}f(x)=p. $$ We say that $ L $ \textit{\color{red}is the left-hand limit} of $ f(x) $ as $ x\to p $.
\end{definition}
An alternate definition of these limits would use $ \varepsilon $ and $ \delta $. For example, $ f(p-)=L $ if for all $ \varepsilon>0 $, we have $|f(p)-L|<\varepsilon $ for all $ x\in(a,b) $ satisfying $ p-\delta<x<p $. The only difference between this and the definition of a limit is we restrict our attention to the $ x $ that are within a distance of $ \delta $ to the left of $ p $. The definition for the right hand limit would be the same, except we look at the interval $ p<x<x+\delta $It  can be proven that if $ f(x+)=f(x-) $, then $ \lim_{x\to p}f(x)=p $. The first type of discontinuity is the type we saw in Figure 37. This type it special, as it does not prohibit the existence of a limit at a point.
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $. If $ f $ is discontinuous at a point $ x $, and if $ f(x+)=f(x-) $, then we say $ f $ has a \textit{\color{red}removable discontinuity} at $ x $. 
\end{definition}
With removable discontinuities, it's important to remember that $ f$ still needs to be defined at the point of discontinuity. It doesn't mean anything to say that $ f $ has a discontinuity at a point where it is not defined. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (-.5,8.8) node {$ $};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw[fill=red!15, thick, dashed, opacity=0.8] (7,6.5) rectangle (9,7.5);
	\draw[thick, blue] (0,5) -- (8,5);

	\filldraw[color=blue, fill=white] (8,5) circle (2pt);
	\draw[blue] (11,7.5) node {$ f(x)$};
	\filldraw[color=white, fill=blue] (8,7) circle (2pt);
	\draw (8,-0.3) node {$ x_0$};
	\draw (7,-0.3) node {$ x_0-\delta$};
	\draw (9,-0.3) node {$ x_0+\delta$};
	\draw[thin, dashed] (8,0) -- (8,7);
	\draw[thin, dashed] (7,0) -- (7,7);
	\draw[thin, dashed] (9,0) -- (9,7);
	\draw[thin, dashed] (0,6.5) -- (8,6.5);
	\draw[thin, dashed] (0,7.5) -- (8,7.5);
	\draw (-0.5,7) node {$ f(x_0)$};
	\draw (-0.8,6.5) node {$ f(x_0)-\varepsilon$};
	\draw (-0.8,7.5) node {$ f(x_0)+\varepsilon$};
	\draw[fill=red!15, thick, dashed, opacity=0.8] (7,6.5) rectangle (9,7.5);
	\filldraw[red] (7.2,5) circle (1.3pt) node [anchor= north west] {\tiny$ f(x) $};
	\draw[thick, blue] (8,7) -- (16,7);
	\filldraw[color=blue, fill=blue] (8,7) circle (1.5pt)node [anchor= north west] {\tiny$ f(x_0) $};
	\end{tikzpicture}
	\caption{The real function $ f:\R\to\R $ shown here is discontinuous at $ x_0 $. No matter small we take $ \delta $ to be, we can find at least one $ \varepsilon>0 $ and at least one $ x $ satisfying $ |x-x_0|<\delta $ such that $ |f(x)-f(x_0)|\ge\varepsilon $. This is also an example of a jump discontinuity.}
\end{figure}
Our second type of discontinuity is the type shown in Figure 45.
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $. If $ f $ is discontinuous at a point $ x $, and if $ f(x+)\neq f(x-) $, then we say $ f $ has a \textit{\color{red}jump discontinuity} at $ x $. 
\end{definition}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (4.5,3) (6,10)};
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (6.2,10) (8.5,3) (14,1)};
	\draw[blue] (11,2) node {$ f(x)$};
	\draw[thick, dashed] (6.1,0)--(6.1,10);
	\draw[blue] (6.1,-0.3) node {$ x_0$};
	\end{tikzpicture}
	\caption{This real function has an essential discontinuity at the point $ x_0 $. In this particular instance, neither $ f(x_0-) $, nor $ f(x_0+) $ exist.}
\end{figure}
Lastly we treat the case shown in Figure 46.
\begin{definition}
		Let $ f $ be a real function defined on $ (a,b) $. If $ f $ is discontinuous at a point $ x $, and if either $ f(x-) $, or $ f(x+) $ (or both) do not exist, then we say $ f $ has an \textit{\color{red}essential discontinuity} at $ x $. 
\end{definition}
\begin{example}[Dirichlet Function, Nowhere Continuous]
	Define $ f:\R\to\R $ as $$f(x)=\begin{cases}
	1\text{ if }x\in\Q\\0\text{ if }x\notin\Q
	\end{cases} .$$ This function takes the value $ 1 $ at every rational, and $ 0 $ at every irrational. This function happens to be discontinuous on all of $ \R $! Let $ \varepsilon $ be any number in$ (0,1] $ such that $ 1/2 $. If $ x\in\Q $, for any value of $ \delta>0 $, we can always find some irrational $ y\notin\Q $ such that $ |x-y|<\delta $, as the irrational numbers are dense in the rationals. This means we would have $$|f(x)-f(y)|=|0-1|=1>\frac{1}{2}=\varepsilon ,$$ for all $ \delta $! A similar argument holds if $ x\notin\Q $, as the rational numbers are dense in the reals. Another way to see this is by letting $ x_n $ be a sequence of rationals which converge to a real number $ x $.\footnote{We know such a sequence exists by Corollary 3.1.} We have $ f(x_n)=1 $ for all $ x_n $, so $$ f\left(\lim\limits_{n\to\infty}x_n\right)=f(x)=0\neq 1=\lim\limits_{n\to\infty} 1=\lim\limits_{n\to\infty} f(x_n),$$so $ f $ is not continuous at $ x $ by Corollary 4.3. We could show a similar result by using the density of the irrationals in the rationals to construct a sequence of irrationals which converge to an arbitrary rational. We also have that each discontinuity is an essential discontinuity. 
\end{example}

\subsection{Monotonicity}
Finally, we'll discuss a special group of functions that are either always weakly increasing or weakly decreasing. These functions will play a very important role in the study of integration, and will have an interesting property related to differentiation.
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $. The function $ f $ is \textit{\color{red}monotonically increasing} on $ (a,b) $ if $ a<x<y<b $ implies $ f(x)\le f(y) $. 
\end{definition}
\begin{definition}
	Let $ f $ be a real function defined on $ (a,b) $. The function $ f $ is \textit{\color{red}monotonically decreasing} on $ (a,b) $ if $ a<x<y<b $ implies $ f(x)\ge f(y) $. 
\end{definition}
\begin{example}
	The following real functions are monotonically increasing on the entirety of their domains: $ f(x)=e^x $, $ f(x)=\ln x $, $ f(x)=x $, $ f(x)=\sqrt{x} $. The following functions are monotonically decreasing on the entirety of their domains: $ f(x)=-x $, $ f(x)=1/x $, $ f(x)=\arccos(x) $.\footnote{You should avoid writing $ \cos^{-1}(x) $, as it is not clear if that either means $ \arcsin(x) $ or $ 1/\cos(x) $.} 
\end{example}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw [blue,  thick] (1,1)--(2,1);
	\draw [blue,  thick] (2,2)--(3,2);
	\draw [blue,  thick] (3,3)--(4,3);
	\draw [blue,  thick] (4,4)--(5,4);
	\draw [blue,  thick] (5,5)--(6,5);
	\draw [blue,  thick] (6,6)--(7,6);
	\draw [blue,  thick] (7,7)--(8,7);
	\draw [blue,  thick] (8,8)--(9,8);
	\draw [blue,  thick] (9,9)--(14,9);
	\draw [red, thick] plot [smooth, tension=1] coordinates { (1,9) (10,6) (14,1)};
	\draw[red] (13,4) node {$ f(x)$};
	\draw[blue] (13,8.5) node {$ g(x)$};
	\filldraw[color=blue, fill=white] (2,1) circle (2pt);
	\filldraw[color=white, fill=blue] (2,2) circle (2pt);
	\filldraw[color=blue, fill=white] (3,2) circle (2pt);
	\filldraw[color=white, fill=blue] (3,3) circle (2pt);
	\filldraw[color=blue, fill=white] (4,3) circle (2pt);
	\filldraw[color=white, fill=blue] (4,4) circle (2pt);
	\filldraw[color=blue, fill=white] (5,4) circle (2pt);
	\filldraw[color=white, fill=blue] (5,5) circle (2pt);
	\filldraw[color=blue, fill=white] (6,5) circle (2pt);
	\filldraw[color=white, fill=blue] (6,6) circle (2pt);
	\filldraw[color=blue, fill=white] (7,6) circle (2pt);
	\filldraw[color=white, fill=blue] (7,7) circle (2pt);
	\filldraw[color=blue, fill=white] (8,7) circle (2pt);
	\filldraw[color=white, fill=blue] (8,8) circle (2pt);
	\filldraw[color=blue, fill=white] (9,8) circle (2pt);
	\filldraw[color=white, fill=blue] (9,9) circle (2pt);
	\end{tikzpicture}
	\caption{We have two real functions, $ f:\R\to\R $, and $ g:\R\to\R $. The function $ f $ is monotonically decreasing, whereas the function $ f(x) $ is monotonically increasing. Note that $ g $ is not continuous on $ \R $. }
\end{figure}
If a function is either monotonically increasing or monotonically decreasing, we may just refer to it as \textit{\color{red}monotonic}. Neither Definition 4.11 nor Definition 4.12 make any mention of continuity. As Figure 47 shows, monotonicity does not imply continuity. In fact, neither of the presented definitions seem to have anything whatsoever to do with continuity. Monotonicity alone is not a strong enough condition for us to make meaningful statements about continuity, \textit{but} it does provide us with information about discontinuities. If a function is monotonic, we can deduce three facts about any discontinuities it may have.  
\begin{proposition}
	Let $ f $ be a monotonically increasing real function on $ (a,b) $. The limits $ f(x+) $ and $ f(x-) $ exist at every point of $ (a,b) $. Furthermore, $$\sup\limits_{a<t<x}f(t)=f(x-)\le f(x)\le f(x+)=\inf\limits_{x<t<b}f(t) .$$ We also will have $f(x+)\le f(y-) $ for all $ a<x<y<b $.
\end{proposition}
An analogous result holds for monotonically decreasing functions, and the proof is similar. 
\begin{proof}
	The range $ f((a,x))\subset\R $ has an upper bound in $ f(x) $. By the completeness of $ \R $, there exists some $ A=\sup f((a,x)) $. By the definition of the supremum, $ A\le f(x) $. We will show that $ A=f(x-) $.
	
	Fix $ \varepsilon>0 $. By the definition of $ A $ as a least-upper-bound, there exists some $ \delta>0 $ such that $ a<x-\delta<x $ and $$ A-\varepsilon<f(x-\delta)\le A.\footnote{We found a value $ x-\delta $ such that $ f(x-\delta) $ is in $ f((a,x)) $, but just barely below the set's supremum or equal to it. It's so close to that supremum, that $f(x-\delta)\in(A-\varepsilon,A]$ for our arbitrarily small $ \varepsilon$.}$$ The function $ f $ is monotonic, so for all $ t $ satisfying $ x-\delta<t<x $, we have $$f(x-\delta)\le f(t)\le A.$$ These two inequalities imply that for all $ t\in(a,b) $ satisfying $ x-\delta<t<x $, we have $$|f(t)-A|<\varepsilon ,$$ so $$ A=\sup f((a,b))=\sup\limits_{a<t<x} f(t)=f(x-) $$ as desired. To show that $ \sup\limits_{x<t<b}f(t)=f(x+) $, we let $ B=\inf f((x,b))  $, and repeat this process. This gives the inequality presented in the proposition. 
	
	If we now let $ a<x<y<b $, then we can apply the presented inequality to the point $ x\in(a,y) $ and write $$ f(x+)=\inf\limits_{x<t<y} f(t) .$$ Applying it to $ y\in(x,b) $ gives $$f(y-)=\sup\limits_{x<t<y}f(t) .$$ This infimum and supremum are taken over the same set, so the infimum is less that the supremum, which gives the second desired inequality, $$f(x+)=\inf\limits_{x<t<y} f(t)\le \sup\limits_{x<t<y}f(t)=f(y-).$$
\end{proof}
The first inequality of Proposition 4.1 is shown in Figure 48. Perhaps more useful than Proposition 4.1, is one of its immediate consequences. The $ \sup $ and $ \inf $ in question will always exist, implying the existence of both $ f(x-) $ and $ f(x+) $. In light of definition 4.10, we arrive at a corollary. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,10) node {$ \R$};
	\draw (-.5,8.8) node {$ $};
	\draw (14,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,10);
	\draw[thick,->] (0,0) -- (14,0);	
	\draw[thick, blue] (1,5) -- (8,5);
	\filldraw[color=blue, fill=white] (8,5) circle (2pt);
	\draw[blue] (11,7.5) node {$ f(x)$};
	\filldraw[color=white, fill=blue] (8,7) circle (2pt);
	\draw[thick, blue] (8,7) -- (12,7);
	\filldraw[color=blue, fill=blue] (8,7) circle (1.5pt);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (12,-0.2) -- (12,0.2);
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (12,-.5) node {$ b $};
	\draw (8,-0.3) node {$ x$};
	\draw[thin, dashed] (8,0) -- (8,7);
		\draw[thick] (-0.2,5) -- (0.2,5);
	\draw[thick] (-0.2,7) -- (0.2,7);
	\draw[black] (-1.8,4.9) node {$ \sup\limits_{a<t<x}f(t)=f(x-) $};
	\draw[black] (-1.8,6.9) node {$ \inf\limits_{x<t<b}f(t)=f(x+) $};
	\end{tikzpicture}
	\caption{The first inequality of Proposition 4.1 illustrated.}
\end{figure}
\begin{corollary}
	A monotonic function in $ \R $ has no essential discontinuous. 
\end{corollary}
This result makes sense if we think about Figure 46. For a function to have an essential discontinuity like this, we need it to increase to infinity from the left or right. This means the function must decrease once we move past the discontinuity. This would contradict monotonicity because the function would increase for certain values of its domain, and then decrease for others.  

Perhaps the most interesting result pertaining to discontinuities of monotonic functions, is that they will always form a countable set. We saw functions with an uncountable number of discontinuities (Example 4.26), but we'll see that monotonicity does not allow for this. This fact will become especially useful when we develop the theory of Riemann integration.
\begin{proposition}
	Let $ f $ be a monotonic function on $ (a,b) $.The set of points $ (a,b) $ at which $ f $ is discontinuous is at most countable. 
\end{proposition}
\begin{proof}
	We will prove this result for a monotonically increasing function. Let $ E $ be the set of points at which $ f $ is discontinuous. To show $ E $ is countable, it suffices to show that we can write a bijection from $ E $ to some countable set.\footnote{This works because sets having equal cardinality is transitive. See Proposition 1.5.}
	
	The rationals are dense in $ \R $, so we can always find a rational in number in between $ f(x-) $ and $ f(x+) $. For all $ x\in r(x) $ we can associate a rational number $ r(x) $ such that $$  f(x-)<r(x)<f(x+).$$ Since $ x_1<x_2 $ implies $ f(x_1+)\le f(x_2-)  $ by Proposition 4.1, $ r(x_1)\neq r(x_2) $ if $ x_1=x_2 $. We therefore have a bijection from $ E $ to a subset of $ \Q $ which is countable.\footnote{We used the density of $ \Q $ in $ \R $ to make the mapping surjective, and then used Proposition 4.1 to insure injectivity.}
\end{proof}
\subsection{Exercises}
min and max continuous

examples of open close compact bounded and complete not being preserved in wrong direction

proof of 4.8 with real functions, examples where inf won't work for a general comapct $ X $. 

Uniform cont and cauchy

left and right limits equal
\newpage
\section{Differentiation}
We now will treat the first of two major topics in calculus -- differentiation. For this whole section, we will be dealing with real functions of a single variable. Most of the results should be very familiar, and as such the number of examples will be limited. For the most part, differentiation as you saw it in calculus is rigorously defined. The only things that should be new are the proofs of results, and the emphasis put on certain topics. When people first take calculus, most of it is just learning how to take derivatives. That won't be so important here. The emphasis will instead be placed on the theorems, and how to use them to prove certain results.    
\subsection{The Definition of a Derivative}
We begin by defining the rate of change of a function between two points.
\begin{definition}
	Let $ f $ be a real function defined on $ X\subset \R$. For any $ x\in  X $ we define the \textit{\color{red}difference quotient} $\phi(t) $ as $$\phi(t)=\frac{f(t)-f(x)}{t-x} $$ where $ t\in  X $ and $ t\neq 0 $. 
\end{definition} 
For a fixed value $ x $, the difference quotient captures the rate of change of a function between points $ x $ and $ t $. If $ X $ is an open interval, then $ \varphi $ will not be defined at the endpoints of $ X $, as there are excluded from the set. We arrive at the definition of a derivative by letting $ t $ tend towards $ x $, thereby let $ |x-t| $ going to 0.
\begin{definition}
	Let $ f $ be a real function defined on $ X $. If $ \lim_{t\to x}\phi(t) $ exists, then we write $$ f'(x)=\lim_{t\to x}\phi(t),$$ and call $ f' $ the \textit{\color{red}derivative of $ f $}. The derivative $ f' $ is a function defined at every point where $ \lim_{t\to x}\phi(t) $ exists. If $ f' $ exists at a point $ x $ we say $ f $ is \textit{differentiable at $ x $\color{red}}. If $ f' $ exists at every point of $ E\subset X $, we say $ f $ is \textit{\color{red}differentiable on $ E $}. 
\end{definition}
We will sometimes refer to $ f'(x) $ as the ``instantaneous rate of change'' of $ f(x) $ at $ x $. 
\begin{remark}[Is the Derivative a Paradox?]
	As \href{https://www.youtube.com/watch?v=9vKqVkMQHKk&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=2}{this video} points out, calling the derivative the ``instantaneous rate of change'' can be interpreted as a paradox. The phrase ``rate of change'' implies that something actually changes over a time period. The derivative correspond to a single point. Does something change at one given point in time? The phrase ``instantaneous rate of change'' should be meaningless. In using it in reference to the derivative, we are giving it a definition that it would otherwise not have in the physical world.  
\end{remark}
\begin{remark}[An Equivalent Definition]
	The definition of $ f'(x) $ as given in Definition 5.2, may not be the way you first saw the derivative defined in calculus. Instead, you may have seen $$f'(x)=\lim\limits_{h\to 0}\frac{f(x+h)-f(x)}{h} .$$ This is \textit{perfectly valid}, and the \textit{exact same definition}. We arrive at this equation by letting $ t-x=h $, and taking $ h\to 0 $. The variable $ h $ just represents the ``distance'' (or displacement) over which we are calculating the rate of change. The choice of presenting it as an alternative is based off the fact that \cite{rudin1964principles} and \cite{tao2006analysis} give the definition of $ f'(x) $ in terms of the difference quotient. While all the results will hold regardless of what definition we use, sometimes one definition more be more suitable for a proof or example. In all these cases, we could simply take Definition 5.1, let $ t-x=h $, and take $ h\to 0 $. This step won't be explicitly shown after this, so make sure this sits well. As we'll see later, using the definition where $ h\to 0 $ will help us build intuition when using the derivative to approximate functions. Much later on when we consider differentiation with multiple variables, we will also opt to use a definition with $ h\to 0 $.
\end{remark}
\begin{note}
	There are many different ways to notate the derivative, the two most common being Lagrange's notation of $ f' $, and Leibniz's notation of $ \frac{df}{dx} $. We will exclusively use Lagrange's notation for real functions of a single variable. This decision is motivated by the fact that $ f' $ does not make reference to the variable used to denote the input of $ f $. We are free to write $ f(x) $, $ f(t) $, $ f(\theta) $, etc. If we use $ \frac{df}{dx} $, then we must always write $ f(x) $. This is a matter of purely notational flexibility. Another reason to shy away from $ \frac{df}{dx} $, is that it encourages one to interpret $ df $ and $ dx $ as infinitesimally small numbers. Historically, this is how Leibniz interpreted the derivative, but it was never formal in this context. We will make it formal when treating differential forms. 
\end{note}
\begin{example}[The Power Rule]
	Let $ f:\R\to\R $ be defined as $ f(x)=x^n $. We will show that $ f'(x)=nx^{n-1} $ for all $ x\in\R $. First, we should point out that $$(t-x)^n=(t-x)\left[x^{n-1}+tx^{n-2}+\cdots +t^{n-2}x+t^{n-1}\right]=(t-x)\displaystyle\sum_{k=0}^{n-1}t^kx^{(n-1)-t} .$$ We will need to use this to rewrite the denominator of the difference quotient.
	\begin{align*}
		f'(x)&=\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}\\
			&=\lim\limits_{t\to x}\frac{t^n-x^n}{t-x}\\
			&=\lim\limits_{t\to x}\frac{(t-x)\left[x^{n-1}+tx^{n-2}+\cdots +t^{n-2}x+t^{n-1}\right]}{t-x}\\&=\lim\limits_{t\to x}\left[x^{n-1}+tx^{n-2}+\cdots +t^{n-2}x+t^{n-1}\right]\\&=x^{n-1}+x\cdot x^{n-2}+\cdots +x^{n-2} \cdot x\\&=nx^{n-1}
	\end{align*}
\end{example}
\begin{example}
	Let $ f:\R\to\R $ be defined as $ e^x $. What is $ f'(x)? $
	\begin{align*}
		f'(x)=\lim\limits_{h\to 0}\frac{f(x+h)-f(x)}{h}=\lim\limits_{h\to 0}\frac{e^{x+h}-e^x}{h}=\lim\limits_{h\to 0}\frac{e^{x}e^h-e^x}{h}=e^x\cdot \lim\limits_{h\to 0}\frac{e^{h}-1}{h}.
	\end{align*}
	At this point we will let $ u=e^h-1 $ and perform a substitution, noting that $ h=\ln(u+1) $.\footnote{I find substitutions like this very unsettling. It feels like another ``trick'' used in proofs that you would only know if you've proved the result before. I also could be very alone in this regard, and just don't think ``maybe I should try substitution'' as much as I should.} We also have $ u\to 0 $ as $ h\to 0 $.\footnote{The limit of $ u=e^h-1$ as $ h\to0 $ is $ e^0-1=0$ because $ e^x $ is continuous. This gives $ u\to 0 $ as $ h\to 0 $.}
	$$ 	f'(x)=e^x\cdot\lim\limits_{u\to 0}\frac{e^{h}-1}{h}=e^x\cdot\lim\limits_{u\to 0}\frac{u}{\ln(u+1)}=e^x\cdot\lim\limits_{u\to 0}\frac{1}{\frac{1}{u}\cdot\ln(u+1)}=e^x\cdot\lim\limits_{u\to 0}\frac{1}{\left[\ln(u+1)^\frac{1}{u}\right]}=e^x\cdot\frac{1}{\ln\left[\lim\limits_{u\to 0}(u+1)^\frac{1}{u}\right]}$$ Why are we able to pass the limit into the natural log?\footnote{Because the natural log is continuous, and you can bring limits into continuous functions, as continuity preserves limits.} Finally note that $$ \lim_{u\to 0}(u+1)^{1/u}=\lim_{n\to\infty}(1+1/n)^{1/n}=e ,$$ which we first took as a fact in Example 3.7. $$ 	f'(x)=e^x\cdot\frac{1}{\ln\left[\lim\limits_{u\to 0}(u+1)^\frac{1}{u}\right]}=e^x\cdot\frac{1}{\ln(e)}=e^x\cdot \frac{1}{1}=e^x$$ We have that the derivative of $ e^x$ is $e^x $. 
\end{example}
\begin{theorem}[Differentiability implies Continuity]
	Let $ f $ be defined on $  X $. If $ f $ is differentiable at $ x\in  X $, then $ f $ is continuous at $ x $.  
\end{theorem}
\begin{proof}
	We can use Theorem 4.3, and show $ \lim_{t\to x}f(t)=f(x) $ if $ f $ is differentiable at $ x $. We have
	\begin{align*}
	f(t)-f(x)&=\frac{f(t)-f(x)}{t-x}(t-x)\\
	\lim\limits_{t\to x}[f(t)-f(x)]&=\lim\limits_{t\to x}\phi(t)(t-x)\\\lim\limits_{t\to x}f(t)-f(x)&=f'(x)\cdot 0\\&=0.
	\end{align*}
	If $ \lim\limits_{t\to x}f(t)-f(x)=0 $, then $ \lim\limits_{t\to x}f(t)=f(x) $. 
\end{proof}
Where would this proof go wrong if we did not know $ f $ is differentiable at $ x $?\footnote{ If $ \lim\limits_{t\to x}\phi(t) $ did not exist, then there would have been no way to calculate $ (\lim\limits_{t\to x}\phi(t))\cdot 0 $. } It is quite easy to come up with examples where the converse of Theorem 5.1 fails. 
\begin{example}
	Let $ f:\R\to\R $ be defined as $ f(x)=|x| $. This function is continuous at $ 0 $ (and all of $ \R $), but is not differentiable at $ 0 $. We have $$\phi(t)=\frac{f(t)-f(0)}{t-0}=\frac{|t|}{t} .$$ If we take the left-hand limit and right-hand limit of the difference quotient, we will find they do not agree.
	\begin{align*}
		\lim\limits_{t\to 0^+}\phi(t)=\lim\limits_{t\to 0^+}\frac{|t|}{t}=\lim\limits_{t\to 0^+}\frac{t}{t}=\lim\limits_{t\to 0^+}1=1\\\lim\limits_{t\to 0^-}\phi(t)=\lim\limits_{t\to 0^-}\frac{|t|}{t}=\lim\limits_{t\to 0^-}\frac{-t}{t}=\lim\limits_{t\to 0^-}1=-1
	\end{align*}
	These two limits agreeing is both a necessary and sufficient condition for the limit existing. We therefore have that $ \lim_{t\to 0}\phi(t) $ does not exist, so $ f' $ is undefined at $ 0 $. Note that $ f $ is still differentiable on $ \R\backslash\{0\} $. 
\end{example} 
\subsection{Familiar Properties of the Derivative}
We will now prove the rules of differentiation that are presented in a calculus course. After proving the most basic properties, we will introduce the Chain Rule, and a basic version of the Inverse Function Theorem, two results that are not immediately obvious.  
\begin{theorem}
Suppose $ f $ and $ g $ are defined on $  X $, and are differentiable at $ x\in  X $.
\begin{enumerate}
	\item If $ f(x)=c $ for some constant $ c\in\R $, then $ f'(x)=0 $.
	\item $ (cf)'(x)=cf'(x) $.
	\item (Sum Rule) $ (f+g)'(x)=f'(x)+g'(x)$.
	\item (Product Rule) $(fg)'(x)=f'(x)g(x)+g'(x)f(x) $.
	\item (Quotient Rule) $\left(\frac{f}{g}\right)'(x)=\frac{f'(x)g(x)-g'(x)f(x)}{g(x)^2} $.
\end{enumerate}
\end{theorem}
\begin{proof} The functions $ f $ and $ g $ are differentiable at $ x $. By Theorem 5.1, these functions are continuous at $ x $, so we will have $ \lim_{t\to x}g(t)=g(x) $ and $ \lim_{t\to x}f(t)=f(x) $ by Theorem 4.3. We will use these equalities in the proof of the Product Rule and the Quotient Rule.
	\begin{enumerate}
	\item $$\lim\limits_{t\to x}\phi(t)=\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}=\lim\limits_{t\to x}\frac{c-c}{t-x}=\lim\limits_{t\to x}\frac{0}{t-x}=\lim\limits_{t\to x}0=0 $$
	\item $$\lim\limits_{t\to x}\phi(t)=\lim\limits_{t\to x}\frac{cf(t)-cf(x)}{t-x}=\lim\limits_{t\to x}c\cdot\frac{f(t)-f(x)}{t-x}=c\cdot\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}=cf'(x) $$
	\item \begin{align*}
		\lim\limits_{t\to x}\phi(t)&=\lim\limits_{t\to x}\frac{(f+g)(t)-(f+g)(x)}{t-x}\\&=\lim\limits_{t\to x}\frac{[f(t)+g(t)]-[f(x)+g(x)]}{t-x}\\
						&=\lim\limits_{t\to x}\frac{f(t)-f(x)+g(t)-g(x)}{t-x}\\
						&=\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}+\lim\limits_{t\to x}\frac{g(t)-g(x)}{t-x}\\
						&=f'(x)+g'(x)
	\end{align*}
	\item \begin{align*}
	\lim\limits_{t\to x}\phi(t)&=\lim\limits_{t\to x}\frac{(fg)(t)-(fg)(x)}{t-x}\\&=\lim\limits_{t\to x}\frac{f(t)g(t)-f(x)g(x)+0}{t-x}\\
	&=\lim\limits_{t\to x}\frac{f(t)g(t)-f(x)g(x)+[f(t)g(x)-f(t)g(x)]}{t-x}\\
		&=\lim\limits_{t\to x}\frac{g(x)[f(t)-f(x)]+f(t)[g(t)-g(x)]}{t-x}\\
	&=g(x)\cdot\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}+\lim\limits_{t\to x}f(t)\cdot\frac{g(t)-g(x)}{t-x}\\&=g(x)f'(x)+\lim\limits_{t\to x}f(t)\cdot \lim\limits_{t\to x}\frac{g(t)-g(x)}{t-x}\\&=f'(x)g(x)+g'(x)f(x)
	\end{align*}
	\item 
	\begin{align*}
		\lim\limits_{t\to x}\phi(t)&=\lim\limits_{t\to x}\frac{(f/g)(t)-(f/g)(x)}{t-x}\\
		&=\lim\limits_{t\to x}\frac{\frac{f(t)}{g(t)}-\frac{f(x)}{g(x)}+0}{t-x}\\
		&=\lim\limits_{t\to x}\frac{\frac{f(t)}{g(t)}-\frac{f(x)}{g(x)}+\left[\frac{f(x)}{g(t)}-\frac{f(x)}{g(t)}\right]}{t-x}\\&=\lim\limits_{t\to x}\frac{\frac{f(t)}{g(t)}-\frac{f(x)}{g(t)}+\frac{f(x)}{g(t)}-\frac{f(x)}{g(x)}}{t-x}\\
		&=\lim\limits_{t\to x}\frac{\frac{1}{g(x)}\left[\frac{f(t)g(x)}{g(t)}-\frac{f(x)g(x)}{g(t)}\right]-\left[\frac{f(x)g(t)}{g(x)g(t)}-\frac{f(x)g(x)}{g(t)g(x)}\right]}{t-x}\\	&=\lim\limits_{t\to x}\frac{\frac{g(x)}{g(x)g(t)}\left[f(t)-f(x)\right]-\frac{f(x)}{g(x)g(t)}\left[g(t)-g(x)\right]}{t-x}\\
		&=\lim\limits_{t\to x}\frac{1}{g(x)g(t)}\left[g(x)\cdot\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}-f(x)\cdot\lim\limits_{t\to x}\frac{g(t)-g(x)}{t-x}\right]\\&=\frac{1}{g(x)g(x)}[g(x)f'(x)-f(x)g'(x)]\\&=\frac{f'(x)g(x)-g'(x)f(x)}{g(x)^2}
	\end{align*}
	
\end{enumerate}
\end{proof}

\begin{example}[Linearity of Derivatives] If we combine the Sum Rule and Part 2 of Theorem 5.2, we have that the operation of differentiation is linear. If $ f $ and $ g $ are defined on $ [a,b] $ and are differentiable at $ x\in[a,b] $, then for constants $ c,d\in\R $ we have
$$ (cf+dg)'(x)=cf'(x)+dg'(x).$$
\end{example}
\begin{example}[Difference Rule]
Suppose $ f $ and $ g $ are defined on $ [a,b] $, and are differentiable at $ x\in[a,b] $. We can derive\footnote{Pun intended.} the Difference Rule by using the linearity of differentiation.
$$(f-g)'(x)=(f+(-1)g)'(x)=f'(x)+(-1)g'(x)=f'(x)-g'(x) $$
\end{example}
\begin{example}[Polynomials]
	If $ \mathscr{P}(\R) $ is the set of all polynomials with real coefficients, then every element of $ \mathscr{P}(\R) $ is differentiable on all of $ \R $. We can show this using the Power Rule, Sum Rule, and Part 2 of Theorem 5.2. We can write $ p(x)\in\mathscr{P}(\R) $ as $$p(x)=a_0+a_1x+\cdots+a_{n-1}x^{n-1}+a_nx^n=\sum_{k=0}^{n}a_kx^k $$ for $ a_k\in\R $ for all $ k $. We have $$p'(x)=a_1+2a_2x+\cdots+(n-1)a_{n-1}x^{n-2}+na_nx^{n-1}=\sum_{k=1}^{n}ka_kx^{k-1}. $$ This is define for all $ x\in\R $. The ease at which we can differentiate polynomials is one of the reasons we like working with them.
\end{example}
One of the benefits of composition preserving the continuity of two functions (Theorem 4.4) is that it means most function we work with are continuous. If you were tasked with writing down some random function, it could most likely would be the result of a composition of simpler functions. The ubiquity of functions composed via other functions makes it useful to know how to differentiate such functions. This is where the Chain Rule comes in. Before formally stating and proving the Chain Rule, it's worth building a \textit{\textbf{non-technical}}\footnote{I'll explain why my explanation is technically wrong afterwards.} intuition as to why it works. 

Suppose you have a real function $ g(y) $, and you want to find the rate of change with. We know that this rate is given as $ g'(y) $. Now let there be a second function $ f(x) $. You're now challenged with finding the rate of change of $ g $ with respect to $ x $, after taking $ y=f(x) $ to be an input of $ g $. That is, we need to find $ h'(x) $ for $ h(x)=g(f(x)) $. We have two pieces of information:  $ g'(y) $ (how $ g $ changes with respect to its input), and $ f'(x) $ (how $ f $ changes with respect to its input). To measure the change of $ g $ at $ f(x) $ with respect to $ x $, we need to somehow convert a measure of change in $ y=f(x) $ to a change in $ x $. This can be achieved by using the change in $ y $ with respect to $ x $ as an exchange rate. If we think of this as a problem related to converting between two measures of change, then it seems the logical thing to do would be multiply $ g $'s change with respect to $ y=f(x) $ by the conversion rate of $ y=f(x) $'s change with respect to $ x $. 
$$g'(y)\cdot f'(x)=\frac{\text{change in }g}{{\text{change with respect to }y=f(x)}}\times \frac{\text{change in }y=f(x)}{\text{change with respect to }x}=\frac{\text{change in }g}{\text{change with respect to }x} $$ Writing everything in terms of $ x $ gives us $ g'(f(x))f'(x)$.
\begin{remark}[I Broke the Rules]
	Okay so this explanation of the Chain Rule is kind of wrong. In Notation 5.1, I said that I would be avoiding the notation of $ \frac{dy}{dx} $, as it encourages people to think of $ dy $ and $ dx $ as their own numbers which form a fraction. This is not what derivatives are, but I just treated them as fractions when explaining the Chain Rule. In fact, had I used Leibniz's notation, the Chain Rule becomes very aesthetically pleasing and easy to remember: $$ \frac{dg}{dx}=\frac{dg}{dy}\frac{dy}{dx} .$$ Contrary to what it looks like, we aren't multiplying fractions when we write this. Treating $ dx $ and $ dy $ like their own numbers can lead to problems and at this point lacks the rigor that analysis aims to achieve. Remark 10.1.17 in \cite{tao2006analysis} makes similar points about this issue. 
\end{remark}
\begin{theorem}[The Chain Rule]
Let $ X $ and $ Y $ be subsets of $ \R $. If $ f:X\to\R $ is continuous on $ X $, $ f'(x) $ exists at some point $ x\in X $, and $ g:Y\to \R $ is differentiable at $ f(x) $. If we define $h(t)=g(f(t)) $, then $ h $ is differentiable at $ x $, and $$ h'(x)=g'(f(x))f'(x).$$
\end{theorem}
\begin{proof}
	Let $ y=f(x) $. The functions $ f $ and $ g $ are differentiable at $ x $ and $ y $ respectively. 
	\begin{align*}
		f'(x)=\lim_{t\to x}\frac{f(t)-f(x)}{t-x}\\g'(y)=\lim_{s\to y}\frac{g(s)-g(y)}{s-y}
	\end{align*}
	Define $ u(t) $ and $ v(s) $ as 
	\begin{align*}
		u(t)=\begin{cases}
		\frac{f(t)-f(x)}{t-x}-f'(x)\text{ if }t\neq x\\
		0\text{ if }t=x
		\end{cases},\\
			v(s)=\begin{cases}
		\frac{g(s)-g(y)}{s-y}-g'(s)\text{ if }s\neq y\\
		0\text{ if }s=y
		\end{cases},
	\end{align*}
for $ t\in X $ and $ s\in Y $. We have $ \lim_{t\to x}u(t)=0 $ and $ \lim_{s\to y}v(s)=0 $. These functions allow to rewrite the derivatives of $ g $ and $ f $ as \begin{align}
	f(t)-f(x)=(t-x)[f'(x)+u(t)],\\g(s)-g(y)=(s-y)[g'(y)+v(s)],
\end{align}
\textit{as we take the limits} $ t\to x $ and $ s\to y $.\footnote{Equation (78) is simply $ f(t)-f(x)=(t-x)\left[f'(x)+\frac{f(t)-f(x)}{t-x}-f'(x)\right] $ when $ t\neq x $. When we let $ t\to x $ we have $ u(t)\to 0 $, and we arive at $ f(t)-f(x)=(t-x)f'(x) $ as $ t\to x  $. In other words, $ f'(x)=\lim_{t\to x}\frac{f(t)-f(x)}{t-x} $, which is true. Equation (7) is therefore perfectly valid. The same argument works for Equation (7).}
If we let $ f(t)=s $ in (8), we can use (7) and $ (8) $ to write
\begin{align*}
	h(t)-h(x)&=g(f(t))-g(f(x))\\&=[f(t)-f(x)][g'(y)+v(s)]\\&=(t-x)[f'(x)+u(t)][g'(y)+v(f(t))]\\\frac{h(t)-h(x)}{(t-x)}&=[f'(x)+u(t)][g'(y)+v(f(t))]
\end{align*}
for $ t\neq x $. If we let $ t\to x $, then 
\begin{align*}
h'(x)&=\lim_{t\to x}\frac{h(t)-h(x)}{(t-x)}\\&=\lim_{t\to x}[f'(x)+u(t)][g'(y)+v(f(t))]	\\ &=\left[f'(x)+\lim_{t\to x}u(t)\right]\left[g'(y)+\lim_{t\to x}v(f(t))
\right]\\ &=\left[f'(x)\right]\left[g'(y)+v(y)\right]\\&=f'(x)g'(y)\\&=g'(f(x))f'(x).\end{align*}
Note that to conclude $ \lim_{t\to x}v(f(t))=v(y) $, it must be the case that $ \lim_{t\to x}f(t)=f(x)=y $. This follows by the assumed continuity of $ f $ on $ X $. 
\end{proof}
Our last rule concerning the calculation of derivatives pertains to the inverse of a function. If we let $ X $ and $ Y $ be subsets of $ \R $, we may want to know the derivative of $ f^{-1}:Y\to X $ at a point $ y=f(x) $, but only know the derivative of $ f^:X\to Y $ at $ x $. If we know that $ f^{-1} $ is differentiable, then this is a straightforward application of the chain rule and composition of $ f^{-1} $ and $ f $. 
\begin{align*}
	(f^{-1}\circ f)'(x)&=(f^{-1})'(f(x))f'(x)\\
	1&=(f^{-1})'(y)f'(x)\\
	(f^{-1})'(y)&=\frac{1}{f'(x)}
\end{align*} 
There is one shortcoming in this approach. In order to use the Chain Rule, we needed to assume that $ f^{-1} $ was differentiable. If we do not know beforehand that $ f^{-1} $ is invertible, then perhaps we cannot conclude $ (f^{-1})'(y)=\frac{1}{f'(x)}  $. As it turns out, we still can by The Inverse Function Theorem, which says that knowing $ f^{-1} $ is continuous suffices to conclude $ (f^{-1})'(y)=\frac{1}{f'(x)}  $. This result is rarely given this name in the context of a single variable, and in fact it may not show up in an analysis course to begin with. It's not very special for functions in $ \R $. A far more general version of the Inverse Function Theorem will hold with functions in several variables. The introduction here is just to give some early exposure to the idea of the theorem (which I'm calling a proposition here as an allusion to it not being the full-fledged Inverse Function Theorem).  
\begin{proposition}[The Inverse Function Theorem]
 Let $ X $ and $ Y $ be subsets of $ \R $, and $ f:X\to Y $ be an invertible function with inverse $ f^{-1}:Y\to X $. Suppose $ x\in X $ and $ y\in Y $ such that $ f(x)=y $. If $ f $ is differentiable at $ x_0 $, $ f^{-1} $ is continuous at $ y_0 $, and $ f'(x_0)\neq 0 $, then $ f^{-1} $ is differentiable and $$ (f^{-1})'(y)=\frac{1}{f'(x)}.$$
\end{proposition}
\begin{proof}
	We need to show that $$\lim\limits_{s\to y}\frac{f^{-1}(s)-f^{-1}(y)}{s-y}=\frac{1}{f'(x)} .$$ By the assumed continuity of $ f^{-1} $, we can show  $$\lim\limits_{n\to \infty}\frac{f^{-1}(s_n)-f^{-1}(y)}{s_n-y}=\frac{1}{f'(x)},$$ for a sequence $ \{s_n\} $ in $ Y $ which converges to $ s $. Let $ t_n=f^{-1}(s_n) $ be a sequence in $ X $.By continuity, we have $$\lim_{n\to\infty} t_n=\lim_{n\to\infty} f^{-1}(s_n)=  f^{-1}\left(\lim_{n\to\infty}s_n \right)=f^{-1}(s)=t.$$ Because $ f $ is differentiable (and therefore continuous) at $ x $, we have $$\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}=\lim_{n\to\infty}\frac{f(t_n)-f(x)}{t_n-x}=f'(x) .$$ This difference quotient is not zero since $ t_n\neq x $, and $ f(t_n)\neq f(x) $ ($ f $ is a bijection after all). We can use the limit laws of Theorem 3.2 to invert the difference quotient and arrive at $$ \lim_{n\to\infty}\frac{t_n-x}{f(t_n)-f(x)}=\frac{1}{f'(x)}.$$ But since $  t_n=f^{-1}(s_n) $ and $ x=f^{-1}(y) $, we have $ \lim\limits_{n\to \infty}\frac{f^{-1}(s_n)-f^{-1}(y)}{s_n-y}=\lim\limits_{s\to y}\frac{f^{-1}(s)-f^{-1}(y)}{s-y}=\frac{1}{f'(x)} .$
\end{proof}
\subsection{Local Extrema}
The derivative gives us a useful tool for finding \textit{local} minima and maxima of a function. 
\begin{definition}
	Let $ X $ be a subset of $ \R $. The function $ f:X\to Y $ has a \textit{\color{red}local maximum} at a point $ p\in X $ if there exists $ \delta>0 $ such that $ f(x)\le f(p) $ for all $ x\in X $ with $ d(p,x)<\delta $.
\end{definition}
\begin{definition}
	Let $ X $ be a subset of $ \R $. The function $ f:X\to Y $ has a \textit{\color{red}local minimum} at a point $ p\in X $ if there exists $ \delta>0 $ such that $ f(x)\ge f(p) $ for all $ x\in X $ with $ d(p,x)<\delta $.
\end{definition}
We use the word local to emphasize the fact that local extrema need not be \textit{the} maximum or \textit{the} minimum on $ X $. We see this in Figure 49, $ f $ has a local maximum at $ p_0 $, despite not having a \textit{global} maximum at $ p_0 $. All we care about is that there is some small neighborhood of $ X $ over $ f $ attains a maximum at $ p_0 $. This neighborhood is the set of all $ x\in X $ such that $ d(p_0,x)<\delta $ for $ \delta>0 $, which can also be written as $ N_\delta(p_0)\subset X $. 
\begin{figure}[h!]
\centering
\begin{tikzpicture}
\draw (-.3,9) node {$ \R$};
\draw (15,-0.3) node {$ \R$};		
\draw[thick,->] (0,0) -- (0,9);
\draw[thick,->] (0,0) -- (15,0);
\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,8) (4.25, 4) (7.5,6) (10.75, 4) (14,8) };
\draw[thin, dashed] (0,6) -- (12.88,6);
\draw[thin, dashed] (7.5,0) -- (7.5,6);
\draw[thin, dashed] (2.12,0) -- (2.12,6);
\draw[thin, dashed] (12.88,0) -- (12.88,6);
\draw[thin, dashed] (4.7,0) -- (4.7,4);
\draw[thin, dashed] (10.3,0) -- (10.3,4);
\draw[black] (-.7,6) node {$ f(p_0) $};
\draw[black] (7.5,-.5) node {$ p_0 $};
\draw[black] (4.7,-.5) node {$ p_1 $};
\draw[black] (10.3,-.5) node {$ p_2 $};
\draw[black] (12.88,-.5) node {$ p_0+\delta $};
\draw[black] (2.12,-.5) node {$ p_0-\delta $};
\draw[blue] (12,5.5) node {$ f(x) $};
\end{tikzpicture}
\caption{The function has a local maximum at $ p_0 $, and local minima at $ p_1 $ and $ p_2 $. The points $ p_0-\delta $ and $ p_0+\delta $ correspond to the endpoints of the interval $ (p_0-\delta,p_0+\delta) $. We have $ f(x)\le f(p_0) $ for all $ x\in(p_0-\delta,p_0+\delta) $, as prescribed in Definition 5.3.}
\end{figure}
We can relate the local extrema of a function to its derivative. Note that the proposition will present is a necessary condition for local extrema, but not a sufficient condition. Our main use for this will be proving other theorems. In many applied settings which rely on optimization, this result is assigned more significance. 
\begin{proposition}[Fermat's Theorem for Extrema]
Let $ X=[a,b] $ be a subset set of $ \R $. If $ f:X\to\R $ has a local extrema at a point $ p\in(a,b) $, and if $ f'(p) $ exists, then $ f'(p)=0 $.  
\end{proposition} 
\begin{proof}
	We will prove the case for local maxima. Let $ p\in (a,b) $ be a local maximum. There exists some $ \delta $ such that $ f(p)\ge f(x) $ for all $ x $ satisfying $$ x<p-\delta<x<p+\delta<b .$$
	If $ p-\delta<t<p $, then $$ \frac{f(t)-f(p)}{t-p}\ge 0$$ as $ f(p)\ge f(t)  $, and $ p>t $. This means $$ \lim_{t\to p}\frac{f(t)-f(p)}{t-p}=f'(p)\ge 0 .$$ 	If instead $ p<t<p+\delta $, then $$ \frac{f(t)-f(p)}{t-p}\le 0$$ as $ f(p)\ge f(t)  $, and $ p<t $. This means $$ \lim_{t\to p}\frac{f(t)-f(p)}{t-p}=f'(p)\le 0 .$$ If $ f'(p)\le 0 $ and $ f'(p)\ge 0 $, then $ f'(p)=0 $.
\end{proof}
As the next three examples show, there are several drawbacks to Fermat's Theorem.
\begin{example}[Saddle Point]
Let $ f:\R\to\R $ be the function $ f(x)=x^3 $. We have $ f'(0)=0 $, but the point $ 0 $ is neither a local minimum nor a local maximum. This follows from the fact that Lemma 5.1 is a necessary condition, but not a sufficient condition. 
\end{example}
\begin{example}[Endpoints]
Let $ f:[0,1]\to\R $ be the function $ f(x)=x $. This function has a local maximum at $ 1 $ and local minimum at $ 0 $. Lemma 5.1 does not hold in this case, because it assumes that $ p\in (a,b) $.
\end{example} 
\begin{example}[Not Differentiable]
Let $ f:\R\to\R $ be the function $ f(x)=|x| $. This function has a local minimum at $ 0 $, but $ f'(0)$ is undefined. Lemma 5.1 does not hold in this case, as it assumes that $ f'(p) $ exists.
\end{example} 
 
\subsection{Mean Value Theorems}
As far as analysis is concerned, the Mean Value Theorem is the most important result involving differentiation. When discussing the Intermediate Value Theorem, it was noted that it gave us a way to ``convert'' information about $ f $ to information about the domain of $ f $. A similar metaphor holds for the Mean Value Theorem, as it allows us to gather some information about $ f' $ is we know information about $ f $ (or vice-versa). If we know that $ f $ is differentiable on some interval $ [a,b] $, then the Mean Value Theorem tells use the value of $ f'(c) $ for $ c\in[a,b] $, namely it is $$f'(c)=\frac{f(b)-f(a)}{b-a} $$ 

We will present three versions of the Mean Value Theorem, each more general than the last. 
\begin{theorem}[Rolle's Theorem]
Let $ a<b $, and let $ f:[a,b]\to R $ be a continuous function which is differentiable on $ (a,b) $. Suppose also that $ g(a)=g(b) $. Then there exists an $ x\in(a,b) $ such that $ g'(x)=0 $.
\end{theorem}
\begin{proof}
	We have that $ f $ is continuous on $ [a,b] $. By the Extreme Value Theorem (Theorem 4.7), $ f  $ attains a maximum $ M $ at some point $ x\in[a,b] $, and a minimum $ m $ at some point $ y\in[a,b] $.  
	\begin{enumerate}
		\item [Case 1.]	If $ x,y\in\{a,b\} $,\footnote{That is, they are the endpoints of the interval $ [a,b] $.} then $ f $ is constant on $ [a,b] $ because $ f(a)=f(b) $, so $ f'(z)=0 $ for all $ z\in(a,b)$.   
		\item [Case 2.] Suppose $ x $ is not an endpoint of $ [a,b] $. We have a local maximum of $ f $ at $ x $, so by Fermat's Theorem (Proposition 5.2), $ f'(x)=0 $.
		\item [Case 3.] Suppose $ y $ is not an endpoint of $ [a,b] $. We have a local minimum of $ f $ at $ y $, so by Fermat's Theorem (Proposition 5.2), $ f'(y)=0 $.
	\end{enumerate}
\end{proof}
\begin{corollary}[Mean Value Theorem] Let $ a<b $, and let $ f:[a,b]\to R $ be a continuous function which is differentiable on $ (a,b) $. Then there exists an $ c\in(a,b) $ such that $$ f'(c)=\frac{f(b)-f(a)}{b-a}.$$
\end{corollary}
\begin{proof}
	Define a new function as $$h(x)=f(x)-\frac{f(b)-f(a)}{b-a}\cdot x .$$ The function $ h $ is continuous on $ [a,b] $, differentiable on $ (a,b) $,\footnote{The continuity and differentiability of $ h $ follow immediately from $ f $.} and was defined so $ h(a)=h(b) $. By Rolle's Theorem, there exists a $ c\in(a,b) $ such that $ h'(c)=0 $. 
	\begin{align*}
		h'(c)=f'(c)-\frac{f(b)-f(a)}{b-a}=0\implies f'(c)=\frac{f(b)-f(a)}{b-a}
	\end{align*}
\end{proof}
A geometric interpretation of the Mean Value Theorem is found in Figure 50.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,3) };
	\draw[red, dashed, thick] (1,1) -- (14,3); 
	\draw[red, dashed, thick] (1,5.27) -- (14,7.27);
	\draw[thick] (-0.2,6.05) -- (0.2,6.05);
	\draw[thick] (-0.2,1) -- (0.2,1);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[thick] (6,-0.2) -- (6,0.2);
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (6,-.5) node {$ c $};
	\draw[black] (14,-.5) node {$ b $};
	\draw[black] (-.6,6.05) node {$ f(b) $};
	\draw[black] (-.6,1) node {$ f(a) $};
	\draw[blue] (12,5) node {$ f(x) $};	
	\draw[red] (9,7) node {rate of change $ =f'(c) $};
	\draw[red] (9,3) node {rate of change $ =\frac{f(b)-f(a)}{b-a} $};	
	\end{tikzpicture}
	\caption{The Mean Value Theorem.}
\end{figure}
The Mean Value Theorem can be extended to the case of parametric curves. 
\begin{proposition}[Cauchy's Mean Value Theorem]
Let $ a<b $, and let $ f:[a,b]\to R $ and $ g:[a,b]\to \R $ be continuous functions which are differentiable on $ (a,b) $. Then there exists an $ c\in(a,b) $ such that $$ \frac{f'(c)}{g'(c)}=\frac{f(b)-f(a)}{g(b)-g(a)}.$$
\end{proposition}
\begin{proof}
	Like we did while proving the Mean Value Theorem, define a function $$h(x)=f(x)-\frac{f(b)-f(a)}{g(b)-g(a)}\cdot g(x) .$$ If we apply Rolle's Theorem, the result follows. 
\end{proof}
Figure 51 illustrates Cauchy's Mean Value Theorem.

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw[ thick, blue] (7,5) circle (4cm); 
	\draw[red, dashed, thick] (10,2.35) -- (7,9); 
	\draw[red, dashed, thick] (4.23,1.35) -- (1.23,8);
	\filldraw[blue] (10,2.35) circle (2pt) node[anchor= north west] {$ (f(a),g(a)) $};
	\filldraw[blue] (7,9) circle (2pt) node[anchor= south west] {$ (f(b),g(b)) $};
	\filldraw[blue] (3.3,3.5) circle (2pt) node[anchor= south west] {$ (f(c),g(c)) $};
	\draw[red] (7.5,4) node {\scriptsize rate of change $ =\frac{f(b)-f(a)}{g(b)-g(a)} $};
	\draw[red] (2.8,8) node {\scriptsize rate of change $ =\frac{f'(c)}{g'(c)} $};
	\end{tikzpicture}
	\caption{Cauchy's Mean Value Theorem. Note that the parameterization begins at $ (f(a),g(a)) $ and then is orientated clockwise.}
\end{figure}
\begin{example}
	Let $ f(t)=\cos (t) $ and $ g(t)=\sin t $. If we let $ a=0 $ and $ b=\pi/2 $. Then we can find a value of $ c\in(0,\pi/2) $ such that $$ \frac{f'(c)}{g'(c)}=\frac{-\sin c}{\cos c}=\frac{0-1}{1-0}=-1.$$ You can verify that $ c=\pi /4 $ works. 
\end{example}
\subsection{L'H\^{o}pital's Rule}
Now we will prove a classic result from calculus that allows us to calculate a limit using derivatives.
\begin{proposition}[L'H\^{o}pital's Rule]
	Suppose $ f $ and $ g $ are real and differentiable in $ (a,b) $, and $ g'(x)\neq 0 $ for all $ x\in(a,b) $, where $ -\infty\le a< b\le\infty $. Suppose we have $$\lim_{x\to a}\frac{f'(x)}{g'(x)}=A .$$ If $ \lim_{x\to a}f(x)=0 $ and $ \lim_{x\to a}g(x)=0$, or if $ \lim_{x\to a}g(x)=\infty $, then $$\lim_{x\to a}\frac{f(x)}{g(x)}=A . $$ 
\end{proposition} 
\begin{proof}
	{\color{red}FINISH}
\end{proof}
\subsection{Higher Order Derivatives}
The derivative $ f' $ is a function in its own right. This means we could not only differentiate $ f' $ itself, but differentiate the resulting function. This idea gives rise to derivatives of higher order.
\begin{definition}
	If $ f $ has a derivative $ f' $ on an interval, and if $ f' $ is itself differentiable, we write $  (f')'=f''$, and call $ f'' $ the \textit{\color{red}second derivative of $ f $}. Continuing in this manner gives $$ f,f',f'',f^{(3)},\ldots,f^{(n)},$$ where $ f^{(n)} $ is the \textit{\color{red}$ n $th derivative of $ f $}.  
\end{definition}
\begin{remark}[Smoothness]
	For $ f^{(n)}(x) $ to exist at a point $ x $, we need to be able to take the limit $$\lim_{t\to x}\frac{f^{(n-1)}(t)-f^{(n-1)}(x)}{t-x}. $$ This means that not only is a requirement that $ f^{(n-1)}(t) $ is differentiable at $ x $, but also we need $ f^{(n-1)}(t) $ to be defined in some neighborhood of $ x $. What does it mean for $ f^{(n-1)}(t) $ to exist in s neighborhood of $ x $? It would mean that $ f^{(n-2)} $ is differentiable in that neighborhood. This reasoning follows for all $ f^{(n-2)},\ldots, f $. For this reason, the highest order derivative you are able to take of a function $ f $ is often associated with how ``smooth'' it is.
	
	A continuous function is ``nice'', but it doesn't need to be smooth.\footnote{Think of $ |x| $ at the points $ x=0 $} A function that is differentiable is smooth. A function that is twice differentiable is even smoother, as it means that $ f' $ is differentiable, and therefore continuous! To be formal we say that $ f $ is a \textit{\color{red}smooth function} if it has continuous derivatives up to some desired order. Having a continuous derivative of order $ n-1 $, is implied by being $ n $ times differentiable.  In many applied setting this becomes important. For example, you may see theorems assume that a certain function has a continuous second derivative.\footnote{For example, in statistics and econometrics there is a method of estimation known as maximum likelihood estimation. This method is only ``efficient'' if we assume that a probability distribution is three times differentiable, i.e it's second derivative is continuous.} The smoothest functions are those which are infinitely differentiable. 
\end{remark}
\begin{example}
	Every real polynomial $ p(x) $ is infinitely differentiable. Let 
	$$p(x)=a_0+a_1x+\cdots+a_{n-1}x^{n-1}+a_nx^n=\sum_{k=0}^{n}a_kx^k $$ for $ a_k\in\R $ be a polynomial of degree $ n $.
	We have \begin{align*}
		p'(x)&=a_1+2a_2x+\cdots+(n-1)a_{n-1}x^{n-2}+na_nx^{n-1}\\
		p''(x)&=2a_2+\cdots+(n-1)(n-2)a_{n-1}x^{n-3}+n(n-1)a_nx^{n-2}\\
		&\vdots\\
		p^{(n-1)}(x)&=n!a_nx\\
		p^{(n)}(x)&=n!a_n\\
		p^{(n+1)}(x)&=0\\
		&\vdots
	\end{align*} 
	The polynomial is still infinitely differentiable even if we keep getting zero, as the zero function is differentiable.
\end{example}
\begin{example}
	The functions $ f(x)=\sin x $ and  $g(x)=\cos x $ are infinitely differentiable, and each of their derivatives are given by 
	\begin{align*}
	f^{(n)}(x)=\begin{cases}
	\cos x \text{ if }n=1,5,9,\ldots\\
	-\sin x \text{ if }n=2,6,10,\ldots\\
	-\cos x \text{ if }n=3,7,11,\ldots\\
	\sin x \text{ if }n=4,8,12,\ldots\\
	\end{cases} \ \ \ \ \ 	g^{(n)}(x)&=\begin{cases}
	-\sin x \text{ if }n=1,5,9,\ldots\\
	-\cos x \text{ if }n=2,6,10,\ldots\\
	-\sin x \text{ if }n=3,7,11,\ldots\\
	\cos x \text{ if }n=4,8,12,\ldots\\
	\end{cases}
	\end{align*}
\end{example}
\begin{remark}[Polynomials and Trig Functions]
	Polynomials, $ \cos $, and $ \sin $ are not only infinitely differentiable, but they're also reall easy to differentiate. This makes working with them preferable. If we have a complicated function, it would be nice if we could find some way to express it as a polynomial or in terms of $ \sin $ and $ \cos $. This is more of a longterm goal we will return to later. 
\end{remark}
\subsection{Approximation}
We now turn to a subject involving differentiation that will return several times -- approximation. First we will discuss the derivative in the context of linear approximation of a function, then we will explore how we can get increasingly accurate approximations of a function by using higher order derivatives.

In Remark 5.2, an alternate definition of the derivative was presented. $$f'(x_0)=\lim\limits_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}$$ A reformulation of this allows us to write the derivative as the difference quotient (with $ h $) if we introduce some ``remainder term''. 
\begin{align}
f'(x_0)+hr(h)=\frac{f(x_0+h)-f(x_0)}{h}
\end{align}
As long the ratio $ r(h)/h\to 0 $ as $ h\to 0 $, then this is a valid equation, as \begin{align*}
	f'(x_0)&=\frac{f(x_0+h)-f(x_0)}{h}+\frac{r(h)}{h}\\
	f'(x_0)&= \lim\limits_{h\to 0}\left[\frac{f(x_0+h)-f(x_0)}{h} + \frac{r(h)}{h}\right]\\
	f'(x_0)&= \lim\limits_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}+0.
\end{align*}
A valid question is, why do we the ratio $ r(h)/h $ goes to zero and not just that $ r(h)\to 0 $? We need the remainder $ r(h) $ to ``shrink'' faster than $ h $, otherwise $ r(h)/h\not\to $ as $ h\to 0 $. In fact, $ r(h)/h $ could very well ``blow up'' if we don't stipulate $ r(h)/h\to 0 $ as $ h\to 0 $!\footnote{This would happen if $ h\to 0 $ ``faster'' than $ r(h)\to 0 $.} If this where to happen, then we would not end up with the definition of the derivative. If we multiply by $ h $ and rearrange some terms, (9) becomes 
\begin{align}
	f(x_0+h)=f(x_0)+f'(x_0)h+hr(h).
\end{align}
Equation (10) may seem random, but it's \textit{very} interesting. We know that $ hr(h) $ is going to be small, so we could be informal and write $$ f(x_0+h)\approx f(x_0)+f'(x_0)h. $$ For a given $ x_0 $, this tells us that $ f(x_0)+f'(x_0)h $ is an approximation for $ f(x_0+h) $ ($ f $ near some point $ x_0 $). Furthermore, this approximation is linear, as $ f(x_0) $ and $ f'(x_0) $ are constants, $ f(x_0)+f'(x_0)h $ is a linear function of $ h $. Not only is this a linear approximation, it is the \textit{best} linear approximation we are able to achieve, a fact we can prove.
\begin{note}[``Little $ o $'']
We say that $ f(h)=o(g(h)) $ (read as \textit{\color{red}``$ f(h) $ is little $ o $ of $ g(h) $ as $ h\to 0 $''}) if $$ \lim\limits_{h\to 0} \frac{f(h)}{g(h)}=0.$$ This means that $ f(h) $ tends to zero faster than $ g(h) $ as $ h\to 0 $. For example, $$\lim\limits_{h\to 0}\frac{h^3}{h}=0 ,$$ so we could write $ o(h^3)=o(h) $.   
\end{note}
When we noted that $ r(h)/h\to 0 $, what we really were saying was $ r(h)=o(h) $.\footnote{One of the weird things about little $ o $ notation is we use ``$ = $'' as a stand in for ``is''. There is no actual equality when we write $ o(h) $ instead of $ r(h) $. In general this means when you se $ o(h) $ you should think ``something here is $ o(h) $, the particular form of which is not that important because it is going to zero.''} In the case of linear approximation, having a remainder which is $ o(h) $ is as good as it gets, because $ h $ enters the equation linearly. For example, we cannot end up with some $ h^nr(h) $ that vanishes in (10), giving $ r(h)=o(h^n) $, because (10) would no longer be a linear function of $ h $. We now can state and prove the result we have been building to.  
\begin{theorem}[Best Linear Approximation]
	If $ f $ is differentiable at $ x_0 $, then $$f(x_0+h)=f(x_0)+f'(x_0)h+o(h) .$$ Conversely, if there exists some linear approximation $$ f(x_0+h)=B+Ah+o(h) ,$$ then not only is $ f $ differentiable at $ x_0 $, but also $ B=f(x_0) $ and $ A=f'(x_0) $. 
\end{theorem}
\begin{proof}
	While we did not use the phrase ``if and only if'' in Theorem 5.5, this theorem is a biconditional statement. A function will have a best linear approximation if and only if it is differentiable.
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ f $ is differentiable at $ x_0 $. While building up to this theorem, we have already shown that $ f(x_0+h)=f(x_0)+f'(x_0)h+o(h) $, as $ r(h)=o(h) $ in (10).
		\item [$ (\Longleftarrow) $] Suppose $ f $ has a (best) linear approximation of $ f(x_0+h)=B+Ah+o(h)$, for scalers $ A,B\in\R $. Setting $ h=0 $ gives $f(x_0)=B $. We therefore have  $$f'(x)=\lim_{h\to0}\frac{f(x_0+h)-f(x_0)}{h}=\lim_{h\to0}\frac{f(x_0+h)-B}{h}=\lim_{h\to0}\frac{Ah+o(h)}{h}=\lim_{h\to0}\left[A+\frac{o(h)}{h}\right]=A .$$ 
	\end{enumerate}

\end{proof}
The first part of Theorem 5.5 tells us that (10) is a good approximation, while the second part tells us that if such a good approximation exists, $ f $ is differentiable, and the approximation is given by (10). There is no other way to say it -- the second part of this theorem is \textit{****ing} amazing. The simple concept of approximating any function with a line, gives rise to the concept of the derivative.\footnote{In fact, this is how the derivative is introduced to many people in calculus courses.} Is this a surprise though? 

If you want to approximate a nonlinear function $ f $ with a line at a point $ x_0 $, you have two degrees of freedom: a point on the line, and the slope of the line.\footnote{Any point and slope uniquely define a line, hence point-slope form from middle school: $ y-y_1=m(x-x_1) $.} The choice of point is easy. You want to approximate $ f $ at $ x $, so you should pick $ (x_0,f(x_0)) $. Now you have an infinite choice of slopes, which one do you pick? Pick the slope to be the rate of change at that point! This is just $ f'(x_0) $ though. You literally\footnote{And I mean the definition of the word ``literally'', not ``figuratively''.} cannot make a better choice of point and slope. 
\begin{remark}[Another Alternate Definition]
	Theorem 5.5 inspires yet another equivalent definition of differentiation. We can say that $ f $ is differentiable at a point $ x $ if there exists some scaler $ A\in\R $ such that $$ \lim\limits_{h\to 0}\frac{f(x+h)-f(x)-A\cdot h}{h}=0.$$ In this case we write $ f'(x)=A $. You sometimes hear the phrase ``locally linear'' associated with the existence of the derivative, and it comes from this definition. All we did was subtract $ f(x) $ from both sides of the definition given in Remark 5.2. We now are expressing $ f'(x) $ as some linear map where $ h\mapsto f'(x)h $. This coincides with (10).
\end{remark}
\begin{example}
	We can show that $ f(x)=|x| $ is not differentiable at $ x_0=0 $ using Theorem 5.5 and Remark 5.4. Suppose we had some linear approximation of $ |x| $ at the point $ 0 $. 
	\begin{align*}
		f(x_0+h)&=B+Ah+r(h)\\
		|0+h|&=B+Ah+r(h)\\
		|h|&=B+Ah+\underbrace{(|h|-B-Ah)}_{r(h)}
	\end{align*}
	But do we have $ r(h)=o(h) $? $$\lim_{h\to0}\frac{r(h)}{h}=\lim_{h\to0}\frac{(|h|-B-Ah)}{h}=\lim_{h\to0}\frac{(|h|-B-A)}{h}\neq 0 $$ This limit is undefined, as $ |h|/h $ has no limit as $ h\to 0 $. This is the \textit{exact} same limit we attempted to take in Example 5.3, but could not, render $ |x| $ not differentiable at $ 0 $. 
\end{example}
\begin{example}
	The equation of the line given in Theorem 5.5 looks a little strange. In calculus, the equation for a tangent line looks a bit like this, but something is off. For a fixed $ x_0 $, it's a function of $h$, where $ x_0+h $ is a point close to $ x_0 $.  Instead, we can write write this nearby point as $ x=x_0+h $. Equation $ (10) $ now becomes 
	\begin{align*}
		f(x)&=f(x_0)+f'(x_0)(x-x_0)+r(h)\\f(x)&\approx f(x_0)+f'(x_0)(x-x_0).
	\end{align*}
	This final equation is what most people see in calculus. Figure 52 shows the approximation for $ x_0+h $ and $ x $.
\end{example}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (6,6) (14,3) };
	\draw[red, thick] (1,5.23) -- (14,7.23);
	\draw[thick] (6,-0.2) -- (6,0.2);
	\draw[thick] (-0.2,6.01) -- (0.2,6.01);
	\draw[black] (-.6,6.01) node {$ f(x_0) $};
	\draw[black] (6,-.5) node {$ x_0 $};
	\draw[thick] (9,-0.2) -- (9,0.2);
	\draw[black] (9,-.5) node {$ x=x_0+h $};
	\draw[blue] (12,5) node {$ f $};	
	\filldraw[black] (6,6.01) circle (2pt);
	\draw[red] (10,7.6) node{\scriptsize $ f(x_0)+f'(x_0)(x-x_0)=f(x_0)+f'(x_0)h $};
	\filldraw (9,5.7) circle (1pt) node[anchor = north east] {\scriptsize$ f(x)=f(x_0+h) $};	
	\filldraw (9,6.46) circle (1pt);
	\draw[thick, dashed] (9,5.7) -- (9,6.46);
	\draw (9.6,6.05) node{\scriptsize $ r(h) $};
	\end{tikzpicture}
	\caption{The equation of a tangent line to $ f $ at point $ x_0 $ is given as $ f(x_0)+f'(x_0)(x-x_0) $ or $f(x)+f'(x)h $. The difference between the value of the function and the value of the approximation at a nearby point $ x=x_0+h $ is given by $ r(h) $. If we add $ r(h) $ to the approximation, then we have an equation for $ f $ as given in Equation (10).}
\end{figure}

Our next example not only calculates the best linear approximation for a function at a point, but also serves as motivation for our next theorem. 
\begin{example}
Suppose we want to approximate $ e^x $ at the point $ x_0=0 $ using Theorem 5.5 (which we can do because $ e^x $ is differentiable). Note in this case $ x=h+x_0=h+0=h $
	\begin{align*}
	f(x_0+h)&=f(x_0)+f'(x_0)h+r(h)\\
	e^{0+h}&=e^0+e^0h+r(h)\\
	e^h&=1+h+(e^h-1-h)\\
	e^h&\approx 1+h\\
	e^x&\approx 1+x.
	\end{align*}
We won't be able to get a better \textit{linear} approximation than this, but can we get a better approximation? 

The linear approximation for $ f $ at $ x_0 $ is a result of picking the best point and slope to define the approximation. It makes sense that we should pick the point $ (x_0,f(x_0)) $ and the slope $ f'(x_0) $. This is all the information about $ f $ we can incorporate into our approximation as far as derivatives go, because the equation of a line \textit{must} have a second derivative of zero.\footnote{All subsequent derivatives will also be $ 0 $.} What if we allowed our approximation to have a nonzero second derivative? For $ e^x $, such an approximation would be quadratic. It would look like $$ e^x\approx 1+x+C x^2,$$ for a scaler $ C $. We can now incorporate more information about $ e^x $ into our approximation, namely we can pick a value of $ C $ such that the second derivative of our approximation matches the second derivative of $ e^x $ at our point of interest $ x_0=0 $. For $ f(x)=e^x $, we have $ f''(x)=e^x $, giving $ f''(x_0)=f''(0)=1 $. Perhaps we should try $ C=1 $. 
\begin{align*}
	e^x&\approx 1+x+C x^2\\
	e^x&\approx 1+x+x^2
\end{align*}
But if we differentiate our approximation, then we have $ (1+x+x^2)''(0)=2\neq 1 $. Letting $ C=f''(x_0) $ failed to account for the power rule, so we need to scale it by $ 1/2 $ to account for this. Our updated quadratic approximation is
$$ e^x\approx1+x+\frac{1}{2}x^2.$$
This approximation has the same value, derivative, and second derivative as $ e^x $ at $ x_0=0 $. But in what sense is this better than our linear approximation? We can address this with $ r(h) $.
$$ e^x\approx1+x+\frac{1}{2}x^2+\underbrace{(e^x-1-x-\frac{1}{2}x^2)}_{r(h)}$$
Because $ h=x $ in this case, we have $r(h)=e^h-1-h-\frac{1}{2}h^2 $. Not only do we have $ r(h)=o(h) $, but we have $ r(h)=o(h^2) $!
$$ \lim_{h\to 0}\frac{r(h)}{h^2}=\lim_{h\to 0}\frac{e^h-1-h-\frac{1}{2}h^2}{h^2}=0$$ The remainder is moving to $ 0 $ faster than $ h^2 $. In a sense, it's shrinking \textit{twice} as fast as $ h $.  This makes our quadratic approximation superior to our linear approximation, as our quadratic remainder converges to $ 0 $ even faster than that of the linear approximation. But why stop at quadratic, why not cubic, or quartic, \textit{or} quintic? If we repeated this process we would arrive at the following equations:
\begin{align*}
	e^x&=1+x+o(x)\\
	e^x&=1+x+\frac{1}{2}x^2+o(x^2)\\
	e^x&=1+\frac{1}{2}x^2+\frac{1}{6}x^3+o(h^3)\\
	e^x&=1+\frac{1}{2}x^2+\frac{1}{6}x^3+\frac{1}{24}x^4+o(h^4)\\
	e^x&=1+\frac{1}{2}x^2+\frac{1}{6}x^3+\frac{1}{24}x^4+\frac{1}{120}x^5+o(x^5)\\
\end{align*}
Each time we increase the degree of our polynomial approximation, we can incorporate the information given by a higher order derivative of $ f $. 
\end{example}
The idea of approximating a differentiable function with a polynomial of some fixed degree is formalized in Taylor's Theorem. For this theorem, we will opt to use $ (x-x_0) $ instead of $ h $.
\begin{theorem}[Taylor's Theorem]
Suppose $ f $ is a real function on $ [a,b] $, $ n $ is a positive integer, and $ f $ is $ n $ times differentiable at a point $ x_0 $. Then we have $$ f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)+\ldots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+o((x-x_0)^n).$$ We refer to the polynomial (excluding the remainder term) as the \textit{\color{red}$ n $-th order Taylor Polynomial of $ f $ at $ x_0 $}, and write $$ P_n(x)=\sum_{k=0}^{n}\frac{f^{(k)}(0)}{k!}(x-x_0) .$$
\end{theorem}
The proof just amounts to applying L'H\^{o}pital's Rule $ n-1 $ tines
\begin{proof}
We just need to show that the remainder term is in fact $ o((x-x_0)^n) $. If $$ r((x-x_0))=f(x)-P_n(x) $$ is our remainder term, then we need to verify that $$ \lim\limits_{(x-x_0)\to 0}\frac{f(x)-P_n(x)}{(x-x_0)^n}=0.$$ By construction, $ f^{(k)}(x_0)=P_n^{(k)}(x_0) $ for $ k=0,1,\ldots n $. This means that our desired limit will give the indeterminate form of $ 0/0 $, so we must use L'H\^{o}pital's Rule.\footnote{You should verify that we meet the conditions required of Proposition 5.4.} But if we use it once, we still get an indeterminate form of $ 0/0 $. We in fact must apply it $ n-1 $ times before we do not end up with $ 0/0 $:
\begin{align*}
\lim\limits_{(x-x_0)\to 0}\frac{f(x)-P_n(x)}{(x-x_0)^n}&=\lim\limits_{(x-x_0)\to 0}\frac{f'(x)-P_n'(x)}{n(x-x_0)^{n-1}}\\
&=\cdots\\
&=\lim\limits_{(x-x_0)\to 0}\frac{f^{(n-1)}(x)-P_n^{(n-1)}(x)}{n!(x-x_0)}\\
&=\frac{1}{n!}\lim\limits_{x\to x_0}{f^{(n)}(x)-P_n^{(n)}(x)}\\
&=\frac{1}{n!}f^{(n)}(x_0)-P_n^{(n)}(x_0)\\
&=0
\end{align*}
Therefore we have  $ o((x-x_0)^n) $. 
\end{proof}
\begin{remark}[Taylor's Theorem and the MVT]
	The presentation of Taylor's Theorem \cite{rudin1964principles} is slightly different. I suspect his (equally valid) treatment was motivated by the connection between the Mean Value Theorem and Taylor's Theorem. \cite{rudin1964principles} explicitly writes ``For $ n=1 $, the [Taylor's Theorem] is just the mean value theorem.'' If we use Taylor's Theorem to find $ P_n(x_0) $ for an $ n+1 $ differentiable function, we have $$ f(x)=\underbrace{f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)+\ldots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n}_{P_n(x)}+\underbrace{\frac{f^{(n+1)}(x_0)}{(n+1)!}(x-x_0)^{n+1} }_{R_n(x)},$$ where $ R_n(x) $ is a remainder term. Rudin's version says that under stronger conditions, we can always find some value between $ \xi\in(x,x_0) $ which satisfies $$ f(x)=P_n(x)+R_n(\xi).$$ If we do this for $ P_0(x) $ and $ R_0(x) $, we get $$ f(x)=f(x_0)+f'(\xi)(x-x_0)\implies f'(\xi)=\frac{f(x)-f(x_0)}{x-x_0},$$ which is the Mean Value Theorem.  
\end{remark}

A natural question that arises with Taylor's Theorem is what happens when we let the $ n\to\infty $. If $ f $ is infinitely differentiable at $ x_0 $, then what is stopping us from writing the Taylor Polynomial as an infinite series? If we do this, will the series have any remainder? If it has no remainder does that mean it simply equals the function at every point it is defined? We will answer all these questions in Section 7! 
\subsection{Exercises}
uniform continuity and second derivative

derivative of $ x^n $ using $ h $ definition

derivative of $ a^x $

taylor approx is best
\newpage
\section{Riemann Integration}
The theory of differentiation may at times have seemed painless. It's eerily similar to calculus. Isn't analysis supposed to be hard and confusing? Fear not, because here comes integration to bring us back to reality! The definition of Riemann integration cannot be given immediately like that of the derivative, as we need to do some prep work. Even once it is defined, proofs involving the Riemann integral are a bit more sophisticated than the average proof up until this point. This is a constant theme in math -- integration is \textit{way} harder than differentiation. Our goal is to develop a form of integration that works reasonably well for real functions. Fortunately, this simplifies things, as we're comfortable with real functions. There is good news. The Riemann integral mostly achieves our goal!\footnote{There is a reason that it is the only form of integration most people ever need to know and use.} Nevertheless, there will be some drawbacks of Riemann integration, and we will only be able to go so far with it. For this reason, we will return to integration again in Sections 12-18. Consider this a first pass at what turns out to be a much more sophisticated problem. 

Why are we even interested in integration though? Many problems are concerned with accumulation over time, which is captured by the area underneath a curve. For example, the probability of a certain event can be interpreted as the area under a probability distribution function. Similar problems exist in nearly every field of science, so it's important that we have some tool for measuring area under the curve of some function. 

First, we should ask, when is it even possible to calculate the area under the curve of a function? Is it possible to measure the area over some infinite interval, such as all of $ \R $? It is not immeadetly clear how to do this, as we have not yet developed a way of measuring the length of $ \R $,\footnote{We'll do this in Section 12.} so we should stick to some bounded interval $ [a,b]\subset\R $. We also want $ f $ to be bounded on $ \R $, otherwise the area under the curve would not be well defined. For these reasons, \textbf{we will restrict our attention to bounded real functions on an interval $ \textbf{[a,b] }$} for this whole section. 
\subsection{Partitions}
First we need to develop some notation about partitioning an interval of the real line into smaller intervals. 
\begin{definition}
	Let $ [a,b]\subset\R $. A \textit{\color{red}partition} $ P $ of $ [a,b] $ is a finite set of points $ P=\{x_0,x_1,\ldots,x_n\} $ such that $$a=x_0\le x_1\le\ldots\le x_{n-1}\le x_n=b .$$ We will write $ \delta x_i=x+i-x_{i-1} $ for $ i=1,\ldots,n $. We will denote the set of all partitions of $ [a,b] $ as $ \textbf{P}([a,b]) $.\footnote{$ \textbf{P}([a,b]) $ is \textit{not} standard notation.} 
\end{definition}
Figure 53 shows a partition of the interval $ [a,b] $. Contrary to what this figure shows, the points which comprise a partition need not be evenly spaced out along $ [a,b] $. We can refine a partition by adding more points to $ P=\{x_0,x_1,\ldots,x_n\} $.

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,<->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (2,-0.2) -- (2,0.2);
	\draw[thick] (3,-0.2) -- (3,0.2);
	\draw[thick] (4,-0.2) -- (4,0.2);
	\draw[thick] (5,-0.2) -- (5,0.2);
	\draw[thick] (6,-0.2) -- (6,0.2);
	\draw[thick] (7,-0.2) -- (7,0.2);
	\draw[thick] (8,-0.2) -- (8,0.2);
	\draw[thick] (10,-0.2) -- (10,0.2);
	\draw[thick] (11,-0.2) -- (11,0.2);
	\draw[thick] (12,-0.2) -- (12,0.2);
	\draw[thick] (13,-0.2) -- (13,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[thick] (9,-0.2) -- (9,0.2);	
	\draw[black] (1,-.5) node {$ a=x_0 $};
	
	\draw[black] (7,-.5) node {$ x_{6} $};
	\draw[black] (8,-.5) node {$ x_{7} $};
	\draw[black] (7.5,.3) node {$ \Delta x_7 $};
	\draw[black] (12.5,.5) node {$ P $};
	\draw[black] (14,-.5) node {$ b=x_{13} $};	
	\end{tikzpicture}
	\caption{A Partition $ P=\{x_0,x_1,\ldots,x_{13}\} $ of the interval $ [a,b] $.}
\end{figure}
\begin{definition}
	A partition $ P^* $ is \textit{\color{red}refinement} of $ P $ if $ P\subset P^* $. Given two partitions, $ P_1 $ and $ P_2 $, we say $ P^* $ is their \textit{\color{red}common refinement} if $ P^*=P_1\cup P_2 $. 
\end{definition}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,<->] (0,0) -- (16,0);
	\draw[thick](1,-0.2) -- (1,0.2);
	\draw[thick](2,-0.2) -- (2,0.2);
	\draw[thick](3,-0.2) -- (3,0.2);
	\draw[thick](4,-0.2) -- (4,0.2);
	\draw[thick](5,-0.2) -- (5,0.2);
	\draw[thick](6,-0.2) -- (6,0.2);
	\draw[thick](7,-0.2) -- (7,0.2);
	\draw[thick](8,-0.2) -- (8,0.2);
	\draw[thick](10,-0.2) -- (10,0.2);
	\draw[thick](11,-0.2) -- (11,0.2);
	\draw[thick](12,-0.2) -- (12,0.2);
	\draw[thick](13,-0.2) -- (13,0.2);
	\draw[thick](14,-0.2) -- (14,0.2);
	\draw[thick](9,-0.2) -- (9,0.2);	
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\draw (16,-1.8) node {$ \R$};		
	\draw[thick,<->] (0,-1.5) -- (16,-1.5);
	\draw[thick](1,-1.7) -- (1,-1.3);
	\draw[thick](1.5,-1.7) -- (1.5,-1.3);
	\draw[thick](3,-1.7) -- (3,-1.3);
	\draw[thick](4.4,-1.7) -- (4.4,-1.3);
	\draw[thick](2.4,-1.7) -- (2.4,-1.3);
	\draw[thick](6.1,-1.7) -- (6.1,-1.3);
	\draw[thick](6.8,-1.7) -- (6.8,-1.3);
	\draw[thick](7.5,-1.7) -- (7.5,-1.3);
	\draw[thick](10.4,-1.7) -- (10.4,-1.3);
	\draw[thick](9.3,-1.7) -- (9.3,-1.3);
	\draw[thick](11.2,-1.7) -- (11.2,-1.3);
	\draw[thick](11.6,-1.7) -- (11.6,-1.3);
	\draw[thick](12.7,-1.7) -- (12.7,-1.3);
	\draw[thick] (14,-1.7) -- (14,-1.3);	
	\draw[black] (1,-2) node {$ a $};
	\draw[black] (14,-2) node {$ b $};	
	
	
		\draw (16,-3.3) node {$ \R$};
	
		\draw[thick](2,-2.8) -- (2,-3.2);
		\draw[thick](3,-2.8) -- (3,-3.2);
		\draw[thick](4,-2.8) -- (4,-3.2);
		\draw[thick](5,-2.8) -- (5,-3.2);
		\draw[thick](6,-2.8) -- (6,-3.2);
		\draw[thick](7,-2.8) -- (7,-3.2);
		\draw[thick](8,-2.8) -- (8,-3.2);
		\draw[thick](10,-2.8) -- (10,-3.2);
		\draw[thick](11,-2.8) -- (11,-3.2);
		\draw[thick](12,-2.8) -- (12,-3.2);
		\draw[thick](13,-2.8) -- (13,-3.2);
		\draw[thick](9,-2.8) -- (9,-3.2);		
	\draw[thick,<->] (0,-3) -- (16,-3);
	\draw[thick](1,-2.8) -- (1,-3.2);
	\draw[thick](1.5,-2.8) -- (1.5,-3.2);
	\draw[thick](3,-2.8) -- (3,-3.2);
	\draw[thick](4.4,-2.8) -- (4.4,-3.2);
	\draw[thick](2.4,-2.8) -- (2.4,-3.2);
	\draw[thick](6.1,-2.8) -- (6.1,-3.2);
	\draw[thick](6.8,-2.8) -- (6.8,-3.2);
	\draw[thick](7.5,-2.8) -- (7.5,-3.2);
	\draw[thick](10.4,-2.8) -- (10.4,-3.2);
	\draw[thick](9.3,-2.8) -- (9.3,-3.2);
	\draw[thick](11.2,-2.8) -- (11.2,-3.2);
	\draw[thick](11.6,-2.8) -- (11.6,-3.2);
	\draw[thick](12.7,-2.8) -- (12.7,-3.2);
	\draw[thick] (14,-2.8) -- (14,-3.2);	
	\draw[black] (1,-3.5) node {$ a $};
	\draw[black] (14,-3.5) node {$ b $};
	\draw[black] (7.5,-.5) node {$ P_1 $};
	\draw[black] (7.5,-2) node {$ P_2 $};
	\draw[black] (7.5,-3.5) node {$ P^*=P_1\cup P_2 $};
	\end{tikzpicture}
	\caption{The partition $ P^* $ is a common refinement of $ P_1 $ and $ P_2 $.  }
\end{figure}
\subsection{Upper and Lower Riemann Integrals}
We will now use partitions to approximate the area under a curve. This will be nearly identical to the method developed in calculus, but with one small difference. Calculus courses normally use left and right Riemann sums, which use the points of a partition to determine the height of the rectangles use to approximate the area under a function. We will instead determine the height of the rectangles using the infimum and supremum of a function.
\begin{definition}
	Suppose $ f $ is a bounded real function defined on $ [a,b] $. Corresponding to each partition $ P $ of $ [a,b] $, we have \begin{align*}
		M_i&=\sup_{x\in[x_{i-1},x_i]} f(x)\\
		m_i&=\inf_{x\in[x_{i-1},x_i]} f(x)\\
		U(P,f)&=\sum_{i=1}^{n}M_i\Delta x_i,\\
		L(P,f)&=\sum_{i=1}^{n}m_i\Delta x_i,
	\end{align*} 
	 We call $ U(P,f) $ an \textit{\color{red}upper Riemann sum} and $ L(P,f) $ a \textit{\color{red}lower Riemann sum}.
\end{definition}
Figure 55 shows what $ m_i\Delta x_i $ and $ M_i\Delta x_i $ may look like for a given function and partition. 
 \begin{figure}[h!]
 	\centering
 	\begin{tikzpicture}
 	\definecolor{green(ncs)}{rgb}{0.0, 0.62, 0.42}
 	\draw (-.3,7) node {$ \R$};
 	\draw (16,-0.3) node {$ \R$};		
 	\draw[thick,->] (0,0) -- (0,7);
 	\draw[thick,->] (0,0) -- (16,0);
 	\draw[thick] (1,-0.2) -- (1,0.2);
 	\draw[thick] (14,-0.2) -- (14,0.2);
 	\draw[thick] (13,-0.2) -- (13,0.2);
 	\draw[thick] (5,-0.2) -- (5,0.2);
 	\draw[blue] (14,5.5) node {$ f(x) $};	
 	\draw[black] (1,-.5) node {$ a $};
 	\draw[black] (14,-.5) node {$ b $};	
 	\draw[black] (5,-.5) node {$ x_{i-1} $};
 	\draw[black] (13,-.5) node {$ x_i $};	
 	\draw[dashed,thin] (0,1) -- (10.5,1);
 	\draw (-0.3,1) node {$ m_i$};
 	\draw (-0.3,6) node {$ M_i$};
 	\draw[color=red, fill=red!15, thick, opacity=1] (5,0) rectangle (13,6);
 	\draw[color=green, fill=green!15, thick, opacity=0.6] (5,0) rectangle (13,1);
 	\draw[dashed,thin] (0,6) -- (7,6);
 	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (7,6) (10.5,1) (14,5) };
 	\draw[red] (10,5.5) node {$ M_i\Delta x_i $};
 	\draw[green] (10,0.5) node {$ m_i\Delta x_i $};
 	\end{tikzpicture}
 	\caption{ The particular values  $ m_i\Delta x_i $ and $ M_i\Delta x_i $  for some function $ f $ and partition of $ [a,b] $.}
 \end{figure}
Figure 56 and Figure 57 show the upper Riemann sum and lower Riemann sum  for the partition of $ [a,b] $ that was shown in Figure 53, respectively. \begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[blue] (14,5.5) node {$ f(x) $};	
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\draw[fill=gray!15, thin, opacity=0.6] (1,0) rectangle (2,1);
	\draw[fill=gray!15, thin, opacity=0.6] (2,0) rectangle (3,2.35);
	\draw[fill=gray!15, thin, opacity=0.6] (3,0) rectangle (4,3.5);
	\draw[fill=gray!15, thin, opacity=0.6] (4,0) rectangle (5,4.5);
	\draw[fill=gray!15, thin, opacity=0.6] (5,0) rectangle (6,5.25);
	\draw[fill=gray!15, thin, opacity=0.6] (6,0) rectangle (7,5.77);
	\draw[fill=gray!15, thin, opacity=0.6] (7,0) rectangle (8,5.69);
	\draw[fill=gray!15, thin, opacity=0.6] (8,0) rectangle (9,3.55);
	\draw[fill=gray!15, thin, opacity=0.6] (9,0) rectangle (10,1.2);
	\draw[fill=gray!15, thin, opacity=0.6] (10,0) rectangle (11,0.98);
	\draw[fill=gray!15, thin, opacity=0.6] (11,0) rectangle (12,1);
	\draw[fill=gray!15, thin, opacity=0.6] (12,0) rectangle (13,1.6);
	\draw[fill=gray!15, thin, opacity=0.6] (13,0) rectangle (14,2.9);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (7,6) (10.5,1) (14,5) };
	\draw[black] (7,2) node {$ L(P,f)=\displaystyle\sum_{i=1}^{n}m_i\Delta x_i $};
	\end{tikzpicture}
	\caption{A lower Riemann sum.}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[blue] (14,5.5) node {$ f(x) $};	
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\draw[fill=gray!15, thin, opacity=0.6] (1,0) rectangle (2,2.35);
	\draw[fill=gray!15, thin, opacity=0.6] (2,0) rectangle (3,3.5);
	\draw[fill=gray!15, thin, opacity=0.6] (3,0) rectangle (4,4.5);
	\draw[fill=gray!15, thin, opacity=0.6] (4,0) rectangle (5,5.25);
	\draw[fill=gray!15, thin, opacity=0.6] (5,0) rectangle (6,5.77);
	\draw[fill=gray!15, thin, opacity=0.6] (6,0) rectangle (7,6);
	\draw[fill=gray!15, thin, opacity=0.6] (7,0) rectangle (8,6);
	\draw[fill=gray!15, thin, opacity=0.6] (8,0) rectangle (9,5.6);
	\draw[fill=gray!15, thin, opacity=0.6] (9,0) rectangle (10,3.55);
	\draw[fill=gray!15, thin, opacity=0.6] (10,0) rectangle (11,1.2);
	\draw[fill=gray!15, thin, opacity=0.6] (11,0) rectangle (12,1.6);
	\draw[fill=gray!15, thin, opacity=0.6] (12,0) rectangle (13,2.9);
	\draw[fill=gray!15, thin, opacity=0.6] (13,0) rectangle (14,5);
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (7,6) (10.5,1) (14,5) };
	\draw[black] (7,2) node {$ U(P,f)=\displaystyle\sum_{i=1}^{n}M_i\Delta x_i $};
	\end{tikzpicture}
	\caption{An upper Riemann sum.}
\end{figure}
So far, this is fairly similar to the way integration was developed in calculus, but now we're going to do something a little different. Instead of taking a limit of Riemann sums, we will simply say a function is Riemann integrable if the supremum of all possible upper Riemann sums coincides with the infimum of all the possible lower Riemann sums. 
\begin{definition}
	Suppose $ f $ is a bounded real function  on the interval $ [a,b] $. Define the values  
	We refer to these as the \textit{\color{red}lower Riemann integral of $ f $ over $ [a,b] $} and \textit{\color{red}upper Riemann integral of $ f $ over $ [a,b] $} respectively.
\end{definition}
The upper and lower Riemann integral will \textit{always} exist for any bounded function on $ [a,b] $. The set $ \mathbf{P}([a,b])\neq\emptyset$, as any interval $ [a,b] $ has a trivial partition of $ \{x_0=a,x_1=b\} $. The set is also bounded, as for all $ P\in\mathbf{P}([a,b]) $, $$\inf_{x\in[a,b]}f(x)\cdot(b-a)\le L(P,f)\le U(P,f)\le \sup_{x\in[a,b]}f(x)\cdot(b-a) .$$ We know the infimum and supremum of $ f $ exist on $ [a,b] $, because $ f $ is bounded. This gives us two nonempty bounded subsets of $ \R $ in the form of $ \{U(P,f)\}_{P\in\textbf{P}([a,b])} $ and $ \{L(P,f)\}_{P\in\textbf{P}([a,b])} $. The supremum and infimum of these sets are guaranteed to exist by the completeness of $ \R $. 

We define the Riemann integral using these upper and lower integrals.
\begin{definition}
	Suppose $ f $ is a bounded real function  on the interval $ [a,b] $. If $$ \lowint_a^b f(x)\ dx=\upint_a^b f(x)\ dx,$$ then we say $ f $ is \textit{\color{red}Riemann integrable (on $ [a,b] $)} and we write the common value of the upper and lower Riemann integral as $$ \int_{a}^{b}f(x)\ dx.$$ We refer to this common value as the \textit{\color{red}Riemann integral of $ f $ on [a,b]}. 
\end{definition}
Figure 58 shows a Riemann integrable function, and the value of the integral on $ [a,b] $. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[blue] (14,5.5) node {$ f(x) $};	
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\fill[fill=gray!15] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5) -- (14,0) -- (1,0) -- (1,1);
	\draw[thick, blue] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5);
	\draw[black] (5.5,2) node {$\displaystyle\lowint_{a}^{b}f(x)\ dx = \displaystyle\upint_{a}^{b}f(x)\ dx =\displaystyle\int_{a}^{b}f(x)\ dx $};
	\end{tikzpicture}
	\caption{ The function $ f $ is Riemann integrable on $ [a,b] $.}
\end{figure}
\begin{remark}[Bounded Functions on a Closed Interval]
	It cannot be stressed enough that we are only able to define the Riemann integral for real functions that are bounded, and can only do so on a closed interval.
\end{remark}
While Definition 6.5 may make sense on a theoretical level, it's not at all practical to use it to verify a function is actually integrable. How are we supposed to possibly find the supremum of all possible lower Riemann sums, and find the infimum of all possible upper Riemann sums?! A simple example will be presented, but it will be the only time we actually use Definition 6.5 to verify integrability. We will therefore need to find some alternate criterion for Riemann integrability.    
\begin{example}
	Let $ f(x)=c $ on $ [a,b] $ for some constant $ c\in\R  $. Suppose $ P=\{0,x_1,\ldots,x_{n-1},1\} $ is a partition of $ [a,b] $. We have  \begin{align*}
			M_i&=\sup_{x\in[x_{i-1},x_i]} f(x)=c,\\
		m_i&=\inf_{x\in[x_{i-1},x_i]} f(x)=c,\\
	\end{align*} 
	For $ i=1,\ldots, n $. The lower and upper Riemann sums will agree, as $ M_i=m_i $ for $ i=1,\ldots, n $. $$ U(P,f)=L(P, f)=\sum_{i=1}^{n}i\cdot(x_k-x_{k-1})=(x_n-x_{n-1})+(x_{n-1}+x_{n-2})+\ldots (x_2-x_1)+(x_1-x_0)=x_n-x_0=b-a.$$ We let $ P $ be arbitrary, so these sums will always be $ 1 $.
	\begin{align*}
		\lowint_a^b f(x)\ dx=\sup\limits_{P\in\textbf{P}([a,b])} L(P,f)=\sup\{c\}=c\\ \upint_a^b f(x)\ dx =\inf\limits_{P\in\textbf{P}([a,b])} U(P,f)=\inf\{c\}=c.
	\end{align*}  
	These values agree, so $ f $ is Riemann integrable, and $$\int_{a}^{b} c\ dx=c(b-a) .$$
\end{example}
\subsection{An Alternative Interpretation: Simple Functions (Very Optional)}
Before we explore the properties of Riemann integration and develop a way to verify we can integrate a function, we can give an alternate formulation of the upper and lower Riemann integral. Not only is this  how \cite{tao2006analysis} opts to introduce the Riemann integral, but it is how we will go about defining a superior form of integration in Section 13. The general idea is that we define the Riemann integral for for very ``simple'' functions, and then relate the integration of these functions to bounded real functions on $ [a,b] $. \textbf{Disclaimer}: this introduction to simple functions is slightly informal for reasons that will be acknowledged afterwards. 

We begin by defining a special type of step function.
\begin{definition}
	Let $ X $ be a subset of $ \R $, and $ E\subset X $. We define the \textit{\color{red}characteristic function on E} as $ \chi_E:X\to\{0,1\} $, where 
	$$ \chi_E(x)=\begin{cases}
	1\text{ if }x\in E\\
	0\text{ if }x\notin E
	\end{cases} $$
\end{definition}
This function is sometimes called the indicator function, as it indicates whether or not an element of $ X $ is in the subset $ E $. We will be interested in characteristic functions in $ \R $, and an example of such a function is seen Figure 59.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,3) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,3);
	\draw[thick,->] (0,0) -- (16,0);	
	\draw[thick, blue] (0,0) -- (8,0);
	\draw[thick, blue] (12,0) -- (16,0);
	\draw[thick] (-0.2,1) -- (0.2,1);
	\draw (12, -0.3) node {$ b$};	
	\draw (8,-0.3) node {$ a$};	
		\draw (10,1.3) node {$ \chi_{[a,b]}$};			
	\draw[thick, blue] (8,1) -- (12,1);
	\filldraw[color=blue, fill=white] (8,0) circle (2pt);
	\filldraw[color=blue, fill=white] (12,0) circle (2pt);
	\filldraw[color=white, fill=blue] (8,1) circle (2pt);
		\filldraw[color=white, fill=blue] (12,1) circle (2pt);
	\end{tikzpicture}
	\caption{The characteristic function $ \chi_{[a,b]}:\R\to\{0,1\} $.}
\end{figure}
We can use characteristic functions to write \textit{any} step function. Step functions written this way will be the ``simple'' functions which we are interested in.
\begin{definition}
	Let $ X $ be a subset of $ \R $. A \textit{\color{red}simple function} $ \varphi: X \to\R $ is a function which can be written as a \textit{finite} linear combination of characteristic functions on $ E\subset X $. That is there exists a set of finite scalers $ \{c_1,\ldots, c_n\}\subset\R $ and finite sets $ E_1,\ldots\,E_n\subset X $ such that $$ \varphi(x)=\sum_{k=1}^nc_k\chi_{E_k}(x).$$ 
\end{definition}
A simple functions domain will always be $ X $, even if the sets on which the characteristic functions are defined do not cover $ X $, i.e $ X\not\subset\bigcup_{k=1}^nE_k $. We simply have $ \varphi(x)=0 $ on the set $ X\backslash\bigcup_{k=1}^nE_k $. The codomain of a simple function will \textit{always} be $ \R $, as the output of a simple function is determined by the set of real scalers $ \{c_1,\ldots,c_n\} $. Where the function attains each value $ c_k $ in the range is determined by the set $ E_k $. A concrete example where $ X\subset \R $ may make things clearer. 
\begin{example}
	Let $ \varphi:\R\to\R $ be the simple function $$ \varphi(x)=2\chi_{[1,3.5)}+5\chi_{[3.5,4)}-2\chi_{[4,7)}+4\chi_{[7,8)}-\chi_{[8,10)}+\chi_{[10,14)}+2\chi_{[14,15]}$$ on $ \R $, where each $ \chi_E $ is defined on a subset $ E\subset[1,15]$. This function is shown in Figure 60. In this case, $ E_1\cup\ldots\cup E_8\neq\R $, but $ \varphi(x) $ is still defined on all of $ \R $. The function takes on the value $ 0 $ outside of $ E_1\cup\ldots\cup E_7 $, but is still defined there. We could also define $ \varphi:[1,15]\to\R $ in a similar fashion. The only difference would be that $ \varphi $ would not be defined as zero on $ (-\infty,1)\cup(15,\infty) $. 
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}
		\draw (-.3,6) node {$ \R$};
		\draw (16,-0.3) node {$ \R$};	
		\draw (15,-0.3) node {$ 15$};
		\draw (1,-0.3) node {$ 1$};	
		\draw[blue] (11,1.3) node {$ \varphi(x)$};		
		\draw[thick,->] (0,-3) -- (0,6);
		\draw[thick,->] (0,0) -- (16,0);
		\draw[thick, blue] (0,0) -- (1,0);	
		\draw[thick, blue] (1,2) -- (3.5,2);
		\draw[thick, blue] (3.5,5) -- (4,5);
		\draw[thick, blue] (4,-2) -- (7,-2);	
		\draw[thick, blue] (7,4) -- (8,4);
		\draw[thick, blue] (8,-1) -- (10,-1);
		\draw[thick, blue] (10,1) -- (14,1);
		\draw[thick, blue] (14,2) -- (15,2);
		\draw[thick, blue] (15,0) -- (16,0);
		\filldraw[color=blue, fill=white] (1,0) circle (2pt);
		\filldraw[color=white, fill=blue] (1,2) circle (2pt);
		\filldraw[color=blue, fill=white] (3.5,2) circle (2pt);
		\filldraw[color=white, fill=blue] (3.5,5) circle (2pt);
		\filldraw[color=blue, fill=white] (4,5) circle (2pt);
		\filldraw[color=white, fill=blue] (4,-2) circle (2pt);
		\filldraw[color=blue, fill=white] (7,-2) circle (2pt);
		\filldraw[color=white, fill=blue] (7,4) circle (2pt);
		\filldraw[color=blue, fill=white] (8,4) circle (2pt);
		\filldraw[color=white, fill=blue] (8,-1) circle (2pt);
		\filldraw[color=blue, fill=white] (10,-1) circle (2pt);
		\filldraw[color=white, fill=blue](10,1) circle (2pt);
			\filldraw[color=blue, fill=white] (14,1) circle (2pt);
		\filldraw[color=white, fill=blue] (14,2) circle (2pt);
					\filldraw[color=white, fill=blue] (15,2) circle (2pt);
		\filldraw[color=blue, fill=white] (15,0) circle (2pt);
		\end{tikzpicture}
		\caption{A simple function $ \varphi:\R\to\R $. }
	\end{figure}
\end{example}
In this particular case the set $ \{E_k\} $ are piecewise disjoint. This need not be the case. Just know that if the intervals are not piecewise disjoint, we can always rewrite $ \varphi(x) $ so they are. We will prove this later on when we treat simple functions with a bit more care. 

Simple functions get their name, as in the context of integration they are simple. Because a simple function is written as a finite linear combination, it is bounded on the entirety of its domain.\footnote{The function is bounded by $ \max\{|c_1|,\ldots|c_n|\} $.} If $ \varphi $ is only defined on some interval $ [a,b] $, then every set $ E_k\subset[a,b] $ is bounded for all $ k $. These facts allow us to appeal directly to geometry to calculate the Riemann integral of a simple function. 
\begin{definition}
	Suppose $ \varphi:[a,b]\to\R $ is a simple function which can be expressed as $ \sum_{k=1}^nc_k\chi_{E_k}(x) $ for $ E_k=(a_k,b_k)\subset[a,b] $ and $ c_k\in\R $. Then we define the \textit{\color{red}Riemann integral of a simple function (on $ [a,b] $)} as $$\int_{a}^{b}\varphi(x)\ dx=\sum_{k=1}^{n}c_k\cdot(b_k-a_k) .$$
\end{definition}
For each ``step'' of the simple function, we multiply the width  of the interval, $ (b_k-a_k) $, by the height the function achieves on that interval, $ c_k $. Adding the area of these rectangles up gives the integral. In Definition 6.8, $ E_k $ is open, but it really does not matter. It could be closed, or half-open. We will freely interchange them in this section, as it won't make a difference. The length of the interval would still be $ b_k-a_k $. 
\begin{example}
	Define $ \varphi:[1,15]\to\R $ as $$ \varphi(x)=2\chi_{[1,3.5)}+5\chi_{[3.5,4)}-2\chi_{[4,7)}+4\chi_{[7,8)}-\chi_{[8,10)}+\chi_{[10,14)}+2\chi_{[14,15]}.$$ Definition 6.8 gives \begin{align*}
		\int_{1}^{15}\varphi(x)\ dx&=\sum_{k=1}^7c_k\cdot(b_k-a_k)\\
							&=2(3.5-1)+5(4-3.5)-2(7-4)+4(8-7)-(10-8)+(14-10)+2(15-14)\\
							&=9.5.
	\end{align*}
	This integral can be seen in Figure 61.
		\begin{figure}[h!]
		\centering
		\begin{tikzpicture}
		\draw (-.3,6) node {$ \R$};
		\draw (16,-0.3) node {$ \R$};	
		\draw[blue] (11,1.3) node {$ \varphi(x)$};		
		\draw[thick,->] (0,-3) -- (0,6);
		\draw[thick,->] (0,0) -- (16,0);
				\draw (15,-0.3) node {$ 15$};
		\draw (1,-0.3) node {$ 1$};		
		\draw[thick, blue] (1,2) -- (3.5,2);
		\draw[thick, blue] (3.5,5) -- (4,5);
		\draw[thick, blue] (4,-2) -- (7,-2);	
		\draw[thick, blue] (7,4) -- (8,4);
		\draw[thick, blue] (8,-1) -- (10,-1);
		\draw[thick, blue] (10,1) -- (14,1);
		\draw[thick, blue] (14,2) -- (15,2);
		
		\draw[fill=gray!15, thin, opacity=0.6] (1,0) rectangle (3.5,2);
		\draw[fill=gray!15, thin, opacity=0.6] (3.5,0) rectangle (4,5);
		\draw[fill=gray!15, thin, opacity=0.6] (4,0) rectangle (7,-2);
			\draw[fill=gray!15, thin, opacity=0.6] (7,0) rectangle (8,4);
			\draw[fill=gray!15, thin, opacity=0.6] (8,0) rectangle (10,-1);
			\draw[fill=gray!15, thin, opacity=0.6] (10,0) rectangle (14,1);	
				\draw[fill=gray!15, thin, opacity=0.6] (14,0) rectangle (15,2);
		\filldraw[color=white, fill=blue] (1,2) circle (2pt);
		\filldraw[color=blue, fill=white] (3.5,2) circle (2pt);
		\filldraw[color=white, fill=blue] (3.5,5) circle (2pt);
		\filldraw[color=blue, fill=white] (4,5) circle (2pt);
		\filldraw[color=white, fill=blue] (4,-2) circle (2pt);
		\filldraw[color=blue, fill=white] (7,-2) circle (2pt);
		\filldraw[color=white, fill=blue] (7,4) circle (2pt);
		\filldraw[color=blue, fill=white] (8,4) circle (2pt);
		\filldraw[color=white, fill=blue] (8,-1) circle (2pt);
		\filldraw[color=blue, fill=white] (10,-1) circle (2pt);
		\filldraw[color=white, fill=blue](10,1) circle (2pt);
		\filldraw[color=blue, fill=white] (14,1) circle (2pt);
		\filldraw[color=white, fill=blue] (14,2) circle (2pt);
		\filldraw[color=white, fill=blue] (15,2) circle (2pt);
		\end{tikzpicture}
		\caption{The integral of a simple function $ \varphi:[1,15]\to\R $}
	\end{figure}
\end{example}
\begin{example}
	Let $ \varphi:[a,b]\to\R $ be the trivial simple function of $ \varphi(x)=\chi_{[a,b] }.$ We have $$\int_{a}^b\varphi(x)\ dx=b-a .$$
\end{example}
Now we that we can integrate simple functions, how do we extend this to any bounded real function on $ [a,b] $? 
\begin{definition}
	Let $ X $ be a set, $ Y $ be an ordered set, $ f:X\to Y $, and $ g:X\to Y  $. We say \textit{\color{red}$ f  $ is greater than or equal to $ g $ (on X)} if $ f(x)\ge g(x) $ for all $ x\in X $, and write $ f\ge g $. Similarly, we say \textit{\color{red}$ f  $ is less than or equal to $ g $ (on X)} if $ f(x)\le g(x) $ for all $ x\in X $, and write $ f\le g $. 
\end{definition}
Like many of the definitions that have been introduced, it will be important to specify a domain when we say a function is greater than or less than another. It should always be specified if it is unclear.

Let there be some real valued function $ f $ that is bounded on the interval $ [a,b] $. Suppose for two simple functions defined on $ [a,b] $, call them $ \varphi $ and $ \psi $, we have $ \varphi\le f\le \psi $ on $ [a,b] $, as seen in Figure 62.
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}
		\draw (-.3,7) node {$ \R$};
		\draw (16,-0.3) node {$ \R$};		
		\draw[thick,->] (0,0) -- (0,7);
		\draw[thick,->] (0,0) -- (16,0);
		\draw[thick] (1,-0.2) -- (1,0.2);
		\draw[thick] (14,-0.2) -- (14,0.2);
		\draw[blue] (14,5.5) node {$ f(x) $};	
		\draw[red] (8,6.4) node {$ \psi(x) $};
		\draw[green] (8,4) node {$ \varphi(x) $};
		\draw[black] (1,-.5) node {$ a $};
		\draw[black] (14,-.5) node {$ b $};	
		\draw[red, thick] (1,2.35)--(2,2.35);
		\draw[red, thick] (2,3.5)--(3,3.5);
		\draw[red, thick] (3,4.5)--(4,4.5);
		\draw[red, thick] (4,5.25)--(5,5.25);
		\draw[red, thick] (5,5.77)--(6,5.77);
		\draw[red, thick] (6,6)--(7,6);
		\draw[red, thick] (7,6)--(8,6);
		\draw[red, thick] (8,5.7)--(9,5.7);
		\draw[red, thick] (9,3.55)--(10,3.55);
		\draw[red, thick] (10,1.2)--(11,1.2);
		\draw[red, thick] (11,1.6)--(12,1.6);
		\draw[red, thick] (12,2.9)--(13,2.9);
		\draw[red, thick] (13,5)--(14,5);
		\draw[green, thick] (1,1)--(2,1);
		\draw[green, thick] (2,2.35)--(3,2.35);
		\draw[green, thick] (3,3.5)--(4,3.5);
		\draw[green, thick] (4,4.5)--(5,4.5);
		\draw[green, thick] (5,5.25)--(6,5.25);
		\draw[green, thick] (6,5.77)--(7,5.77);
		\draw[green, thick] (7,5.69)--(8,5.69);
		\draw[green, thick] (8,3.55)--(9,3.55);
		\draw[green, thick] (9,1.2)--(10,1.2);
		\draw[green, thick] (10,0.98)--(11,0.98);
		\draw[green, thick] (11,1)--(12,1);
		\draw[green, thick] (12,1.6)--(13,1.6);
		\draw[green, thick] (13,2.9)--(14,2.9);
		\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (7,6) (10.5,1) (14,5) };
		\end{tikzpicture}
		\caption{The function $ f $ is real and bounded on the interval $ [a,b] $. We have $ \varphi\le f\le\psi $ on $ [a,b] $ for two simple functions defined on that interval. }
	\end{figure}
Can we somehow use the well defined integrals of $ \varphi $ and $ \psi $ to approximate that of $ f $? If you compare Figure 62 to Figure 56 and Figure 57, then things become clearer. The integrals of $ \varphi $ and $ \psi $ on $ [a,b] $ coincide with the lower and upper Riemann sums shown in Figure 56 and Figure 57! In fact, we can think of a Riemann sum as the integral of a simple function. For a partition $ P=\{x_0,\ldots,x_n\} $ of $ [a,b] $ we may write
\begin{align*}
	\varphi(x)=\sum_{i=1}^{n-1}m_i\chi_{[x_{i-1},x_i)}+m_n\chi_{[x_{n-1},x_n]},\\\psi(x)=\sum_{i=1}^{n-1}M_i\chi_{[x_{i-1},x_i)}+M_n\chi_{[x_{n-1},x_n]},
\end{align*} 
which gives
\begin{align*}
\int_{a}^{b}\varphi(x)\ dx&=\sum_{i=1}^nm_i\Delta x_i,\\
\int_{a}^{b}\psi(x)\ dx&=\sum_{i=1}^nM_i\Delta x_i.	
\end{align*}
Thus we have established the following set inclusions:
\begin{align}
	\{L(P,f)\}_{P\in\textbf{P}([a,b])}&\subset \left\{\int_a^b\varphi(x)\ dx\mid \varphi\le f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\},\\
	\{U(P,f)\}_{P\in\textbf{P}([a,b])}&\subset \left\{\int_a^b\varphi(x)\ dx\mid \varphi\ge f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\}.
\end{align} 
I'm now going to make what may seem like a bold claim, but is certainly corroborated by Figure 62 and our geometric understanding of the area under a curve:
\begin{align*}
\lowint_a^b f(x)\ dx=\sup\{L(P,f)\}_{P\in\textbf{P}([a,b])}&=\sup \left\{\int_a^b\varphi(x)\ dx\mid \varphi\le f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\},\\
\upint_a^b f(x)\ dx=\inf\{U(P,f)\}_{P\in\textbf{P}([a,b])}&=\inf\left\{\int_a^b\varphi(x)\ dx\mid \varphi\ge f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\}.
\end{align*} 
Let's provide an informal proof sketch as to why this works for the lower Riemann integral.\footnote{The case for the upper Riemann integral is similar.}

We know that $ \sup A\le \sup B $, for $ A\subset B $, so (11) gives us the first half of the result. We just need to show that
$$\sup\{L(P,f)\}_{P\in\textbf{P}([a,b])}\ge\sup \left\{\int_a^b\varphi(x)\ dx\mid \varphi\le f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\} .$$ We can show that any integral of such a simple function is less than or equal to a lower Riemann sum, implying that the supremum of the set must take the form of a lower Riemann sum. Suppose $ \varphi\le f$ on $ [a,b]  $ and is writen as $$\varphi(x)=\sum_{i=k}^{n}c_k\chi_{E_k}=\sum_{k=1}^{n}c_k\chi_{[a_k,b_k)} $$ for $ c_k\in\R $ and $ a_k,b_k\in [a,b] $. Assume that $ a_{k+1}=b_{k} $, so $ \cup_{k=1}^n[a_k,b_k)=[a,b] $. We have $ \varphi\le f $, so $ c_k\le\inf_{x\in[a_k,b_k)}f(x) $. Therefore 
$$\int_{a}^{b}\varphi(x)\ dx=\sum_{k=1}^{n}c_k(b_k-a_k)\le \sum_{k=1}^{n} \inf\limits_{x\in[a_k,b_k)}f(x)(b_k-a_k) .$$ But this final sum is just the integral of a simple function corresponding to a lower Riemann sum. Therefore for any integral of a simple function $ \varphi\le f $, there exists lower Riemann sum that is greater than or equal to it. 

This all justifies the following alternate definition of the upper and lower Riemann integrals:
\begin{align*}
\lowint_a^b f(x)\ dx=\sup \left\{\int_a^b\varphi(x)\ dx\mid \varphi\le f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\},\\
\upint_a^b f(x)\ dx=\inf\left\{\int_a^b\varphi(x)\ dx\mid \varphi\ge f\text{ on }[a,b]\text{, } \varphi\text{ simple}\right\}.
\end{align*} 
\textbf{\color{red}If this makes no sense}, that is fine. For the rest of this section, we will use the definitions presented in Subsection 6.1. In fact, we will never formally use any proofs involving Riemann integrals. We will however use simple functions in certain examples. This was only presented in an effort to make Section 13 easier. When we return to the idea of simple functions then, it will actually be even clearer than this case, as Section 12 will develop a formal concept of measuring an interval.  
\begin{remark}[About That Disclaimer]
	Before the introduction of simple functions, I warned that it would not be totally formal. I made a very strong assumption that the length of an interval $ [a,b] $ is $ b-a $, and I made many assumptions about the properties of this length, and the length of the analogous open interval $ (a,b) $. Once again, we haven't defined any notion of length on $ \R $, so this was careless. \cite{tao2006analysis} does a very good job of introducing the basic idea of length on $ \R $ when discussing partitions and simple functions. 
\end{remark}
\subsection{Verifying Riemann Integrability}
Okay, that detour is now over. We return to the more pressing problem of us not having any easy way of verifying a bounded real function on $ [a,b] $ is Riemann integrable. After intorudcing a series of lemmas, we will arrive at a familiar criterion that we will use to verify Riemann integrability.
\begin{lemma}
	Let $ P $ be a partition of the interval $ [a,b] $. If $ P^* $ is a refinement of $ P $, then\begin{align*}
		L(P,f)\le L(P^*,f)\\
		U(P^*,f)\le U(P,f)
	\end{align*} 
\end{lemma}
\begin{proof}
	We will show the result for the first inequality, as the proof for the second is analogous. Suppose that $ P^* $ contains exactly one more point than $ P^* $, call it $ x^* $. There are two consecutive points of $ P $, $ x_{i-1} $ and $ x_i $, such that $x_{i-1}<x^*<x_i $. Define 
	\begin{align*}
		w_1&=\inf\limits_{x\in[x_{i-1},x^*]}f(x),\\w_2&=\inf\limits_{x\in[x^*,x_i]}f(x).
	\end{align*}
	For $ m_i=\inf_{x\in[x_{i-1},x_i]}f(x) $, we have $ w_1\ge m_i $ and $ w_2\ge m_i $.\footnote{The bound $ m_i $ is the original infimum on the interval $ [x_{i-1},x_i] $. When we add $ x^* $ to our partition, this interval gets split into two intervals: $ [x_{i-1},x^*] $, and $ [x^*,x_i] $. We're saying that the infima of these two new subintervals are weakly greater than that of the original interval. Drawing a picture of this can be helpful!} We therefore have 
	\begin{align*}
		L(P^*,f)-L(P,f)&=w_1(x^*-x_{i-1})+w_2(x_i-x^*)-m_i[x_i-x_{i-1}]\\&=(w_1-m_1)(x^*-x_{i-1})+(w_2-m_2)(x_i-x^*)
	\end{align*}
	This is the desired result if $ P^* $ only has one additional point.\footnote{How did we end up with such a simple expression for $ L(P^*,f)-L(P,f) $? Well the only difference between $ P^* $ and $ P $ is that $ P^* $ has the intervals  $ [x_{i-1},x^*] $, and $ [x^*,x_i] $ \textit{instead of} $ [x_{i-1},x_i]  $. All the other intervals are the same and cancel out. We're left with the two additional intervals, and need to subtract the interval which they replaced.} If instead $ P^* $ contains $ k $ more points that $ P $, we simply repeat this process $ k $ times to arrive at our result.  
\end{proof}
\begin{example}
	Let $ f $ be the bounded real function on $ [0,1] $ defined as $ f(x)=x^2 $. Suppose $ P={0,1} $, and $ P^*={0,1/2,1} $. We have \begin{align*}
		L(P,f)&=\inf\limits_{x\in[0,1]}x^2=0\\
		L(P^*,f)&=\inf\limits_{x\in[0,1/2]}x^2+\inf\limits_{x\in[1/2,1]}x^2=0+\frac{1}{4}=\frac{1}{4}\\
	\end{align*}
	This agrees with Lemma 6.1 (as it should).
\end{example}
Our next lemma provides an inequality we would hope holds for the upper and lower Riemann integral.
\begin{lemma}
	Let $ f $ be a bounded real function on $ [a,b] $. We have 
	$$\lowint_{a}^bf(x)\ dx\le \upint_{a}^bf(x)\ dx.$$
\end{lemma}
\begin{proof}
	Let $ P_1 $ and $ P_2 $ be partitions of $ [a,b] $, and $ P^* $ be a common refinement of $ P_1 $ and $ P_2 $. By Lemma 6.1,
	$$ L(P_1,f)\le L(P^*,f)\le U(P^*,f)\le U(P_2,f).$$ If we fix $ P_1 $ and $ P_2 $, we can take the supremum and infimum over all of $ \mathbf{P}([a,b]) $. 
	\begin{align*}
	\sup\limits_{P_1\in\textbf{P}([a,b])}	L(P_1,f)&\le \inf\limits_{P_2\in\textbf{P}([a,b])}U(P_2,f)\\\lowint_{a}^bf(x)\ dx&\le \upint_{a}^bf(x)\ dx
	\end{align*}
\end{proof}
When does Lemma 6.2 hold with equality? By Definition 6.5, this occurs precisely when $ f $ is Riemann integrable, because this is how we defined Riemann integrability. This would imply that we have $$\lowint_{a}^bf(x)\ dx< \upint_{a}^bf(x)\ dx $$ if and only if $ f $ is \textit{not} Riemann integrable. At the end of this section, we will see some examples of this.

We are able to present the first of several results that will allow us to determine if a function is Riemann integrable. This theorem not only gives us sufficient conditions for integration, but it also gives a necessary condition for integrability. This gives it much more bite. 
\begin{theorem}[Riemann's Criterion]
	Suppose $ f $ is a bounded real function on $ [a,b] $. The function $ f $ is Riemann integrable \textit{if and only if} for all $ \varepsilon>0 $ there exists a partition such that $$ U(P,f)-L(P,f)<\varepsilon.$$
\end{theorem} The proof of this crucial theorem amounts to just shuffling around inequalities. It only looks so long due to liberal typesetting.  
\begin{proof}{\color{white}a}
		\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ f $ is Riemann integrable, and let $ \varepsilon>0 $. By the completeness of the real numbers and properties of the supremum, there exists a partition $ P_1 $ such that \begin{align*}
			U(P_1,f)-\sup\limits_{P\in\textbf{P}([a,b])}U(P,f)&<\frac{\varepsilon}{2},\\
			U(P_1,f)-\upint_a^b f(x)\ dx&<\frac{\varepsilon}{2},\\
			U(P_1,f)-\int_a^b f(x)\ dx&<\frac{\varepsilon}{2}.
		\end{align*} 
		Similarly, there exists a partition $ P_2 $ such that $$\int_a^bf(x)\ dx-L(P_2,f)<\frac{\varepsilon}{2}. $$ If we choose $ P^* $ to be the common refinement of $ P_1 $ and $ P_2 $,\footnote{Remember all the proofs with convergent sequences where we had two values $ N_1 $ and $ N_2 $, and took $ N=\max\{N_1,N_2\} $ (see the proof of Proposition 3.2 for the first example of this)? This is essentially what we're doing by taking the common refinement of $ P_1 $ and $ P_2 $. We know that the all the $ \varepsilon $ inequalities will hold simultaneously by Lemma 6.1, allowing us to combine them.} then by Lemma 6.1, \begin{align*}
U(P^*,f)-\int_a^b f(x)\ dx&<U(P_1,f)-\int_a^b f(x)\ dx<\frac{\varepsilon}{2},\\\int_a^bf(x)\ dx-L(P^*,f)&<\int_a^bf(x)\ dx-L(P_2,f)<\frac{\varepsilon}{2}.
		\end{align*}
		Combining these two inequalities for $ P^* $ yields
		\begin{align*}
U(P^*,f)-L(P^*,f)&=U(P^*,f)-L(P^*,f)+0\\&=U(P^*,f)-L(P^*,f)+\left[-\int_a^b f(x)+\int_a^bf(x)\ dx\right]\\&=\left[U(P^*,f)-\int_a^b f(x)\right]+\left[\int_a^bf(x)\ dx-L(P^*,f)\right]\\&<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}\\&=\varepsilon.
		\end{align*}
		\item [$ (\Longleftarrow) $] Suppose for all $ \varepsilon>0 $ there exists a partition such that $ U(P,f)-L(P,f)<\varepsilon $. By the definition of supremum and infimum we have 
		\begin{align*}
				L(P,f)\le\sup\limits_{P\in\textbf{P}([a,b])}U(P,f)=\lowint_{a}^b f(x)\ dx,\\
				\upint_{a}^b f(x)\ dx\le \inf\limits_{P\in\textbf{P}([a,b])}U(P,f)\le U(P,f),
		\end{align*}
		for all $ P\in\mathbf{P}([a,b]) $. Combining this inequalities with Lemma 6.2 gives $$ L(P,f)\le\lowint_{a}^b f(x)\ dx\le\upint_{a}^b f(x)\ dx\le U(P,f).$$ This implies that $$0\le\lowint_{a}^b f(x)\ dx- \upint_{a}^b f(x)\ dx \le \varepsilon,$$ but if this holds for all $ \varepsilon>0 $, $$ \lowint_{a}^b f(x)\ dx = \upint_{a}^b f(x)\ dx.$$ Therefore $ f $ is Riemann integrable. 
	\end{enumerate}
\end{proof}
Riemann's Criterion should not catch us off guard. If $ f $ is integrable then the lower and upper Riemann integrals are equal. By the definition of those values, we need that the upper and lower Riemann sums become arbitrarily close to each other. We've seen this type of behavior with sequences, limits, and continuity. This time instead of some $ n $ or $ \delta $ term which correspond to $ \varepsilon $, we have a partition $ P $ that corresponds to $ \varepsilon $. As we take $ \varepsilon $ to be smaller and smaller, we will need to find finer and finer partitions $ P $, but we will always be able to do this in order to satisfy $ U(P,f)-L(P,f)<\varepsilon $.  
\begin{remark}[Riemann Criterion and Cauchy Criterion]
The Riemann Criterion may have nothing to do with sequences, but it shares one important similarity with the Cauchy Criterion. Recall that the Cauchy Criterion is so useful because it does not require us to know the limit of a sequence. The same is true with the Riemann Criterion. We don't need to know the integral of $ f $ on $ [a,b] $ to prove it exists! 
\end{remark}
\begin{example}
Let $ f:[0,1]\to \R $ be $$f(x)=\begin{cases}
1/n\text{ if }x=1/n\\0\text{ otherwise}
\end{cases}. $$ We can use the Riemann Criterion to show this function is integrable on $ [0,1] $. Let $ \varepsilon>0 $. The sequence $ \{1/n\} $ converges to $ 0 $, so for all $ \varepsilon/2>0 $ there exists an $ N $ such that $ 1/n\in[0,\varepsilon/2] $ for all $ n\ge N $. Only a finite number of values of the form $ 1/n $ are in the interval $ [\varepsilon/2,1] $ (Proposition 3.1). We can cover these finite values by intervals $ [x_1,x_2],\ldots,[x_{m-1},x_m] $ such that $ x_i\in[\varepsilon/2,1] $ for all $ i=1,\ldots,m $. The finite length of these $ m $ intervals is less than $ \varepsilon/2 $. Let $ P=\{0,\varepsilon,x_1,\ldots,x_m\} $. We have $$U(P,f)-L(P,f)=(\varepsilon/2-0)+\underbrace{(x_2-x_1)+\ldots+(x_m-x_{m-1}) }_{<\varepsilon/2}<\varepsilon.$$ The function is Riemann integrable.\footnote{If you keep going with this example, you can verify that $ \int_{0}^{1}f(x)\ dx=0 $.} 
\end{example}
The Riemann Criterion gives way to several related results that put integrability in conversation with limitings processes. 
\begin{proposition}
	Suppose $ f $ is a bounded real function on the interval $ [a,b] $.
	\begin{enumerate}
	\item If $ U(P,f)-L(P,f)<\varepsilon $ for some $ P $, then it holds for every refinement of $ P $.
	\item If $ U(P,f)-L(P,f)<\varepsilon $ for some $ P=\{x_0,\ldots,x_n\} $, and if $ s_i,t_i\in[x_{i-1},x_{i}] $, then $$ \sum_{i=1}^n|f(s_i)-f(t_i)|\Delta x_i<\varepsilon.$$
	\item If $ f $ is Riemann integrable, $ U(P,f)-L(P,f)<\varepsilon $ for some $ P=\{x_0,\ldots,x_n\} $, and $ t_i\in[x_{i-1}-x_{i}]$, then $$ \left\lvert \sum_{i=1}^nf(t_i)\Delta x_i-\int_{a}^{b}f(x)\ dx\right\rvert<\varepsilon.$$
	\end{enumerate}
\end{proposition}
\begin{proof}{\color{white}a}
\begin{enumerate}
	\item Let $ P^* $ be a refinement of $ P $. By Lemma 6.1, $$U(P^*,f)-L(P^*,f)<U(P,f)-L(P,f)<\varepsilon .$$ 
	\item We have $ f(s_i),f(t_i)\in[m_i,M_i] $, as $ f $ is bounded on any interval by the supremum and infimum on that interval. This means that $$|f(s_i)-f(t_i)|\le M_i-m_i .$$ Therefore $$\sum_{i=1}^n|f(s_i)-f(t_i)|\Delta x_i\le\sum_{i=1}^n(M_i-m_i)\Delta x_i=\sum_{i=1}^n M_i\Delta x_i-\sum_{i=1}^n m_i\Delta x_i=<U(P,f)-L(P,f)<\varepsilon. $$
	\item We will always have the two following inequalities:
	\begin{align*}
		L(P,f)\le\sum_{i=1}^nf(t_i) \Delta x_i\le U(P,f),\\
		L(P,f)\le\int_{a}^{b}f(x)\ dx\le U(P,f).
	\end{align*}
	These combine to give $$  \left\lvert \sum_{i=1}^nf(t_i)\Delta x_i-\int_{a}^{b}f(x)\ dx\right\rvert\le U(P,f)-L(P,f)<\varepsilon. $$
\end{enumerate}
\end{proof}
This proposition seems like a random string of inequalities,\footnote{To be honest, most of real analysis seems like a random string of inequalities sometimes.} but we will use them to prove many important results.
\subsection{Properties of Riemann Integration}
Now we can present some familiar properties of the integral. Just like when we did this with limits and derivatives, most of these may be familiar, so examples won't be furnished for every single possible property. 
\begin{theorem}
Suppose $ f $ and $ g $ are both bounded real functions on $ [a,b] $ which are integrable. Then 
\begin{enumerate}
	\item $ f+g $ is integrable, and $$ \int_{a}^{b}f(x)+g(x)\ dx=\int_{a}^{b}f(x)\ dx+\int_{a}^{b}g(x)\ dx;$$
	\item $ cf $ is integrable for $ c\in\R $, and $$\int_{a}^{b}cf(x)\ dx=c\int_{a}^{b}f(x)\ dx; $$
	\item (Monotonicity) if $ f\le g $ on $ [a,b] $ we have $$\int_{a}^{b}f(x)\ dx\le \int_{a}^{b}g(x)\ dx.$$
	\item (Additivity) for $ c\in[a,b] $, $ f $ is integrable on $ [a,c] $ and $ [c,b] $, and $$\int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx=\int_{a}^{b}f(x)\ dx $$
	\item if $ |f(x)|\le M $ on $ [a,b] $ we have $$ \left\lvert\int_{a}^{b}f(x)\ dx \right\rvert\le M(b-a)$$
\end{enumerate}
The proofs of these are all really similar and monotonous, so fair warning.\footnote{There is a reason that \cite{rudin1964principles} omits most of them.}
\begin{proof}{\color{white}a}

	\begin{enumerate}
	\item Let $ P $ be a partition of $ [a,b] $. We have 
	\begin{align}
	L(P,f)+L(P,g)\le L(P,f+g)\le U(P,f+g)\le U(P,f)+U(P,g).
	\end{align}
	For $ \varepsilon>0 $ there are partitions $ P_1 $ and $ P_2 $ such that\begin{align*}
	U(P_1,f)-L(P_1,f)<\frac{\varepsilon}{2},\\
	U(P_2,g)-L(P_2,g)<\frac{\varepsilon}{2}.
	\end{align*}
	If we take $ P^* $ to be the common refinement of $ P_1 $ and $ P_2 $, then 
	\begin{align*}
	U( P^*,f)-L( P^*,f)<\frac{\varepsilon}{2},\\
	U( P^*,g)-L( P^*,g)<\frac{\varepsilon}{2}.
	\end{align*}
	These inequalities combined with (13) gives $U(P^*,f+g)-L(P^*,f+g)<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon ,$ so $ f+g $ is Riemann integrable.
	For this same $ P^* $, \begin{align*}
		U(P^*,f)&<\int_{a}^{b} f(x)\ dx +\frac{\varepsilon}{2},\\
		U(P^*,g)&<\int_{a}^{b} g(x)\ dx +\frac{\varepsilon}{2}.
	\end{align*}
	Again by (13), $$ \int_{a}^{b}f(x)+g(x)\ dx\le U(P^*,f+g)\le U(P^*,f)+U(P^*,g)<\int_{a}^{b} f(x)\ dx +\int_{a}^{b} g(x)\ dx +\varepsilon.$$ This holds for all $ \varepsilon>0 $, so $$\int_{a}^{b}f(x)+g(x)\ dx\le\int_{a}^{b} f(x)\ dx +\int_{a}^{b} g(x)\ dx. $$ Now for the reverse direction all at once: 
	\begin{align*}
	L(P^*,f)&>\int_{a}^{b} f(x)\ dx -\frac{\varepsilon}{2},\\
	L(P^*,g)&>\int_{a}^{b} g(x)\ dx -\frac{\varepsilon}{2},\\
	\int_{a}^{b}f(x)+g(x)\ dx\ge L(P^*,f+g)\ge L(P^*,f)&+L(P^*,g)>\int_{a}^{b} f(x)\ dx +\int_{a}^{b} g(x)\ dx -\varepsilon\\\int_{a}^{b}f(x)+g(x)\ dx&\ge\int_{a}^{b} f(x)\ dx +\int_{a}^{b} g(x)\ dx.
	\end{align*}
	Therefore we have $$\int_{a}^{b}f(x)+g(x)\ dx=\int_{a}^{b} f(x)\ dx +\int_{a}^{b} g(x)\ dx. $$
 	\item 	First suppose $ c\ge 0 $. For any set $ A\subset [a,b] $, \begin{align*}
 		\sup\limits_{x\in A} cf(x)=c\sup\limits_{x\in A} f(x),\\\inf\limits_{x\in A} cf(x)=c\inf\limits_{x\in A} f(x).
 	\end{align*}
 	This gives
 	\begin{align*}
 		\upint_a^bf(x)\ dx= \inf\limits_{P\in\textbf{P}([a,b])}U(P,cf)=c\inf\limits_{P\in\textbf{P}([a,b])}U(P,f)=c\upint_a^bf(x)\ dx,\\\lowint_a^bf(x)\ dx= \sup\limits_{P\in\textbf{P}([a,b])}L(P,cf)=c\inf\limits_{P\in\textbf{P}([a,b])}L(P,f)=c\lowint_a^bf(x)\ dx.
 	\end{align*}
 	 Since $ f $ is Riemann integrable, its upper and lower Riemann integrals are equal. This establishes the integrability of $ f $, as $$ c\lowint_a^bf(x)\ dx=c\upint_a^bf(x)\ dx=c\int_a^bf(x)\ dx.$$
 	 
 	 Now let $ c=-1 $. In this case, we can't ``factor'' out a constant from a supremum and infimum. Instead, for any $ A\subset[a,b] $, we have
 	 \begin{align*}
 	 \sup\limits_{x\in A} -f(x)=-\inf\limits_{x\in A} f(x),\\\inf\limits_{x\in A} -f(x)=-\sup\limits_{x\in A} f(x).
 	 \end{align*}
 	 We will have $ U(P,-f)=-L(P,f) $ and $ L(P,-f)=-U(P,f) $ for any partition. This gives
 	 \begin{align*}
 	 \upint_a^b-f(x)\ dx= \inf\limits_{P\in\textbf{P}([a,b])}U(P,-f)=\inf\limits_{P\in\textbf{P}([a,b])}-L(P,f)=-\sup\limits_{P\in\textbf{P}([a,b])}L(P,f)=-\lowint_a^bf(x)\ dx,\\\lowint_a^b-f(x)\ dx= \sup\limits_{P\in\textbf{P}([a,b])}L(P,-f)=\sup\limits_{P\in\textbf{P}([a,b])}-U(P,f)=-\inf\limits_{P\in\textbf{P}([a,b])}U(P,f)=-\upint_a^bf(x)\ dx. 	 \end{align*}
 	 Since $ f $ is Riemann integrable, its negative upper and lower Riemann integrals are equal, so $$ \lowint_a^b-f(x)\ dx=c\upint_a^b-f(x)\ dx=c\int_a^b-f(x)\ dx. $$
 	 In general, if $ c<0 $, we can write it as $ -1\cdot|c| $ and apply the first two cases. 
	\item By the two previous parts, we can show that the integral of $ g-f $ is not negative, as $$ \int_{a}^{b}g(x)\ dx+\int_{a}^{b}-f(x)\ dx=\int_{a}^{b}g(x)-f(x)\ dx.$$ We know $ g-f\ge 0 $, so $ \inf_{x\in A}(f(x)-g(x))\ge 0 $ for any $ A\subset [a,b] $.  This means that for any $ P $, $ L(P,f)\ge 0 $, so $$ \int_{a}^{b}g(x)-f(x)\ dx\ge L(P,f)\ge 0.$$
	\item For all $ \varepsilon>0 $, there exists a partition $ P $ such that $ U(P,f)-L(P,f)<\varepsilon $. Define $ P^*=P\cup\{c\} $ to be the refinement of $ P $ which results from adding $ c $.\footnote{It could be the case that $ c\in P $. This would just mean that $ P^*=P $.} Let $ Q=P^*\cap[a,c] $ and $ R=P^*\cap[c,b] $ be partitions of $ [a,b] $ and $ [c,b] $ respectively. For these partitions 
	\begin{align*}
		U(P^*,f)=U(Q,f)+U(R,f),\\
		L(P^*,f)=L(Q,f)+L(R,f).
	\end{align*}
	We can conclude \begin{align*}
		U(Q,f)-L(Q,f)=U(P^*,f)-L(P^*,f)-[U(R,f)-L(R,f)]\le U(P,f)-L(P,f)<\varepsilon,\\
			U(R,f)-L(R,f)=U(P^*,f)-L(P^*,f)-[U(Q,f)-L(Q,f)]\le U(P,f)-L(P,f)<\varepsilon.
	\end{align*}
	This shows that $ f $ is integrable on $ [a,c] $ and $ [c,b] $ by Riemann's Criterion.
	
	We have 
	\begin{align*}
		\int_{a}^{b}f(x)\ dx\le U(P,f)=U(Q,f)+U(R,f)< L(Q,f)+L(R,f)+\varepsilon< \int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx+\varepsilon,\\
		\int_{a}^{b}f(x)\ dx\ge U(P,f)=U(Q,f)+U(R,f)> U(Q,f)+U(R,f)-\varepsilon> \int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx-\varepsilon,
	\end{align*}
	which combine to give
	$$ \int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx-\varepsilon<\int_{a}^{b}f(x)\ dx< \int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx+\varepsilon.$$ If this for all $ \varepsilon>0 $, then $$\int_{a}^{b}f(x)\ dx=\int_{a}^{c}f(x)\ dx+\int_{c}^{b}f(x)\ dx $$
	\item Treat $ M $ as a constant function on $ [a,b] $. Example 6.1 showed that $$\int_{a}^{b}M\ dx=M(b-a). $$ If $ |f(x)|\le M $, then $ -M\le f(x)\le M $. By monotonicity, $$-M(b-a)=\int_{a}^{b}-M\ dx\le \int_{a}^{b}f(x)\ dx\le \int_{a}^{b}M\ dx=M(b-a), $$ which can be written as $$ \left\lvert\int_{a}^{b}f(x)\ dx \right\rvert\le M(b-a) .$$
	\end{enumerate}

\end{proof}
\end{theorem}
\begin{example}[Linearity]
The first 2 parts of Theorem 6.2 give that Riemann integration is linear. For any $ c,d\in\R $, we will have $$\int_{a}^{b}cf(x)+dg(x)\ dx=c\int_{a}^{b}f(x)+d\int_{a}^{b}g(x)\ dx .$$ This will turn out to be \textit{very very} important in later sections. We will not be working with the Riemann integral than, but we'll see that integration in general is intrinsically linked to linearity. 
\end{example}

While \textit{the } Mean Value Theorem deals with derivatives, a similar result holds for integrals. It asserts that a continuous function must take on its average value on an interval.
\begin{proposition}[Mean Value Theorem for Integrals I]
Let $ f $ be a bounded real function on $ [a,b] $. If $ f $ is continuous, then there exists a $ c\in[a,b] $ such that $$f(c)=\frac{1}{b-a}\int_{a}^{b}f(x)\ dx.$$	
\end{proposition}
\begin{proof}
	Since $ f $ is continuous, we can use the Extreme Value Theorem. The function $ f $ must attain a maximum $ M $ and minimum $ m $ on $ [a,b] $. By monotonicity, 
	\begin{align*}
		\int_{a}^{b}f(m)\ dx\le &\int_{a}^{b}f(x)\ dx\le \int_{a}^{b}f(M)\ dx,\\
		f(m)(b-a)\le &\int_{a}^{b}f(x)\ dx\le f(M)(b-a),\\
		f(m)\le \frac{1}{b-a}&\int_{a}^{b}f(x)\ dx\le f(M).
	\end{align*}
	By the Intermediate Value Theorem, $ f $ takes on every value in $ [m,M] $, so there must be some $ c $ such that $$f(c)=\frac{1}{b-a}\int_{a}^{b}f(x)\ dx.$$
\end{proof}
The integral in Proposition 6.2 corresponds to the average value attained by the function on $ [a,b] $. This proposition has a nice geometric interpretation if the equality is written as $$ f(c)(b-a)=\int_{a}^{b}f(d)\ dx.$$ As shown in Figure 63, we can find some rectangle of height $ f(c) $ on the interval $ [a,b] $ that equals the area under $ f $. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\fill[fill=blue!10, opacity=0.8] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5) -- (14,0) -- (1,0) -- (1,1);
	\fill[fill=red!20, opacity=0.8] (1,0) rectangle (14,3);
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[thick] (-0.2,3) -- (0.2,3);
	\draw (-.6,3) node {$ f(c)$};
	\draw[thick] (2.7,-0.2) -- (2.7,0.2);
	\draw[blue] (14,5.5) node {$ f(x) $};	
	\draw[blue] (6,4) node {$ \displaystyle\int_{a}^{b}f(x) $};
	\draw[red] (10.5,2) node {$ f(c)(b-a) $};
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\draw[black] (2.7,-.5) node {$ c $};
	\draw[thick, blue] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5);
	
	\end{tikzpicture}
	\caption{ The Mean Value Theorem for Integrals says that we can find a value $ c\in[a,b] $ such that the red rectangle is equal in area to the integral shown in blue.}
\end{figure}
A second form of the Mean Value Theorem for Integrals is even more useful, as it allows us to find a value in $ [a,b] $ that allows us to ``factor'' a function out of the integral of a product.
\begin{proposition}[Mean Value Theorem for Integrals II]
Let $ f $ be a bounded real function on $ [a,b] $. If $ f $ is continuous, and $ g $ is integrable function $ [a,b] $ and does not change signs,  then there exists a $ c\in[a,b] $ such that $$\int_{a}^{b}f(x)g(x)\ dx=f(c)\int_{a}^{b}g(x)\ dx.$$
\end{proposition}
\begin{proof}
	If $ g(x)=0 $ for all $ x\in[a,b] $, then the result holds trivially. Assume instead that $ g(x)>0 $. Since $ f $ is continuous, we can use the Extreme Value Theorem. The function $ f $ must attain a maximum $ M $ and minimum $ m $ on $ [a,b] $. 
	\begin{align*}
		f(m)\le &f(x)\le f(M),\\f(m)g(x)\le &f(x)g(x)\le f(M)g(x).
	\end{align*} 
	Multiply by $ g(x) $ will not change the inequality, as $ g(x)>0 $ for all $ x\in[a,b] $.\footnote{The other case where $g(x)<0$ would reverse the order of the inequality, but that is fine. What's important is that we have an upper and lower bound on $ f(x)g(x) $, not what those bounds happen to be.} By monotonicity and the linearity of integration, 
	\begin{align*}
f(m)g(x)\le &f(x)g(x)\le f(M)g(x),\\\int_{a}^{b}f(m)g(x)\ dx\le \int_{a}^{b}&f(x)g(x)\ dx\le \int_{a}^{b}f(M)g(x)\ dx,\\f(m)\int_{a}^{b}g(x)\ dx\le \int_{a}^{b}&f(x)g(x)\ dx\le f(M)\int_{a}^{b}g(x)\ dx.
	\end{align*} 
	If we divide by the integral of $ g $, then $$f(m)\le\frac{\int_{a}^{b}f(x)g(x)\ dx}{\int_{a}^{b}g(x)\ dx}\le f(M). $$ By the continuity of $ f $ and the Intermediate Value Theorem, there exists a $ c\in\R $ such that $$f(c)=\frac{\int_{a}^{b}f(x)g(x)\ dx}{\int_{a}^{b}g(x)\ dx} ,$$ which can be expressed as $$\int_{a}^{b}f(x)g(x)\ dx=f(c)\int_{a}^{b}g(x)\ dx. $$
	As mentioned in Footnote 99, the case where $ g<0 $ is virtually the same. The only difference is that all the inequalities are reversed, but we're still able to apply the Intermediate Value Theorem. 
\end{proof}
\begin{example}
	For Proposition 6.3, let $ g(x)=1$ on $ [a,b] $. In this case we recover Proposition 6.2.
	\begin{align*}
		\int_{a}^{b}f(x)g(x)\ dx&=f(c)\int_{a}^{b}g(x)\ dx,\\
		\int_{a}^{b}f(x)\cdot 1\ dx&=f(c)\int_{a}^{b} 1\ dx,\\
		\int_{a}^{b}f(x)g(x)\ dx&=f(c)(b-a).
	\end{align*}
	This observation shows that Proposition 6.3 is a generalized version of its predecessor.  
\end{example}
\begin{example}[What if $ g $ Changes Sign?]
	Proposition 6.3 stipulates that the function $ g $ cannot change sign on $ [a,b] $. This is essential. If this is not the case, then the inequality  
	$$f(m)g(x)\le f(x)g(x)\le f(M)g(x)$$ will not hold for all $ x\in [a,b] $. Suppose $ g(x)=x $ on $ [-1,1] $, and $ f(x)=x $. In this case $$f(x)\int_{-1}^{1}g(x)\ dx=0 $$ for all $ x $, but $ f(x)g(x)=x^2 $, so $$ \int_{-1}^{1}f(x)g(x)\ dx\neq 0 .$$ Therefore Proposition 6.3 does not hold. 
\end{example}
\subsection{Riemann Integration and Continuity}
With the Riemann Criterion in hand, we're able to prove that a function is integrable. We will now use it to show that two very general classes of functions are integrable, the first being continuous functions.

The fact that all continuous functions (on $ [a,b] $) are integrable should not come as a surprise. Not only is nearly every function integrated in a calculus course continuous, but continuity and integrability are both the results of limiting behavior with an arbitrary $ \varepsilon>0 $ (the latter being the case as a result of Riemann's Criterion). 
\begin{theorem}[Continuity implies Integrability]
If $ f $ is a real continuous function on $ [a,b] $, then $ f $ is Riemann integrable on $ [a,b] $.
\end{theorem}
\begin{proof}
	First note that a continuous function on $ [a,b] $ is in fact bounded on $ [a,b] $ by the Extreme Value Theorem.\footnote{This is important to point out because we only can define the Riemann integral for bounded real functions.} Furthermore, $ [a,b] $ is compact (Heine-Borel), so $ f $ is uniformly continuous on $ [a,b] $ (Theorem 4.8). For all $ \varepsilon>0 $, there exists a $ \delta $ such that $$|f(x)-f(y)|<\frac{\varepsilon}{n(b-a)} ,$$ for all $ x,y\in[a,b] $ which satisfy $ |x-y|<\delta $. Choose a partition  $ P $ of $ [a,b] $ such that $|x_i-x_{i-1}|<\delta $ for $ i=1,\ldots, n$.\footnote{For example, you could take form $ P $ with $ n $ intervals of length $ (b-a)/n $ for $ n>(b-a)/\delta $} 
	
	Our function is continuous, so it achieves its maximum and minimum on each $ [x_{i-1},x_{i}]\subset P $, but in this case those are $ M_i $ and $ m_i $. We therefore have $$|f(s_i)-f(t_i)|<|M_i-m_i|<\frac{\varepsilon}{b-a} $$ for $ |s_i-t_i|<\delta $, where $ f(s_i)=M_i $ and $ f(t_i)=m_i $.\footnote{That is, $ s_i $ and $ t_i $ are where the function acheives its maximum $ M_i $ and minimum $ m_i $ respectively.} This allows us to verify that the Riemann Criterion holds for the partitions $ f $: $$ U(P,f)-L(P,f)=\sum_{i=1}^nM_i\Delta x_i-\sum_{i=1}^nm_i\Delta x_i=\sum_{i=1}^n(M_i-m_i)\Delta x_i<n\cdot\frac{\varepsilon}{n(b-a)}\cdot(b-a)<\varepsilon.$$
\end{proof}
\begin{example}[Integrable But Not Continuous]
	The converse of Theorem 6.3 \textit{is not true}. Example 6.6 showed a function that failed to be continuous on $ [0,1] $ but is still integrable on $ [0,1] $. 
\end{example}
\begin{proposition}[Continuous Composition Preserves Integrability]
Suppose $ f $ is a bounded real function on $ [a,b] $, $ m\le f\le M $, $ \phi $ is continuous on $ [m,M] $, and $ h(x)=\phi\circ f(x)=\phi(f(x)) $ on $ [a,b] $. Then $ h $ is integrable on $ [a,b] $.
\end{proposition}
For the proof of this result, it's important to remember that the domain of $ \phi $ is the range of $ f $. It can be easy to get bogged down with the notation in this one. This also will be the most intense proof involving $ \varepsilon $ yet. 
\begin{proof}
	Let $ \varepsilon>0 $. The function $ \phi $ is continuous on the compact set $ [a,b] $, so it is uniformly continuous. There exists a $ \varepsilon>0 $ such that $ \delta<\varepsilon $ and $$ |\phi(s)-\phi(t)|<\frac{\varepsilon}{(b-a)+2\sup_{v\in[m,M]}|\phi(v)|} $$ whenever $ |s-t|<\delta $ for $ s,t\in[m,M] $.\footnote{Woah, what is up with us saying $ \delta<\varepsilon $!? Well if the inequalities hold for $ \delta $, then it should hold for any number less than $ \delta'=\min\{\delta,\varepsilon\} $. If we have $ \delta'<\varepsilon $, then $ \delta'=\delta $. Therefore we lost no generality in saying $ \delta'<\varepsilon $. We simply introduce it as $ \delta $ instead of explicitly going through this though process with $ \delta' $.}
	
	Since $ f $ is integrable, there is a partition $ P=\{x_0,\ldots,x_n\} $ on $ [a,b] $ for which
	\begin{align}
		U(P,f)-L(P,f)<\delta^2
	\end{align} by Riemann's Criterion. This partition gives a partition of $ [m,M] $ in $ P^*=\{m=f(x_0),f(x_1),\ldots,f(x_n)=M\} $.\footnote{This is the partition that we will use with $ \phi:[m,M]\to\R $.} Let \begin{align*}
		M_i=\sup_{x\in[x_{i-1},x_i]} f(x),\ \ \ \
		m_i=\inf_{x\in[x_{i-1},x_i]} f(x),\ \ \ \
		M_i^*=\sup_{t\in[f(x_{i-1}),f(x_i)]} \phi (t),\ \ \ \ m_i^*=\inf_{t\in[f(x_{i-1}),f(x_i)]} \phi(t).
	\end{align*}
Figure 64 shows what a partition $ M_i^* $ and $ m_i^* $ will look like.
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.4) node {$ f([a,b])$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);

	\draw[thick,dashed] (3,0)-- (3,3.5);
	\draw[thick, dashed] (7,6) -- (0,6);
	\draw[thick, dashed] (13,0) -- (13,3.2);
	\draw[thick, dashed] (10.5,1) -- (0,1);
	\draw[blue] (14,5.5) node {$ \phi(f(x)) $};	
	\draw (-.3,6) node {$ M_i^*$};
	\draw (-.3,1) node {$ m_i^*$};
	\draw[black] (3,-.5) node {$ f(x_{i-1}) $};
	\draw[black] (13,-.5) node {$ f(x_{i}) $};
	\draw[black] (1,-.5) node {$ m $};
	\draw[black] (14,-.5) node {$ M $};	
	\draw[thick, blue] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5);
	\end{tikzpicture}
	\caption{An interval $ [f(x_{i-1}),f(x_{i})]\subset P^* $, and the values $ M_i^* $  and $ m_i^* $.}
\end{figure}
 We can divide the indices for the partition $ P $ into two sets \begin{align*}
A&=\left\{i\mid M_i-m_i<\delta \right\},\\
B&=\left\{i\mid M_i-m_i\ge\delta \right\}.
\end{align*} 
If $ i\in A $, then by the uniform continuity of $ \phi $, $$|\phi(M_i)-\phi(m_i)|=|M_i^*-m_i^*|<\varepsilon .$$ If $ i\in B $ then $$|M_i^*-m_i^*|\le 2K ,$$ for $ K=\sup|\phi(t)| $ and $ m\le t\le M $.\footnote{In the context of Figure 64, this inequality makes a bit more sense. The distance between the $ \sup $ and $ \inf $ of $ \phi(t) $ on the interval $ [f(x_{i-1}),f(x_{i})] $ must be less than twice the absolute value of the $ \sup $ and of $ \phi $ on the whole domain being partitioned $ [m,M] $. If $ m_i^*=-M_i^* $, then $ |M_i^*-m_i^*|=2M_i^* $. If $ M_i^* $ happens to be the supremum on all of $ [m,M] $, that is $ K=M_i^* $, then we have the equality $ |M_i^*-m_i^*|=2K $. Understanding this inequality may be the toughest part of this proof, and it's worth drawing some pictures if it is not clear.} 	By (14), \begin{align*}
	&U(P,f)-L(P,f)=\sum_{i=1}^nM_i\Delta x_i-\sum_{i=1}^nm_i\Delta x_i=\sum_{i=1}^n(M_i-m_i)\Delta x_i<\delta^2\\\implies&\sum_{i\in A}(M_i-m_i)\Delta x_i+\sum_{i\in B}(M_i-m_i)\Delta x_i<\delta^2\\\implies&\sum_{i\in B}\underbrace{(M_i-m_i)}_{\ge\delta}\Delta x_i<\delta^2\\\implies& \delta \sum_{i\in B}\Delta x_i\le\sum_{i\in B}(M_i-m_i)\Delta x_i<\delta^2\\\implies& \sum_{i\in B}\Delta x_i<\delta.
\end{align*}
This final inequality allows us to show the Riemann condition holds for $ \phi(f(x)) $, as required:
\begin{align*}
	U(P,\phi(f))-L(P,\phi(f))&=\sum_{i\in A}\underbrace{(M_i^*-m_i^*)}_{<\varepsilon}\Delta x_i+\sum_{i\in B}\underbrace{(M_i^*-m_i^*)}_{2K}\underbrace{\Delta x_i}_{<\delta}\\&<\frac{\varepsilon}{(b-a)+\sup|\phi(t)|}(b-a)+2K\underbrace{\delta}_{<\frac{\varepsilon}{(b-a)+2\sup|\phi(t)|}}\\&<\frac{\varepsilon}{(b-a)+2K}[(b-a)+2K]\\&=\varepsilon.
\end{align*}
\end{proof}
Along with keeping track of if we're working in $ [a,b] $ or $ [m,M] $, I think another part of this proof that is not clear is how to handle the case where $ M_i-m_i\ge\delta $. Splitting the partition of $ [a,b] $ into two parts, one corresponding to the case where $  M_i-m_i<\delta $ (indexed by $ A $), the other where $  M_i-m_i\ge\delta $ (indexed by $ B $), is a pretty clear answer, but it goes against a lot on the instincts developed when doing analysis. Many proofs require you to show some inequality holds in every possible case, so if you were trying to develop this proof on your own, when you realize $  M_i-m_i\ge\delta $ for some $ i $, it may seem like you're doing something wrong.

Proposition 6.4 allows us to prove two additional properties of integration.
\begin{proposition}
	Suppose $ f $ and $ g $ are both bounded real functions on $ [a,b] $ which are integrable. Then $ fg $ is integrable on $ [a,b] $.
\end{proposition}
\begin{proof}
	By Theorem 6.2, $ f\pm g $ is integrable. Proposition 6.4 tells us that $ \phi(f\pm g) $ and will be integrable as long as $ \phi $ is continuous. If we let $ \phi(t)=t^2 $, then \begin{align*}
		\phi(f-g)=(f-g)^2,\\\phi(f+g)=(f+g)^2,
	\end{align*} are integrable. Again by Theorem 6.2, their difference scaled should be integrable. $$fg=\frac{(f+g)^2+(f-g)^2}{4} $$ This gives that $ fg $ is integrable. 
\end{proof}
The converse of Proposition 6.5 is not true. Being able to factor an integrable function does not guarantee the factors are integrable as the next example shows.
\begin{example}
	Define $ f:\R\to\R $ as $$f(x)=\begin{cases}
	1\text{ if }x\in\Q\\-1\text{ if }x\notin\Q
	\end{cases}. $$ 
This function is not integrable on $ [0,1] $ (see Subsection 6.11), but $ f^2 $ is, as $ f^2(x)=1 $ for all $ x\in\R $. 	
\end{example}
\begin{proposition}
	Suppose $ f $ is bounded real functions on $ [a,b] $ which is integrable. Then $ |f| $ is integrable on $ [a,b] $, and $$\left\lvert\int_{a}^{b}f(x)\ dx \right\rvert\le \int_{a}^{b}|f|\ dx. $$
\end{proposition}
\begin{proof}
	Proposition 6.4 tells us that $ \phi(f) $ and will be integrable as long as $ \phi $ is continuous, so let $ \phi(t)=|t| $. This shows that $ \phi(f)=|f| $ is integrable. Chose $ c\in\{-1,1\} $ such that $$c\int_{a}^{b}f(x)\ dx\ge 0 .$$ We have $ cf\le|f| $, so by the monotonicity and linearty of integration, $$\left\lvert\int_{a}^{b}f(x)\ dx\right\rvert =c\int_{a}^{b}\ f \ dx =\int_{a}^{b}\ cf\ dx \le \int_{a}^{b}|f|\ dx. $$ 
\end{proof}
\begin{example}[Which Direction is the Inequality?]
	If you need to cram for an exam, you're not too concerned with having a deep understanding of propositions and theorems, so remembering the direction of the inequality of Proposition 6.6 may come down to memorization.\footnote{This \textit{may} be slightly autobiographical.} In this case just remember the simple examples of $ f(x)=x $ on the interval $ [-1,1] $. In this case we have \begin{align*}
		\left\lvert\int_{-1}^{1}x\ dx \right\rvert=0\le 1=\int_{-1}^{1}|x|\ dx =0.
	\end{align*}
\end{example}
\subsection{Riemann Integration, Monotonicity, and Discontinuities}
The second class of functions that are integrable are monotonic functions. The integrability on continuous functions was not a surprise, but this is. Proposition 4.2 limited the number of discontinuities of a monotonic function to a countably infinite number, but this is still an infinite number. The fact that such a function could feasibly be integrated is not something we should have expected. 
 \begin{proposition}
 	If $ f $ is monotonic on $ [a,b] $, then $ f $ is integrable. 
 \end{proposition}
 \begin{proof}
	Let $ \varepsilon>0 $. For any $ n\in\N $, define a partition of $ [a,b] $ such that $$ \Delta x_i=\frac{b-a}{n}.$$
	
	We will show the result for a monotonically increasing function. Because the function is monotonically increasing, $ M_i=f(x_i) $, and $ m_i=f(x_i) $.\footnote{If $ f $ is always increasing, then the infimum on $ [x_{i-1},x_i] $ will be achieved at  smallest value in the interval, namely $ x_i $. Similarly, the supremum on the interval will be achieved at  largest value in the interval, namely $ x_{i-1} $.} We have \begin{align*}
		U(P,f)-L(P,f)&=\sum_{i=1}^{n}(M_i-m_i)\Delta x_i\\&=\sum_{i=1}^{n}(f(x_i)-f(x_{i-1}))\frac{b-a}{n}\\&=\frac{b-a}{n}\sum_{i=1}^{n}(f(x_i)-f(x_{i-1}))
	\end{align*}
	By the Archimedean Property of $ \R $, we can find an $ n $ large enough such that $$U(P,f)-L(P,f)=\frac{b-a}{n}\sum_{i=1}^{n}(f(x_i)-f(x_{i-1}))<\varepsilon $$ for all $ \varepsilon>0 $. Because this $ n $ corresponds to a choice of a partition, we have found a partition that gives $ U(P,f)-L(P,f)<\varepsilon$, so Riemann's Criterion is satisfied. 
 \end{proof}
\begin{example}
	
\end{example}
\begin{proposition}
	Suppose $ f $ is bounded on $ [a,b] $ and has only finitely many points of discontinuity on $ [a,b] $. Then $ f $ is Riemann integrable on $ [a,b] $.
\end{proposition}

\begin{proof}
	Let $ \varepsilon>0 $. Set $ M=\sup |f(x)| $, and $ E=\{x\in[a,b]\mid f\text{ discontinuous}\} $. The set $ E $ is finite by assumption, so we can cover $ E $ by finitely many disjoint intervals $ [u_j,v_j]\subset[a,b] $.\footnote{So if $ f $ is discontinuous as $ x_0 $, we form a little interval of length $ \varepsilon $ around it such that no other point in the small interval is a point of discontinuity.} We can pick these intervals such that the sum of their lengths is $ \varepsilon $:
	$$\sum_{j=1}^{m} v_j-u_j<\frac{\varepsilon}{(b-a)+2M}. $$ 
	
	Now we will remove the open segments $ (u_j,v_j) $ from $ [a,b] $, giving a set $ K $. $$K=[a,b]\backslash\bigcup_{j=1}^m(u_j,v_j) $$ Because we removed open segments, $ K $ is still closed, so it is compact.\footnote{Removing segments never put the boundedness of $ [a,b] $ in jeopardy, so clearly $ K $ is bounded.} On the compact set $ K $, $ f $ is continuous,\footnote{Remember, we just removed all the point pf discontinuity by removing $ (u_j,v_j) $.} giving uniform continuity. There exists a $ \delta>0 $ such that $$|f(s)=f(t)|<\frac{\varepsilon}{(b-a)+2M} $$ for any $ s,t\in K $ satisfying $ |s-t|<\delta $.
	
	Now construct a partition $ P=\{x_0,\ldots,x_n\} $ of $ [a,b] $ such that: $ u_j\in P $ for all $ j $, $ v_j\in P $ for all $ j $, $ (u_j,v_j)\cap P=\emptyset $. We will have $ \Delta x_i<\delta $, as $ x_i,x_{i-1}\in K $...{\color{red}FINISH}
	
	
\end{proof}
\begin{remark}[Integrability is Weaker than Continuity]
Integrability is often seen as equivalent to continuity in calculus courses, but this section goes to show this is not at all correct. Any monotonic function is integrable, even though it may have countably infinite discontinuities. Any bounded function with finite points of discontinuity is bounded. These facts make integrability a far weaker condition than continuity. 
\end{remark}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
\draw (-.3,7) node {$ \R$};
\draw (16,-0.3) node {$ \R$};		
\draw[thick,->] (0,0) -- (0,7);
\draw[thick,->] (0,0) -- (16,0);
\draw[thick] (1,-0.2) -- (1,0.2);
\draw[thick] (14,-0.2) -- (14,0.2);
\draw[blue] (14,5.5) node {$ f(x) $};	
\draw[black] (1,-.5) node {$ a $};
\draw[black] (14,-.5) node {$ b $};	
\fill[fill=gray!15] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5) -- (14,0) -- (1,0) -- (1,1);
\draw[thick, blue] (1,1)  to[out=45,in=180]  (7,6) to[out=0,in=180] (10.5,1) to[out=0,in=230] (14,5);
\draw[white, thick] (2,0)--(2,2.16);	
\draw[white, thick] (3,0)--(3,3.5);
\draw[white, thick] (4,0)--(4,4.65);
\draw[white, thick] (5,0)--(5,5.45);
\draw[white, thick] (6,0)--(6,5.88);
\draw[white, thick] (7,0)--(7,6);
\draw[white, thick] (8,0)--(8,5.56);
\draw[white, thick] (9,0)--(9,2.45);
\draw[white, thick] (10,0)--(10,1.06);
\draw[white, thick] (11,0)--(11,1.05);
\draw[white, thick] (12,0)--(12,1.64);
\draw[white, thick] (13,0)--(13,3.3);
\filldraw[color=blue, fill=white]  circle (2pt);
\filldraw[color=blue, fill=white] (2,2.16)circle (2pt);
\filldraw[color=blue, fill=white]  (3,3.5)circle (2pt);
\filldraw[color=blue, fill=white] (4,4.65)circle (2pt);
\filldraw[color=blue, fill=white]  (5,5.45)circle (2pt);
\filldraw[color=blue, fill=white] (6,5.88)circle (2pt);
\filldraw[color=blue, fill=white]  (7,6)circle (2pt);
\filldraw[color=blue, fill=white] (8,5.56)circle (2pt);
\filldraw[color=blue, fill=white] (9,2.45)circle (2pt);
\filldraw[color=blue, fill=white]  (10,1.06)circle (2pt);
\filldraw[color=blue, fill=white]  (11,1.05)circle (2pt);
\filldraw[color=blue, fill=white]  (12,1.64)circle (2pt);
\filldraw[color=blue, fill=white]  (13,3.3)circle (2pt);
\end{tikzpicture}	
\caption{Despite having 12 points of discontinuity on $ [a,b] $, $ f $ is still integrable.}
\end{figure}
\begin{example}[Simple Functions]
	Any simple function (Definition 6.6) is Riemann integrable, despiting them often being discontinuous. Example 6.3 and Figure 61 show an example of this type of discontinuous, yet integrable, function.
\end{example}
\subsection{The Riemann-Stieltjes Integral (Optional)}
\textbf{Disclaimer}: Again, we need to be a bit informal here. The purpose of this section is to build intuition for Sections 12-14, so I'm not as worried about using proper verbiage. Again, I'll be assuming that the ``actual length'', or simply ``length'', of an interval $  [x_{i-1},x_i] $ is $ |x_i-x_{i-1}| $.

If you think back to the definition of the upper and lower Riemann sums, we always treated the width of the rectangle at $ [x_{i-1},x_i]\subset P $ as $ |x_{i-1}-x_i| $. Is there anything stopping us from defining the width of the corresponding rectangle as $ |2x_{i-1}-2x_i| $? Or what about $ |x_{i-1}^2-x_i^2| $?  It turns out, we can still perform Riemann integration if we decide to assign different widths to the rectangles of a Riemann sum. 

Let $ \alpha:[a,b]\to\R $ be any monotonic function. We can redefine the upper and lower Riemann sums as 
\begin{align*}
U(P,f,\alpha)&=\sum_{i=1}^{n}M_i\Delta \alpha(x_i),\\
L(P,f,\alpha)&=\sum_{i=1}^{n}m_i\Delta \alpha(x_i),
\end{align*} 
where $\Delta\alpha(x_i)=|\alpha(x_i)-\alpha(x_{i-1})| $. The function $ \alpha $ is determining how we assign length to different intervals in $ P $. We can think of it as a \textit{weighted} version of the original upper and lower sums, where $ \alpha $ determine the weight assigned to different parts of $ [a,b] $, and each interval is assigned an ``artificial length'' of $ |\alpha(x_i)-\alpha(x_{i-1})| $. By ``weight'', I'm talking about how much ``artificial'' length we assign to an interval relative to it's ``actual'' length. It is meant in the same sense as a ``weighted'' average, where you take some observations to be more significant, and assign them more weight when calculating the mean. This clearly merits several examples.
\begin{example}[$ \alpha(x)=x $]
	Let $ \alpha(x)=x $. In this case we have $$ \alpha( \Delta x_i)=|\alpha(x_i)-\alpha(x_{i-1})|=|x_i-x_{i-1}|,$$ so each interval is assigned it's ``true'' length (Figure 66).
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}	
		\draw[fill=gray!15, thin, opacity=0.6] (2.5,0) rectangle (4.5,2.44);
		\draw[thick,->] (0,0) -- (0,7);
		\draw[thick,->] (0,0) -- (7,0);
	\draw[thick,->] (8,0) -- (15,0);
	\draw[thick,->] (8,0) -- (8,7);
	\draw[thick] (0.5,.2)--(0.5,-.2);
	\draw[thick] (6.5,.2)--(6.5,-.2);
		\draw[black] (0.5,-.5) node {$ a $};
	\draw[black] (6.5,-.5) node {$ b $};	
	\draw[thick] (8.5,.2)--(8.5,-.2);
	\draw[thick] (14.5,.2)--(14.5,-.2);
	\draw[black] (8.5,-.5) node {$ a $};
	\draw[black] (14.5,-.5) node {$ b $};
	\draw[thick, blue] (1,5) to[out=45,in=180] (3,3) to[out=0,in=180] (7,1);
	\draw[thick, blue] (8.5,1) -- (14.5,7);
	\draw[black] (10.5,-.3) node {$ x_{i-1} $};
	\draw[black] (12.5,-.3) node {$ x_{i} $};	
	\draw[blue] (12.5,6) node {$ \alpha(x)=x $};	
	\draw[blue] (3,3.5) node {$ f(x) $};
	\draw[thick, dashed] (12.5,0)--(12.5, 5)-- (8,5);
	\draw[thick, dashed] (10.5,0)--(10.5, 3)-- (8,3);
		\draw[thick, dashed] (0,2.44)--(4.5, 2.44);
		\draw[black] (-0.3,2.44) node {$ m_i $};
	\draw[black] (7.3,5) node {$ \alpha(x_{i-1}) $};
	\draw[black] (7.5,3) node {$ \alpha(x_{i}) $};
	\draw[red!30, ultra thick] (8,5) -- (8,3);
	\draw[black] (2.5,-.3) node {$ x_{i-1} $};
	\draw[black] (4.5,-.3) node {$ x_{i} $};
	\draw[red!30, ultra thick] (2.5,0) -- (4.5,0);
	\draw[red] (3.5, 0.3) node {\tiny $ |\alpha(x_i)-\alpha(x_{i-1})|  $};
	\draw[red] (7, 4) node {\tiny $ |\alpha(x_i)-\alpha(x_{i-1})|  $};
		\end{tikzpicture}
		\caption{To determine how we weight the interval  $ [x_{i-1},x_i] $, we calculate $ |\alpha(x_i)-\alpha(x_{i-1})|  $, and then assign it to the interval.}
	\end{figure}
This is a special case for two reasons. Firstly, each interval is weighted equally.\footnote{Meaning that $ |\alpha(x_i)-\alpha(x_{i-1})|\propto|x_i-x_{i-1}| $ for all $ i $.} Secondly, each interval is weighted such that $ |\alpha(x_i)-\alpha(x_{i-1})|=|x_i-x_{i-1}| $, so we get back the original Riemann sums. The weighted area assigned to the gray rectangle is $$m_i\Delta \alpha(x_i)=m_i \Delta x_i .$$
\end{example}
\begin{example}[$ \alpha(x)=2x $]
	Let $ \alpha(x)=2x $. In this case we have $$ \alpha( \Delta x_i)=|\alpha(x_i)-\alpha(x_{i-1})|=|2x_i-2x_{i-1}|,$$ so each interval is assigned twice its ``actual'' length (Figure 67). 
		\begin{figure}[h!]
		\centering
		\begin{tikzpicture}	
		\draw[fill=gray!15, thin, opacity=0.6] (2.5,0) rectangle (4.5,2.44);
		\draw[thick,->] (0,0) -- (0,7);
		\draw[thick,->] (0,0) -- (7,0);
		\draw[thick,->] (8,0) -- (15,0);
		\draw[thick,->] (8,0) -- (8,7);
		\draw[thick] (0.5,.2)--(0.5,-.2);
		\draw[thick] (6.5,.2)--(6.5,-.2);
		\draw[black] (0.5,-.5) node {$ a $};
		\draw[black] (6.5,-.5) node {$ b $};	
		\draw[thick] (8.5,.2)--(8.5,-.2);
		\draw[thick] (14.5,.2)--(14.5,-.2);
		\draw[black] (8.5,-.5) node {$ a $};
		\draw[black] (14.5,-.5) node {$ b $};
		\draw[thick, blue] (8.5,1) -- (14.5,7);
		\draw[thick, blue] (1,5) to[out=45,in=180] (3,3) to[out=0,in=180] (7,1);
		\draw[black] (10.5,-.3) node {$ x_{i-1} $};
		\draw[black] (12.5,-.3) node {$ x_{i} $};	
		\draw[blue] (12.5,6) node {$ \alpha(x)=2x $};
				\draw[thick, dashed] (0,2.44)--(4.5, 2.44);
		\draw[black] (-0.3,2.44) node {$ m_i $};	
		\draw[blue] (3,3.5) node {$ f(x) $};
		\draw[thick, dashed] (12.5,0)--(12.5, 5)-- (8,5);
		\draw[thick, dashed] (10.5,0)--(10.5, 3)-- (8,3);
		\draw[black] (7.3,5) node {$ \alpha(x_{i-1}) $};
		\draw[black] (7.5,3) node {$ \alpha(x_{i}) $};
		\draw[red, ultra thick] (8,5) -- (8,3);
		\draw[black] (2.5,-.3) node {$ x_{i-1} $};
		\draw[black] (4.5,-.3) node {$ x_{i} $};
		\draw[red, ultra thick] (2.5,0) -- (4.5,0);
		\draw[red] (3.5, 0.3) node {\tiny $ |2x_i-2x_{i-1}|  $};
		\draw[red] (7, 4) node {\tiny $ |\alpha(x_i)-\alpha(x_{i-1})|  $};
		
		\end{tikzpicture}
		\caption{We now assign more weight to $ [x_{i-1},x_i] $  than we did with $ \alpha(x)=x $.}
	\end{figure}
Now we have$$ m_i\Delta \alpha(x_i)=2m_i \Delta x_i.$$ This is still a somewhat special case though, because we are assigning ``artificial'' lengths that are proportional to the ``actual'' length of an interval. For example, this choice of $ \alpha $ gives $ |\alpha(1)-\alpha(0)|=2 $ and $ |\alpha(2)-\alpha(0)|=4 $. In both cases, the ``artificial'' length is double the ``actual'' length. In fact, if $ \alpha(x)$ is linear, then each interval is weighted equally. For $ \alpha(x)=\beta x+\gamma $,
\begin{align*}
	\Delta \alpha( x_i)=|(\beta\Delta x_{i-1}+\gamma)|-|(\beta\Delta x_i+\gamma)|=\beta|x_{i-1},x_i|=\beta \Delta x_i\propto \Delta x_i,
\end{align*} 
so all we're doing is scaling the ``actual'' length. 
\end{example}
\begin{example}{$ \alpha(x)=x^2 $}
 Now let's see what happens for a nonlinear choice of $ \alpha(x) $. Suppose we have two intervals in a partition, $ [0,0.1] $ and $ [1,10] $. With this choice of $ \alpha $ we have \begin{align*}
 	|\alpha(0.1)-\alpha(0)|=0.01,\\
 	|\alpha(10)-\alpha(1)|=81.
 \end{align*}
 This value of $ \alpha $ assigns a lot of weight to large intervals, and a small amount of weight to small intervals. This is the first example we've seen where $ \alpha $ truly assigns different weights to different intervals, and not just scaling up every length by a constant. 
\end{example}
 
Using $ \alpha $, we can redefine the Riemann integral as developed in Subsection 6.2.
$$\lowint_a^b f(x)\ d\alpha=\sup\limits_{P\in\textbf{P}([a,b])} L(P,f,\alpha), \ \ \ \ \upint_a^b f(x)\ d\alpha =\inf\limits_{P\in\textbf{P}([a,b])} U(P,f,\alpha). $$

\begin{definition}
	Suppose $ f $ is a bounded real function  on the interval $ [a,b] $ and $ \alpha $ is a monotonic function on $ [a,b] $. If $$ \lowint_a^b f(x)\ d\alpha=\upint_a^b f(x)\ d\alpha,$$ then we say $ f $ is \textit{\color{red}Riemann–Stieltjes integrable (on $ [a,b] $)} and we write the common value of the upper and lower Riemann integral as $$ \int_{a}^{b}f(x)\ d\alpha.$$ We refer to this common value as the \textit{\color{red}Riemann–Stieltjes integral of $ f $ on [a,b]}. 
\end{definition}
The Riemann–Stieltjes (RS) is simply a weighted integral. Nearly all the results developed up until now can be established for the RS integral. In fact many standard texts such as \cite{rudin1964principles} opt to work exclusively with the RS integral for this reason. 
\begin{remark}[Why Monotonic?]
	If $ \alpha $ is not monotonic, then it could assign negative ``artificial length'' to an interval $ [x_{i-1},x_{i}] $ (which requires $ x_i >x_{i-1}$). 
\end{remark}
\begin{example}[Probability Distributions]
	A random variable $ X $ has a cumulative distribution function (CDF) $ F:[a,b]\to[0,1] $ such that $$ \Prr{c\le X\le d}=F(d)-F(c).$$ The CDF is assigning a probability to $ [c,d] $. Depending on our choice of $ F $, the weight assigned to each interval in probability will differ. If $ F $ is a uniform distribution, every interval is assigned equal probability, i.e equal weight. But this sounds \textit{just like} RS integration! If we let $ \alpha(x)=F(x) $, then we have $$\int_{c}^{d} dF=\int_{c}^{d}1\ dF=1(F(d)-F(c))=\Prr{c\le X\le d}. $$ 
	
	Now suppose that the random variable $ X $ only takes on values $ x\in[a,b] $. If we want to find the expected value of $ X $ over, then we calculate a the RS integral of $ x $ over all of $ [a,b] $. $$ E[x]=\int_{a}^{b}x\ dF$$ This is akin to a weighted average, because we are averaging every single possible realization of $ X $, but we first assign weight to the interval $ [a,b] $ in accordance with the probability of each event occurring. 
\end{example}
\begin{example}[Inertia]
	Suppose there is a rod of unit length where the mass contained in $ [0,x] $ is given by $ m(x) $. The inertia $ I $ is given as $$ I=\int_{0}^{1}x^2\ dm.$$ 
\end{example}
If you have taken a probability of physics course, these last two examples may look somewhat familiar. Using integration to calculate the expected value of a random variable, or the moment of inertia are standard applications, but it is normally introduced as a Riemann integral of a different quantity, not a RS integral. What is the connection between the two? Is it possible to calculate a RS integral as a Riemann integral? The next theorem allows us to do just this.
\begin{theorem}[RS Integral as Riemann Integral]
Assume $ \alpha $ increase monotonically and $ \alpha' $ is integrable on $ [a,b] $. Let $ f $ be a bounded real function on $ [a,b] $. The function $ f $ is RS integrable on $ [a,b] $ \textit{if and only if} $ f\alpha' $ is Riemann integrable on $ [a,b] $. In this case we have 
$$ \int_{a}^{b}f(x)\ d\alpha=\int_{a}^{b}f(x)\alpha'(x)\ dx .$$
\end{theorem} 
Before we prove this, we should rationalize why this equation holds. We want to integrate $ f(x) $ as if the interval $ [a,b] $ is not weighted. To do this we need to convert between $ \alpha(x) $ and $ x $ somehow. For the RS sums, $ \alpha(x) $ is manipulating the width of a rectangle, thereby manipulating its area. But couldn't we instead manipulate the height of the rectangle to achieve the same change in area? If we were to do this, we would need to multiply the height of the rectangle by the same factor that we change the width by. But what quantity captures the change in width? That would be $ \alpha' $. What determines the height of the rectangle? That is $ f $ (vicariously through its supremum or infimum). So if we let the height of the rectangle be $ f(x)\alpha'(x) $, we acheive the same area that resulted from reweighting the width using $ \alpha $.

Let's accompany this argument with a quick example. Using a constant function would be best, as the integral in this case just amounts to a rectangle. Let $ f(x)=10 $ on $ [5,10] $, and $ \alpha(x)=2x $. If we calculate the area using a RS integral, we have $$ \int_{5}^{10}f(x)\ d\alpha=\int_{5}^{10}10\ d\alpha=10[2(10)-2(5)]=100.$$ Alternatively, we could acheive the same area under $ f(x) $ by multiply $ f(x) $ by $ 2 $, which just happens to be $ \alpha' $ (which we are about to prove is no coincidence).
$$ \int_{5}^{10}f(x)a'(x)\ d\alpha=\int_{5}^{10}10\cdot 2\ dx=20(10-5)=100.$$
We get the same exact value. The actual proof, which is on the harder side, is not nearly as important as understanding the intuition behind the result. 
\begin{proof}
The function $ \alpha $ is integrable, so it satisfies the Riemann Criterion. That is for $ \varepsilon>0 $ there exists a partition $ P=\{x_0,\ldots,x_n\} $ of $ [a,b] $ such that $$ U(P,\alpha')-L(P,\alpha')<\frac{\varepsilon}{\sup |f(x)|}.$$ 
By the differentiability of $ \alpha' $, we can apply the Mean Value Theorem on each interval $ [x_{i-1},x_i] $. There exists a $ t_i\in[x_{i-1},x_i] $ such that
\begin{align}
	\alpha_i(x_{i-1})-\alpha_i(x_i)&=\alpha'(t_i)(x_{i-1}-x_i),\nonumber\\\Delta\alpha_i&=\alpha'(t_i)\Delta x_i,
\end{align}
for $ i=i,\ldots,n $. By Part 2 of Proposition 6.1 and the integrability of $ \alpha' $, for $ s_i\in [x_{i-1},x_i]$,\footnote{Part 2 of Proposition 6.1 requires two points in$ s_i,t_i\in [x_{i-1},x_i]$. We will use the $ t_i $ that we found using the Mean Value Theorem.}
\begin{align}
\sum_{i=1}^n|\alpha'(s_i)-\alpha'(t_i)|\Delta x_i<\frac{\varepsilon}{|\sup f(x)|}.
\end{align}
By (15), \begin{align}
	\sum_{i=1}^nf(s_i)\Delta \alpha_i=\sum_{i=1}^nf(s_i)\alpha'(t_i)\Delta x_i.
\end{align}
If we set $ M=\sup|f(x)| $, then (17) and (16) give $$ \left\lvert\underbrace{\sum_{i=1}^nf(s_i)\Delta \alpha_i}_{U(P,f,\alpha)}-\underbrace{\sum_{i=1}^nf(s_i)\alpha'(t_i)}_{U(P,f\alpha')}\Delta x_i \right\rvert\le M\varepsilon.$$ This will hold for all $ s_i\in[x_{i-1},x_i] $, so $$|U(P,f,\alpha)-U(P,f\alpha')|< M\frac{\varepsilon}{|f(x)|}=\varepsilon. $$ This all will hold for any refinement of $ P $, so we can conclude $$\left\lvert \upint_{a}^bf(x)\ d\alpha-\upint_{a}^bf(x)\alpha'(x)\ dx \right\rvert\le\varepsilon .$$ This holds for all $ \varepsilon>0 $, so $$  \upint_{a}^bf(x)\ d\alpha=\upint_{a}^bf(x)\alpha'(x)\ dx.$$ This \textit{exact} reasoning will give $$\lowint_{a}^bf(x)\ d\alpha=\lowint_{a}^bf(x)\alpha'(x)\ dx,$$ so we can conclude $$ \int_{a}^bf(x)\ d\alpha=\int_{a}^bf(x)\alpha'(x)\ dx.$$
\end{proof}   
\begin{example}[Probability Distributions]
 A random variable $ X $ with CDF $ F $, has a probability density function (PDF) defined as $ F'=f $. If we apply Theorem 6.4 to Example 6.16 we get
 \begin{align*}
 	\Prr{c\le X\le d}&=\int_{c}^{d} dF=\int_{c}^{d}F'(x)\ dx=\int_{c}^{d}f(x)\ dx,\\
 	E[x]&=\int_{a}^{b} x\ dF=\int_{a}^{b}xF'(x)\ dx=\int_{a}^{b}xf(x)\ dx.
 \end{align*}
\end{example}
This is the last time we'll see the RS integral, but the ideas which motivate it will return. 
\subsection{The Fundamental Theorem Of Calculus and Consequences}
Wait...how do we actually calculate these things? So far we've only been able to integrate constant functions (Example 6.1).\footnote{If we think of the integral in terms of simple functions, we can integrate those too, but only because we  defined the integral explicitly in that case.} This section will present \textit{the} theorem of calculus. The Fundamental Theorem of Calculus allows us to easily calculate integrals by the means of derivatives. But is this what we really care about? From the standpoint of analysis, who cares about calculating the area under a curve? Mathematicians have been able to do this quite accurately for millennial. What is \textit{amazing} is that the area under a curve is somehow related to differentiation at all. Despite being able to calculate areas and rates for centuries, mathematicians thought these two tasks were totally unrelated. This is to say, \textit{everyone} takes this theorem for granted. When introduced to integrals in calculus, most are taught that integration is the opposite of differentiation only minutes after the integral is defined, but this is not a very obvious of a relationship.\footnote{You're probably thinking ``yes it is'', but keep in mind that you have known this for years, and have most likely seen several interpretations of the relationship.} Math students are so conditioned to think of integration hand-in-hand with differentiation, that it makes it hard to appreciate the Fundamental Theorem from the standpoint of analysis. \textbf{So do not think of this section as a means to calculation, think of it as a beautiful insight into two seemingly disparate topics}. Pontification over. 

For such an important pair of results, The Fundamental Theorem of Calculus is not overwhelmingly difficult to prove. The second part is just an application of the Mean Value Theorem,\footnote{This is one reason that the Mean Value Theorem is so important.}

\begin{definition}
	Let $ F $ be a real differentiable function. If $ F'=f $, then we say $ F $ is the {\color{red}antiderivative} of $ f $. 
\end{definition}
\begin{theorem}[Fundamental Theorem of Calculus I]
Let $ f $ be integrable on $ [a,b] $. For $ a\le x\le b $, let $$ F(x)=\int_{a}^{x}f(t)\ dt .$$ Then $ F $ is continuous on $ [a,b] $; furthermore, if $ f $ is continuous at a point $ x_0\in[a,b] $, then $ F $ is differentiable, and $F'(x_0)=f(x_0)$. 
\end{theorem}
\begin{proof}
	We will first show that $ F $ is continuous Because $ f $ is integrable, it is bounded, so there exists an $ M $ for which $ |f(t)|\le M $ on $ [a,b] $. If $ a\le x<y\le b $, Theorem 6.2 Parts 3 and 5 give\begin{align}
|F(y)-F(x)|=\left\lvert\int_{x}^{y}f(t)\ dt \right\rvert\le M(y-x).
	\end{align}
	 If we have $|y-x|<\varepsilon/M$, then $$ |F(y)-F(x)|<\varepsilon,$$ so $ F $ is continuous.\footnote{It actually ended up being uniformly continuous, so happy day.} 
	
	Now suppose $ f $ is continuous at $ x_0 $. For $ \varepsilon>0 $, there exists a $ \delta>0 $ such that $$|f(t)-f(x_0)|<\varepsilon $$ for $ t\in[a,b] $ which satisfy $ |t-x_0|<\delta $. Therefore if we have $$ x_0-\delta<s\le x_0\le t<x_0+\delta,$$ and $ a\le s<t\le b $, by Theorem 6.2 Part 5 we have \begin{align*}
	\left\lvert\int_{s}^{t}\underbrace{f(u)-f(x_0)}_{<\varepsilon}\ du\right\rvert&<\varepsilon(t-s)
	\end{align*}
	Not only does this inequality take the form of (18),\footnote{Let $ M=\varepsilon $, $ y=t $, $ x=s $, and $ f(t)=f(u) $.} but we satisfy the conditions under which (18) holds, so we have \begin{align*}
		|F(t)-F(s)-f(x_0)|&=\left\lvert\int_{s}^{t}f(u)\ du-f(x_0)\right\rvert<\varepsilon(t-s)\\	\left\lvert\frac{F(t)-F(s)}{s-t}-f(x_0)\right\rvert&=\left\lvert\frac{1}{s-t}\int_{s}^{t}f(u)-f(x_0)\ du\right\rvert<\varepsilon\\\left\lvert\frac{F(t)-F(s)}{s-t}-f(x_0)\right\rvert&<\varepsilon.
	\end{align*}
	That is, for all $ \varepsilon>0 $ this final inequality holds for any $ s\in(x_0-\delta,x_0+\delta)$, so if we take $ t=x_0 $ this becomes the definition of the following limit: $$F'(x_0)=\lim\limits_{s\to x}\frac{F(t)-F(x_0)}{s-x_0}=f(x_0) .$$
\end{proof}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\fill[fill=red!10, opacity=0.8] (0,0) -- (7,3) -- (7,0) -- (0,0) ;
	\draw (7,-0.5) node {$ x $};
	\draw[thick] (7,0.2)--(7,-0.2);
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.4) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick, red] (0,0)  to[out=5,in=190]  (9,1) to[out=10,in=260] (14,7);
	\draw[thick, blue] (0,0) -- (14,6);
	\draw[blue] (8,4) node {$ f $};
	\draw[red] (12.7,4) node {$ F $};
	\draw[red] (5,1) node {\scriptsize $ F(x)=\displaystyle\int_{0}^{x}f(t)\ dt $};
	\end{tikzpicture}
	\caption{The first part of The Fundamental Theorem of Calculus says that $ F'=f $.}
\end{figure}

\begin{theorem}[Fundamental Theorem of Calculus II]
	Let $ f $ be integrable on $ [a,b] $. If there exists a differentiable function $ F $ on $ [a,b] $ such that $ F'=f $, then $$ \int_{a}^{b}f(x)\ dx=F(b)-F(a).$$
\end{theorem}
\begin{proof}
Let $ \varepsilon>0 $. By Riemann's Criterion, there exists some $ P+\{x_0,\ldots,x_n\} $ of $ [a,b] $ such that $ U(P,f)-L(P,f)<\varepsilon $. By the Mean Value Theorem, for each $ i $ we have a $ t_i\in[x_{t-1},x_t] $ such that \begin{align*}
	F(x_i)-F(x_{i-1})&=F'(t_i)(x_i-x_{i-1}),\\
	F(x_i)-F(x_{i-1})&=f(t_i)\Delta x_i.
\end{align*}
Taking the sum over all $ i $ gives $$\sum_{i=1}^{n}f(t_i)\Delta x_i=F(b)-F(a) .$$ By Proposition 6.1 Part 3, $$\left\lvert F(b)-F(a)-\int_{a}^{b}f(x)\ dx\right\rvert=\left\lvert\sum_{i=1}^{n}f(t_i)-\int_{a}^{b}f(x)\ dx\right\rvert<\varepsilon. $$ 
This holds for all $ \varepsilon $, so $$\int_{a}^{b}f(x)\ dx=F(b)-F(a). $$
\end{proof}
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\fill[fill=red!10, opacity=0.8] (5,0) -- (5,15/7) -- (13,39/7) -- (13,0) -- (5,0) ;
	\draw[thick] (5,0.2)--(5,-0.2);
	\draw[thick] (13,0.2)--(13,-0.2);
	\draw (-.3,7) node {$ \R$};
	\draw (5,-0.5) node {$ a$};
	\draw (13,-0.5) node {$ b$};
	\draw (16,-0.4) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick,dashed] (5,0.45) -- (0,0.45);
	\draw[thick,dashed] (13,4) -- (0,4);
	\draw[thick, blue] (0,0)  to[out=5,in=190]  (9,1) to[out=10,in=260] (14,7);
	\draw[thick, blue] (0,0) -- (14,6);
	\draw[blue] (9,4.7) node {$ f(x) $};
	\draw[blue] (14.5,4.5) node {$ F'(x)=f(x) $};
	\draw[red] (7,1.6) node {\scriptsize $ \displaystyle\int_{a}^{b}f(x)\ dx $};
	\draw[red, ultra thick] (0,0.45) -- (0,4);
	\draw[black, thick] (-0.2,4)--(0.2,4);
	\draw[black, thick] (-0.2,.45)--(0.2,.45);
		\draw (-.6,4) node {$ F(b)$};
		\draw (-.6,.45) node {$ F(a)$};
		\draw[red] (-.8,2.225) node {\scriptsize $ F(b)-F(a)$};
	\end{tikzpicture}
	\caption{The second part of The Fundamental Theorem of Calculus says that if $ F'=f $, then we have $ \int_{a}^{b}f(x)\ dt=F(b)-F(a) $.}
\end{figure}
In essence, the Fundamental Theorem of Calculus (FTC) tells us that integration and differentiation are inverse operations.

A great application of the FTC is to find some integration technique that is analogous to the Product Rule of differentiation. 
\begin{proposition}[Integration by Parts]
Suppose $ F $ and $ G $ are differentiable functions on $ [a,b] $, and $ F'=f $ and $ G'=g $ are integrable as well. Then $$ \int_{a}^{b}F(x)g(x)\ dx=F(b)G(b)-F(a)G(a)-\int_{a}^{b}f(x)G(x)\ dx.$$
\end{proposition}
\begin{proof}
	Let $ H(x)=F(x)G(x) $. We have $$H'(x)=F'(x)G(x)+G'(x)F(x)=f(x)G(x)+g(x)F(x) ,$$ so $ H' $ is integrable by Proposition 6.5. The FTC gives \begin{align*}
		\int_{a}^{b}H'(x)\ dx&=H(b)-H(a)\\
		\int_{a}^{b}f(x)G(x)+g(x)F(x)\ dx&=F(b)G(b)-F(a)G(a)\\
		\int_{a}^{b}f(x)G(x)\ dx+\int_{a}^{b}g(x)F(x)\ dx&=F(b)G(b)-F(a)G(a)\\\int_{a}^{b}F(x)g(x)\ dx&=F(b)G(b)-F(a)G(a)-\int_{a}^{b}f(x)G(x)\ dx
	\end{align*}
	\begin{remark}[But What About Discontinuous Functions?]
		Subsection 6.7 showed that the set of Riemann integrable functions includes many more functions than just those are which continuous. Unfortunately, the FTC requires continuity. We still have no clear way of treating discontinuities when integrating. Example 6.20 and Example 6.23 will discuss this more.
	\end{remark}
\end{proof}
\subsection{Change of Variables}
Change of variables may be something you first saw explicitly in multivariable calculus when working with polar or spherical coordinates, but you actually learned a basic form of it earlier. In the case functions of a single variable it is often called $ u $-substitution. For the sake of generalization later on, we'll refer to it as change of variables, but just keep in mind that all it is, is $u$-substitution. 

The general idea of change of variables is that it is the opposite of the Chain Rule. 

$$ \int_{\phi(a)}^{\phi(b)}f(x)\ dx=\int_{a}^{b}f(\phi(y))\phi'(y)\ dy$$
We can justify this equality with reasoning similar to that which accompanied Theorem 6.6. This is best done with an example, as we need to deal with two sets of Riemann sums. 

Let $ f(x)=5 $, $ \phi(y)=2y $,and $ [a,b]=[0,10] $. We have 
\begin{align}
\int_{\phi(a)}^{\phi(b)}f(x)\ dx=\int_{0}^{20}5\ dx=100,\\\int_{a}^{b}\phi(y)\ dy=\int_{0}^{10}2y \ dy=25.
\end{align}
Equation (19) corresponds to a square of area 100, while Equation (20) corresponds to a square of area 25. The goal of change of variables is to manipulate the area of the triangle formed by $ \phi $ so it has an area equal to the square formed by $ f $. This allows us to integrate $ f $ without even considering its domain. We instead work in $ \phi $'s domain. First, we want to change the height of the Riemann sums which form the triangle so they correspond to the height of the sums corresponding to $ f $ and the rectangle (see Figure 68). This is achieved by composing $ \phi $ with $ f $. Our updated area is the integral of $ f(\phi(y))=5 $ over  $ [0,10] $. 
\begin{align}
 \int_{0}^{10}5\ dx=50.
\end{align}
		\begin{figure}[h!]
	\centering
	\begin{tikzpicture}	
	\fill[fill=blue!10] (8,0) rectangle (13,2) ;
	\fill[fill=blue!10] (0,0) -- (3.5,3.5) -- (3.5,0) -- (0,0) ;
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (7,0);
	\draw[thick,->] (8,0) -- (15,0);
	\draw[thick,->] (8,0) -- (8,7);
	
	\draw[thick] (3.5,.2)--(3.5,-.2);
	\draw[thick] (-0.2,3.5)--(.2,3.5);
	\draw[blue] (2,1) node {25};
	\draw[black] (3.5,-.5) node {10};
	\draw[black] (-.5,3.5) node {20};
	\draw[black] (-.5,7) node {$ \phi(y) $};
	\draw[black] (7,-.3) node {$ y $};
	\draw[black] (7.5,7) node {$ f(x) $};
	\draw[black] (15,-.3) node {$ x $};
	\draw[blue] (10.5,1) node {50};
		\draw[black] (13,-.5) node {10};
	\draw[black] (7.7,2) node {5};
	\draw[blue] (6.9,4.5) node {$ f\circ \phi $};
		\draw[thick] (13,-.2)--(13,-.2);
	\draw[thick] (7.8,2)--(8.2,2);
	\draw[thick, ->, blue] (6.4,4) to[out=30,in=150] (7.4,4);	
	\end{tikzpicture}
	\caption{By composing $ \phi $ with $ f $, we modify the height of the integral of $ \phi $.}
\end{figure}
But we aren't done yet. The difference between (21) and (19) is the width of the corresponding rectangles. We cannot change the width of the prior though, because the whole point of this is to integrate in the domain of $ \varphi $. The bounds of integration should not be touched. We can instead scale the height of this rectangle by $ \varphi'(y) $, which is the rate at which the width would be scaled if we decided to move to the domain of $ f $ (Figure 68). 
		\begin{figure}[h!]
	\centering
	\begin{tikzpicture}	
	\fill[fill=blue!10] (0,0) rectangle (5,2) ;
	\fill[fill=blue!10] (8,0) rectangle (13,4) ;
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (7,0);
	\draw[thick,->] (8,0) -- (15,0);
	\draw[thick,->] (8,0) -- (8,7);
	
	\draw[thick] (5,.2)--(5,-.2);
	\draw[thick] (-0.2,2)--(.2,2);
	\draw[blue] (2,1) node {50};
	\draw[black] (5,-.5) node {10};
	\draw[black] (-.5,2) node {5};
	\draw[black] (-.5,7) node {$ \phi(y) $};
	\draw[black] (7,-.3) node {$ y $};
	\draw[black] (7.5,7) node {$ f(x) $};
	\draw[black] (15,-.3) node {$ x $};
	\draw[blue] (10.5,1) node {100};
	\draw[black] (13,-.5) node {10};
	\draw[black] (7.7,4) node {10};
	\draw[blue] (6.9,4.5) node {$ \times\phi'(y) $};
	\draw[thick] (13,-.2)--(13,-.2);
	\draw[thick] (7.8,4)--(8.2,4);
	\draw[thick, ->, blue] (6.4,4) to[out=30,in=150] (7.4,4);	
	\end{tikzpicture}
	\caption{By scaling the height of the square from Figure 67 by a factor of $ \phi' $, we get the desired area.}
\end{figure}
This gives us $$ \int_{0}^{10}5\phi'(y)\ dy=\int_{0}^{10}5\cdot 2\ dy=100.$$ Therefore we have scaled the area in (16) to that in (15) by geometrically transforming the function over which we are integrating. In one step this is: $$\int_{\phi(a)}^{\phi(b)}f(x)\ dx=\int_{0}^{20}5\ dx=100=\int_{0}^{10} 5\times 2\ dy=\int_{a}^{b}f(\phi(y))\phi'(y)\ dy $$

To convince yourself this works, it may be helpful to draw the pictures for the reverse direction. That is, how would we manipulate the square of area 100, to end up with a triangle of area 25. This particular direction is actually the useful direction, as you start with the simpler integral! It is also the direction you proceed in when performing $ u $-substitution. 
\begin{remark}[Why Did I Go to All This Trouble?]
	This very simple example may seem a little redundant, but change of variables becomes very complicated when working with mulitvariable functions. It will be really important to have a strong intution about this process when we move to the general case. It also helps in this simplified case because change of variables doesn't have this nice geometric representation when working with nontrivial functions. Even picking $ f $ to be linear would have prevented us from using well known formulas for the area of shapes. 
\end{remark}
\begin{theorem}[Change of Variables]
	Suppose $ f $ is a bounded real function on $ [a,b] $ which is integrable. If $ \varphi:[a,b]\to[\varphi(a),\varphi(b)] $ is a continuous and strictly monotonic function, then $ f(\varphi) $ is integrable on $ [a,b] $, and $$ \int_{\phi(a)}^{\phi(b)}f(x)\ dx=\int_{a}^{b}f(\phi(y))\phi'(y)\ dy.$$
\end{theorem}
The proof will follow immediately from the Chain Rule and the Fundamental Theorem of Calculus.
\begin{proof}
Let $ F $ be the function such that $ F'=f $. We have $$F(\phi(y))'=F'(\phi(y))\phi'(y)=f(\phi(y))\phi'(y),$$ so $ F(\phi(u)) $ is the antiderivative of $ f(\phi(y))\phi'(y) $. By the Fundamental Theorem of Calculus, $$\int_{\phi(a)}^{\phi(b)}f(x)\ dx=F(\phi(b))-F(\phi(a))=\int_{a}^{b}f(\phi(y))\phi'(y)\ dy. $$
\end{proof}
\begin{example}
	Suppose $ f(x)=(2x^3+1)^7x^2 $. We would like to integrate $$\int_{0}^{1} (2x^3+1)^7x^2\ dx, $$ but it is not clear how to integrate with respect to $ x $ in this case. We will instead change the variable we are integrating with respect to and appeal to Theorem 6.5. Let $ \phi(x)=2x^3+1 $, and $ f(u)=u^7 $. Integration with respect to $ u $ gives \begin{align*}
		\int_{0}^{1} (2x^3+1)^7x^2\ dx=\frac{1}{6}\int_{\phi(0)}^{\phi(1)} f(\phi(x)) 6x^2\ dx=\frac{1}{6}\int_{\phi(0)}^{\phi(1)} f(\phi(x)) \phi'(x)\ dx=\frac{1}{6}\int_{1}^{3}f(u)\ du=\frac{1}{6}\int_{1}^{3}u^7\ du=\frac{410}{3}.
	\end{align*}
\end{example}
\subsection{Shortcomings of Riemann Integration}
We conclude this section by giving several examples of functions that are either not integrable, or we have no means of evaluating the integral of. From now until section 13,\textbf{ when we write ``integral'', we will mean ``Riemann integral''}, but you should keep these examples in the back of your head. They allude to the fact that we have only scratched the surface of integration.
\begin{example}[Dirichlet Function]
	Define $ f:\R\to\R $ as $$f(x)=\begin{cases}
	1\text{ if }x\in\Q\\0\text{ if }x\notin\Q
	\end{cases} .$$ As we say in Example 4.26, this function is nowhere continuous. This function also fails to be integrable on any interval. Without loss of generality, we can show that $ f $ is not integrable on $ [0,1] $. By the density of $ \Q $ in $ \R $, any $ [x_{i-1},x_i] $ will contain both a rational and irrational number for all $ [x_{i-1},x_i]\subset P $. This gives $ M_i=1 $ and $ m_i=0 $ for all $ i $. Therefore $$U(P,f)-L(P,f)=\sum_{i=1}^{n}(M_i-m_i)\Delta x_i=1-0=1 ,$$ for any $ P $. The Riemann Criterion does not hold, so $ f $ is not integrable on $ [0,1] $. While it is hard to argue one would ever need to integrate a nowhere continuous function, this is still dissatisfying. We know that $ \Q $ is countable, where as $ \R $ is not. This means that an uncountably infinite number of points of $ f(x)=0 $ an uncountable number of times, where as $ f(x)=1 $ a countable number of times. Is it crazy to think that the integral on $ [0,1]$ should just be zero then? Regardless of what we think it should be, it still is not Riemann integrable.
\end{example}

\begin{example}[Unbounded Function]
	Let $ f(x)=1/x $. This function cannot be integrated on $ (0,1] $, as it is unbounded on this interval, because $ f(x-)=\infty $. If we instead tried to use the interval $ [0,1] $, then $ f $ would not be defined as $ 0\in[0,1] $.  
\end{example}
\begin{example}[Integral Over All of $ \R $]
	Suppose we want to integrate some function over all of $ \R $. The whole real line is unbounded, so how do we measure the length of an infinite set and find upper and lower Riemann sums? Integration this way is not possible in this case. 
\end{example}
\begin{remark}[But What About Improper Integrals?]
	You've likely seen improper integrals that allow us to evaluate integrals of functions on unbounded sets, but these technically are not Riemann integrals. They are limits of Riemann integrals. This is another topic we will take up in Section 13. 
\end{remark}
\begin{example}[Discontinuities]
While the Fundamental Theorem of Calculus allows us to evaluate integrals for continuous functions, but how do we evaluate discontinuous functions? We know a function with a finite number of discontinuous can be integrated, but we never specified how those discontinuities contribute to the area. If we follow the reasoning from Example 6.11, we would hope that they contribute nothing to area, as there are an uncountably infinite number of continuous points on $ [a,b] $ that greatly out number the discontinuous ones. 
\end{example}


\newpage


\section{Sequences and Series of Functions}
Section 3 looked at sequences and series of numbers. We want to extend these ideas to real functions. It's tempting to think that we have already seen sequences of functions in the context of continuity. If $ f $ is continuous at $ x $, then for any sequence $ \{x_n\} $ which converges to $ x $, we have $ f(x_n)\to f(x) $. This looks like a sequence of functions, but it's still a numeric sequence. Each $ f(x_n) $ is a number. This sequence is just the result of applying a function to a set of numbers. This preliminary example provides a bonafide example of the types of sequences we are turning our attention to.
\begin{example}
	Define $ f_n:\R\to\R $ as $ f_n(x)=x^n $. We have a sequence of functions in $ \{f_n\} $.\footnote{The functions now have a subscript $ n $, indicating that each $ n $ garners a different function.} The first several terms of this sequence look like: $$ x,x^2,x^3,x^4,x^5,\ldots.$$
\end{example} 
We can easily discuss a sequence of functions, or its possible limit, in the context of continuity, differentiation, and integration. Determining how sequence and series interact with these concepts will be a major goal. Then we will focus on approximating and/or expressing a function as a limit of a sequence or series.  
\subsection{Metric Spaces of Functions}
Before we can discuss sequences of functions, we need to think about a metric space of functions. Until now, we've only look at metric spaces of numbers, but we never said a metric space couldn't be comprised of more abstract objects. We'll start by giving several definitions of sets of functions that have already been covered. This will amount to just introducing some convenient notation.
\begin{definition}
	The \textit{\color{red}set of continuous real functions} defined on a set $ X\subset\R $ is denoted $ C(X)$. 
\end{definition}
\begin{definition}
	The \textit{\color{red}set of continuous bounded real functions} defined on a set $ X\subset\R $ is denoted $ BC(X)$. 
\end{definition}
\begin{definition}
	The \textit{\color{red}set of $ n $-times differentiable (real) functions} defined on a set $ X\subset\R $ is denoted $ C^n(X) $. 
\end{definition}
\begin{example}
	We have $ BC(X)\subset C(X),$ and $C^\infty(X)\subset\ldots C^2(X)\subset C^1(X)\subset C(X) .$
\end{example}
\begin{example}
	If we define $ \mathscr{R}(X) $ to be the set of all Riemann integrable functions on $ X $, then $$C^\infty(X)\subset\ldots C^2(X)\subset C^1(X)\subset C(X)\subset\mathscr{R}(X) .$$ We also have $ BC(X)\subset\mathscr{R}(X) $. For now, we should hold off on working with the set $ \mathscr{R} $, because we'll define a larger set of integrable functions later. 
\end{example}
How would we measure the distance between two functions? On what sets of functions is this even possible? Well, if a function is unbounded, then that complicates matters. We cannot exactly measure the distance between two objects if one of them goes off to infinity.\footnote{The Euclidean metric in $ \R $ cannot measure the distance between $ \infty $ and $ x\in\R $ (although this is also because $ \infty\notin\R $). } For that reason, we will define a metric on $ BC(X) $. We would also be able to do this for the set of all bounded functions, but for now we'll just assume continuity. 

For $ g,f\in BC(X) $, what is a reasonable choice $ d(f,g) $? At some point $ x_0\in X $, we know the distance between the two functions is given by the Euclidean metric. $$d(f(x_0),g(x_0))=|f(x_0)-g(x_0)| $$ This isn't really the distance between the two functions though. It's the distance between the \textit{real numbers} the functions take on at the single point $ x_0 $. This doesn't account for all the other points on which the set is defined, $ X\backslash\{x_0\} $. Perhaps we could just take the maximum distance between the functions over $ X $? This would provide us with a conservative estimate, that says ``okay well the distance is \textit{at most} this.'' Just to be careful, we should take the supremum instead of the maximum.\footnote{If $ X $ is closed than each element of $ BC(X) $ achieves a max by the Extreme Value Theorem, so using the supremum in this case would not be strictly required.} Our candidate distance function is $$d(f,g)=\sup_{x\in X}|f(x)-g(x)| .$$
But does this choice of $ d $ satisfy definition 2.1?

We will have $ d(f,g)=0 $ if and only $ f=g $ in $ BC(X) $. We also have $ d(f,g)=f(g,f) $, as $ |f(x)-g(x)|=|g(x)-f(x)|  $. It only remains to be shown that this choice of metric verifies the triangle inequality. For $ f,g,h\in BC(X) $ the triangle inequality in the Euclidean metric gives  
\begin{align*}
	d(f,g)+d(g,h)&=\sup_{x\in X}|f(x)-g(x)|+\sup_{x\in X}|g(x)-h(x)|\\
				 &\ge |f(x)-g(x)|+|g(x)-h(x)|\\
				 &\ge |f(x)-h(x)|.
\end{align*}
This shows that $ \sup_{x\in X}|f(x)-g(x)|+\sup_{x\in X}|g(x)-h(x)| $ is an upper bound of $ |f(x)-h(x)| $, so it is weakly greater than the supremum of $ |f(x)-h(x)| $.
$$	d(f,g)+d(g,h)\ge \sup_{x\in X}|f(x)-g(x)|+\sup_{x\in X}|g(x)-h(x)|\ge \sup_{x\in X} |f(x)-h(x)|=d(f,h) $$
\begin{definition}
	The \textit{\color{red}supremum/sup metric} on $ BC(X) $ is defined as $$d(f,g)=\norm{f-g}_\infty=\sup_{x\in X}|f-g|. $$
\end{definition}
For now, take the sup metric's special notation as given. We will explain where this notation comes from in the beginning of Section 8. We should note that other valid metric spaces are the set of bounded real functions $ B(X) $, and $ C(X) $ as long as $ X $ is compact.\footnote{The function being bounded follows from the compact domain, as these functions are continuous. }

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ X $};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw[thick, dashed] (9.8,3.05) -- (9.8,8.58);
	\draw[ thick, blue] (0,5)  to[out=60,in=120]  (15,5);
	\draw[ thick, red] (0,6)  to[out=0,in=180]  (15,2);
	\draw[red] (14, 1.8) node {$ g $};	
	\draw[blue] (14, 7) node {$ f $};	
	\draw (10.5,5.815) node {$ \norm{f-g} $};	
	\end{tikzpicture}
	\caption{The distance between $ f $ and $ g $ as given by the sup metric is simply the supremum of the distance between $ f(x) $ and $ g(x) $ for a fixed $ x\in X $.}
\end{figure}
\begin{example}
	Let $ f(x)=\sin x $ and $ g(x)=\cos x $. Both of these functions are in the set $ BC(\R) $. We have $$\norm{f-g}_\infty=\sup_{x\in\R}|\cos x-\sin x|=\sqrt{2} .$$ In this case $ |\cos x-\sin x|=\R $ happens for all $ 3k\pi/4\in\R $ where $ k\in\Z $. 
\end{example}

In Subsection 7.3, the sup metric on spaces of functions will prove useful in our discussion of sequences of functions. 
\subsection{Pointwise Convergence}
We begin by defining one type of limit of a sequence of functions.
\begin{definition}
	Let $ E\subset \R $, and $ f_n:E\to\R $ for all $ n $. The sequence of functions $ \{f_n\} $ \textit{\color{red}converges (pointwise)} to $ f:E\to\R $ if for every $ x\in E $ and $ \varepsilon>0 $, there exists a \textit{corresponding} $ N\in\N $ such that $ |f_n(x)-f(x)|<\varepsilon $ whenever $ n\ge N $.  In this case we write $$ f(x)=\lim\limits_{n\to\infty} f_n(x)$$ for $ x\in E $.
\end{definition}
If $ \{f_n\} $ is a sequence of functions on $ E$, then for any fixed $ x\in E $, $ \{f_n(x)\} $ is a sequence of numbers. If this numerical sequence $ \{f_n(x)\} $ converges to $ f(x) $ for all $ x\in E $, then we have pointwise convergence. This means in practice we may use the same limit rules developed for a sequence of numbers on a sequence of functions. An illustration of this can be seen in Figure 73. The sequence of functions in this figure converges pointwise to $ x_0 $. It very well could converge pointwise to $ x_1 $ as well, it may just be that for some $ \varepsilon $ the value of $ N $ that gives $ |f_n(x_0)-f(x_0)|<\varepsilon $ for all $ n\ge N $ will not work for $ |f_n(x_1)-f(x_1)| $. That is to say, \textbf{just because $ \mathbf{\{f_n\}} $ converges pointwise to $ \mathbf{f} $ on $ \mathbf{E} $, that does not mean that one value of $ \mathbf{N} $ will give $ \mathbf{|f_n(x)-f(x)|<\varepsilon}$ for all $ \mathbf{x\in E} $!} This is why in Definition 7.5, we have a \textit{corresponding} $ N $ for each $ x\in E $ and $ \varepsilon>0 $. As the next examples will show, the particular value of $ N $ will depend on $ x $ and $ \varepsilon $. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw[dashed] (0,4.5) -- (15,4.5);
	\draw (-.6,4.5) node {$ f(x_0)$};

	\draw[dashed] (0,5) -- (15,5);
	\draw[dashed] (0,4) -- (15,4);
	\draw (-.8,4) node {$ f(x_0)-\varepsilon$};
	\draw (-.8,5) node {$ f(x_0)+\varepsilon$};
	\draw[ thick, blue] (0,0)  to[out=90,in=180]  (7.5,4.5);
	\draw[ thick, blue] (7.5,4.5) to[out=0,in=270] (15, 9) ;
	\draw[ thick, red] (6,0)  to[out=90,in=180]  (7.5,2);
	\draw[ thick, red]   (7.5,2) to[out=0,in=90] (9,0);
	\draw[ thick, red] (5.5,0)  to[out=90,in=180]  (7.5,3);
	\draw[ thick, red]   (7.5,3) to[out=0,in=90] (9.5,0);
	\draw[ thick, red] (5,0)  to[out=90,in=180]  (7.5,3.5);
	\draw[ thick, red]   (7.5,3.5) to[out=0,in=90] (10,0);
	\draw[ thick, red] (4,0)  to[out=90,in=180]  (7.5,4);
	\draw[ thick, red]   (7.5,4) to[out=0,in=90] (11,0);
	\draw[ thick, green] (3,0)  to[out=90,in=180]  (7.5,4.2);
	\draw[ thick, green]   (7.5,4.2) to[out=0,in=90] (12,0);
	\draw[ thick, green] (2,0)  to[out=90,in=180]  (7.5,4.4);
	\draw[ thick, green]   (7.5,4.4) to[out=0,in=90] (13,0);
	\draw[red] (9.15,0.5) node {\scriptsize$ f_1 $};
	\draw[red] (9.7,0.5) node {\scriptsize$ f_2 $};
	\draw[red] (10.2,0.5) node {\scriptsize$ f_3 $};
	\draw[red] (11.2,0.5) node {\scriptsize$ f_4 $};
	\draw[green] (12.2,0.5) node {\scriptsize$ f_5 $};
	\draw[green] (13.2,0.5) node {\scriptsize$ f_6 $};
	\draw[blue] (13.2,6.7) node {$ f $};
	\draw[thick] (7.5,0.2) -- (7.5,-0.2);
	\draw (7.5,-0.5) node {$ x_0 $};
		\draw[thick] (11.5,0.2) -- (11.5,-0.2);
	\draw (11.5,-0.5) node {$ x_1 $};
	\end{tikzpicture}
	\caption{The sequence of functions $ \{f_n\} $ converges pointwise to $ f $ at $ x_0 $. For this particular choice of $ \varepsilon $, we have $ |f_n(x_0)-f(x_0)|<\varepsilon $ for all $ n\ge 5 $. }
\end{figure}
\begin{example}
Let $ f_n=1/nx $ be a sequence of functions. We can show that $ f_n\to f=0 $ on the interval $ (0,1) $. If we let $ N=1/\varepsilon x $, then $$|f_n(x)-f(x)|=\left\lvert\frac{1}{nx}-0\right\rvert=\frac{1}{nx}<\frac{1}{Nx}=\frac{1}{(1/\varepsilon x)x}=\varepsilon $$ for all $ n\ge N $. In any real application we would calculate the limit in the following way: $$ \lim_{n\to\infty}f_n=\lim_{n\to\infty}\frac{1}{nx}=\lim_{n\to\infty}\left(\frac{1}{n}\right)x=0\cdot x=0$$
\end{example}
\begin{example}
	If we let $ f_n=(nx)/(1+nx^2) $, we have $ f_n\to f(x)=1/x $ on $ (0,\infty) $. 
	If we let $ N=1/\varepsilon x $, then \begin{align*}
		|f_n(x)-f(x)|&=\left\lvert\frac{nx}{1+nx^2}-\frac{1}{x}\right\rvert=\left\lvert\frac{nx^2-1+nx^2}{x+nx^3}\right\rvert=\left\lvert-\frac{1}{x+nx^3}\right\rvert=\frac{1}{x+nx^3}<\frac{1}{x+Nx^3}<\frac{1}{x+\left[\frac{1-x\varepsilon}{\varepsilon x^3}\right]x^3}=\varepsilon
	\end{align*} for all $ n\ge N $. 
\end{example}
One reason that sequences of functions are so useful is because we may be able to express some complicated function as a limit of relatively ``nice'' functions.\footnote{We discussed this type of situation in the passage after Corollary 3.1}. Perhaps if we need to take the limit of, differentiate, or integrate some complicated function, we can instead work with a sequence which converges to that function. If $ f_n\to f $, can we just use the following relationships:
\begin{align*}
	\int_{a}^{b}f(x)\ dx=\int_{a}^{b}\lim\limits_{n\to\infty}f_n(x)\ dx&\stackrel{?}{=}\lim\limits_{n\to\infty}\int_{a}^{b}f_n(x)\ dx\\
	f'(x)=\left(\lim\limits_{n\to\infty}f_n(x)\right)'&\stackrel{?}{=}\lim\limits_{n\to\infty}f_n^\prime(x)\\
	\lim\limits_{t\to x}f(t)= \lim\limits_{t\to x }\left(\lim\limits_{n\to\infty}f_n(x)\right)&\stackrel{?}{=}\lim\limits_{n\to\infty}\lim\limits_{t\to x}f_n(t)
\end{align*}
Is the limit of the integral the integral of the limits? Is the limit of the derivative the derivative of the limits? If $ f_n $ is continuous for all $ n $, then the final proposed relationship asks if the limit of continuous functions is continuous, as we have $ \lim_{t\to x}f_n(t)=f_n(x) $. $$	\lim\limits_{n\to\infty}\lim\limits_{t\to x}f_n(t)=\lim\limits_{n\to \infty }f_n(x)=f(x)\stackrel{?}{=}\lim_{t\to x} f(t) $$
These are all related to the question first mentioned in Remark 4.5. When are we allowed to move limits in and out of operations? These particular cases happen to be of theoretical interest, as integration, differentiation, and limits are all limiting processes. In effect, we want to know when we can interchange these limiting processes with a limit. Can we do this with pointwise convergence. As the next three examples show, the answer is, in general, an emphatic \textbf{NO}!
\begin{example}[Discontinuous Limit of Continuous Functions]
 We will restrict attention to $ [0,1]\subset\R $. Let $ f_n(x)=x^n $. We have $$ \lim\limits_{n\to\infty}f_n(x)=f(x)=\begin{cases}
 0\text{ if } 0\le x<1\\1\text{ if } x=1
 \end{cases} .$$ One way to see this is consider the numerical sequence of $ \{f_n(a)\} $ for a choice of $ a\in[0,1] $. If $ a\in[0,1) $, then $ f_n(a)=a^n $ goes to zero. If $ a=1 $, then $ f_n(a)=1  $ which makes $ \{f_n(a)\}  $ a constant sequence. This explains why $ f $ is zero on $ [0,1) $ and $ 1 $ on $ \{1\} $. Figure 74 shows this sequence of functions. 
 	\begin{figure}[h!]
 	\centering
 	\includegraphics[width=0.7\linewidth]{figures/xn}
 	\caption{A sequence of continuous functions which converges to a discontinuous function.}
 \end{figure}
 Despite $ f_n $ being continuous on $ [0,1] $ for all $ n $, we have the limit of $ \{f_n\} $ is discontinuous on $ [0,1] $. In particular, $$\lim\limits_{t\to 1}f(t)=0\neq 1=\lim\limits_{n\to \infty}1=\lim\limits_{n\to \infty}\lim\limits_{t\to 1}f_n(t) $$
\end{example} 
\begin{example}[Integral of Limit $ \neq $ Limit of Integral]
 Consider the sequence of functions given by $$ f_n(x)=\begin{cases}
 n^2x\text{ if } 0\le x<\frac{1}{n}\\
 2n-n^2x\text{ if } \frac{1}{n}\le x <\frac{2}{n}\\
 0\text{ if } \frac{2}{n}\le x\le 1
 \end{cases}.$$ We have $ f_n\to f=0 $ on $ [0,1] $. Figure 75 shows what $ f_n $ looks like for three values of $ n $. Calculating the integral of $ f_n $ for any $ n $ amounts to finding the area of a triangle of length $ 2/n $ and height $ n^2(1/n) $. $$ \int_{0}^1f_n(x)\ dx=\frac{1}{2}\left[\frac{2}{n}\left(n^2\frac{1}{n}\right)\right]=1 $$ Therefore we have $$\lim\limits_{n\to\infty}\int_{0}^{1}f_n(x)\ dx=\lim\limits_{n\to\infty}1=1\neq 0=\int_{0}^{1}0\ dx=\int_{0}^{1} f(x) dx=\int_{0}^{1}\lim\limits_{n\to\infty} f_n(x)\ dx.$$
 \begin{figure}[h!]
	\centering
 	\begin{tikzpicture}
	 \draw (-.3,9) node {$ \R$};
 	 \draw (15,-0.3) node {$ \R$};		
 	 \draw[thick,->] (0,0) -- (0,9);
     \draw[thick,->] (0,0) -- (15,0);
     \draw[thick, black] (0,0) -- (8,0) -- (10,2) -- (12,0) -- (14,0);
     \draw[thick] (14,0.2) -- (14,-0.2);
     \draw[thick, black] (0,0) -- (5,0) -- (6,4) -- (7,0) -- (14,0);
     \draw[thick, black] (0,0) -- (3,0) -- (3.5,8) -- (4,0) -- (14,0);
     \draw[thick] (14,0.2) -- (14,-0.2);
      \draw[thick, red] (0,0) -- (14,0);
     \draw (14,-0.5) node {$ 1 $};
      \draw[red] (13.5,0.3) node {$ f $};
      \draw (11.5,1) node {$ f_n $};
      \draw (6.8,2) node {$ f_{n'}$};
      \draw (4.3,3) node {$ f_{n''} $};
 	\end{tikzpicture}
	\caption{The sequence defined in Example 7.8 for whole numbers $ n<n'<n'' $. }
 \end{figure}
 
\end{example} 
\begin{example}[Derivative of Limit $ \neq $ Limit of Derivative]
Define $ f_n(x)=(\sin x) / x $. We have $f_n\to f$ where $ f(x)=0 $. We also have $$ \lim\limits_{n\to\infty}f_n'=\lim\limits_{n\to\infty}\cos nx\neq 0=f'=\left(\lim\limits_{n\to\infty}f_n\right)'$$
\end{example}
If we want to apply the topics of the previous three sections to sequences of functions, we'll need a stronger notion of convergence.  
\subsection{Uniform Convergence}
If you think return to the paragraph following Definition 7.5, you may wonder if certain functions converge such that one value of $ N $ only corresponds to $ \varepsilon $ and works for all $ x\in E $. We saw this type of reasoning in the definition of uniform continuity. If a function is unifomrly continuous on an interval, than for any $ \varepsilon>0 $, there exists a single $ \delta $ that gives $ |f(x)-f(y)|<\varepsilon $ when $ |x-y|<\delta $, regardless of the choice of $ x,y $. Similarly, if we find that the $ N $ in Definition 7.5 only corresponds to $ \varepsilon $ and not $ x $, i.e it works for all $ x\in E $, we say $ f $ converges uniformly. 
\begin{definition}
	Let $ E\subset \R $, and $ f_n:E\to\R $ for all $ n $. The sequence of functions $ \{f_n\} $ \textit{\color{red}converges uniformly} to $ f:E\to\R $ if for every  $ \varepsilon>0 $, there exists a  $ N\in\N $ such that $ |f_n(x)-f(x)|<\varepsilon $ whenever $ n\ge N $ \textit{for all} $ x\in E $.
\end{definition} 
\begin{note}
	There is no real standardized notation for uniform convergence. I'll opt to write $ f_n\overset{\text{uni}}{\to} f $ to distinguish between the case of pointwise convergence which is notated as $ f_n\to f $. 
\end{note}
Just like how uniform continuity is a \textit{much} stronger condition than continuity, uniform convergence is far stronger than pointwise convergence. One way to see this is to compare Figure 76 and Figure 73. If a sequence converges pointwise, it doesn't matter when $ f_n $ becomes arbitrarily close to $ f $ for different points $ x\in E $. With uniform convergence, $ f_n $ must get arbitraily close to $ f $  for all $ x\in E $ simultaneously. The next example shows explicitly that $ N $ is not a function of $ x $ if $ f_n\uni f $. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);
	\draw[ thick, blue] (0,0)  to[out=90,in=180]  (7.5,4.5);
	\draw[ thick, blue] (7.5,4.5) to[out=0,in=270] (15, 9) ;
	\draw[dashed] (0.5,-0.5)  to[out=90,in=180]  (7.5,4);
	\draw[dashed] (7.5,4) to[out=0,in=270] (15.5, 8.5) ;
	\draw[dashed] (-0.5,0.5)  to[out=90,in=180]  (7.5,5);
	\draw[dashed] (7.5,5) to[out=0,in=270] (14.5, 9.5) ;
	\draw[blue] (15,9.2) node {\scriptsize $ f $};
	\draw[black] (14.5,9.7) node {\scriptsize $ f+\varepsilon $};
	\draw[black] (15.5,8.7) node {\scriptsize $ f-\varepsilon $};
	\draw[dashed] (0.5,-0.5)  to[out=90,in=180]  (7.5,4);
	\draw[dashed] (7.5,4) to[out=0,in=270] (15.5, 8.5) ;
	\filldraw[white] (-0.52,0.5) rectangle (-0.03,5);
	\filldraw[white] (0,-0.52) rectangle (5,-0.03);
	\draw [red] plot [smooth, tension=1] coordinates { (0,0) (6,6) (15,3) };
	\draw [red] plot [smooth, tension=1] coordinates { (0,0) (4,5) (8,3) (12,7) (15,9)};
	\draw [thick, green] plot [smooth, tension=1] coordinates { (0,0) (2,3) (4,4) (6,4.8) (8,4.3) (12,5) (14, 6.7) (15, 8)};
	\draw [red] plot [smooth, tension=1] coordinates { (0,0) (2,4.5) (4,3) (7,6) (11,4) (14,6) (15,6.5)};
	\draw[black] (14.5,9.7) node {\scriptsize $ f+\varepsilon $};
	\draw[black] (15.5,8.7) node {\scriptsize $ f-\varepsilon $};
	\draw[red](13,8.2) node {\scriptsize $ f_2$};
	\draw[red](14,4) node {\scriptsize $ f_1$};
	\draw[red](14.5,6.6) node {\scriptsize $ f_3$};
	\draw[green](14,7) node {\scriptsize $ f_4$};
	\end{tikzpicture}
	\caption{The sequence $ \{f_n\} $ converges uniformly to $ f $. }
\end{figure}
\begin{example}
	Suppose $ f_n=1/n(1+x^2) $, and $ f=0 $. We have $ f_n\uni f $ on any interval $ [a,b] $. Let $ N=1/\varepsilon $. Because $ 1/(1+x^2)<1 $, we have $$|f_n(x)-f(x)|=\left\lvert\frac{1}{n(1+x^2)}-0\right\rvert=\frac{1}{n(1+x^2)}<\frac{1}{n}<\frac{1}{N}=\varepsilon $$ for all $ n\ge N $. Our choice of $ N $ did not depend on $ x $, hence $ f_n $ converges to $ f $ uniformly. 
\end{example}
If $ \{f_n\} $ converges uniformly and $ \{g_n\} $ converges pointwise, but not uniformly,\footnote{We specify that $ \{g_n\} $ does not converge uniformly, as every uniformly convergent sequence converges pointwise as well.}  then in theory, showing $ \{g_n\} $ converges should be easier. In Example 7.10 we needed to apply the inequality $ 1/(1+x^2)<1 $ to justify our choice of $ N $. We never needed to know any inequalities that would eliminate any reference to $ x $ when proving pointwise convergence. Nevertheless, uniform convergence becomes easier to verify due to our next results. The first is familiar, and eliminates the need for us to know the limit of $ f_n $. 
\begin{theorem}[Cauchy Criterion for Uniform Convergence]
	Let $ \{f_n\} $ be a sequence of real functions defined on $ E $. The sequence converges \textit{if and only if} for every $ \varepsilon>0 $ there exists a $ N\in \N $ such that
	$$|f_n(x)-f_m(x)|<\varepsilon $$ for all $ x\in E $ when $ m,n\ge N $. 
\end{theorem}
\begin{proof}{\color{white}a}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] Suppose $ f_n\uni f $ on $ E $. There exists a $ N\in\N $ such that \begin{align*}
			|f_n(x)-f(x)|&<\frac{\varepsilon}{2},\\
			|f_m(x)-f(x)|&<\frac{\varepsilon}{2},
		\end{align*}
		for all $ x\in E $ whenever $ n,m\ge N $. Therefore $$|f_n(x)-f_m(x)|=|f_n(x)-f_m(x)+0|=|f_n(x)-f_m(x)+f(x)-f(x)|\le |f_n(x)-f(x)|+|f(x)-f_m(x)|<\varepsilon $$ for all $ x\in E $ whenever $ n,m\ge N $. 
		\item [$ (\Longleftarrow) $]  Suppose for every $ \varepsilon>0 $ there exists a $ N\in \N $ such that
		$$|f_n(x)-f_m(x)|<\varepsilon $$ for all $ x\in E $ when $ m,n\ge N $. By our assumption, the Cauchy Criterion holds for the numerical sequence $ \{f_n(x)\} $ for all $ x\in E $, so for each $ x $, $ \{f_n(x)\} $ converges. Let $ \{f(x)\} $ be the limit of $ \{f_n(x)\} $ for each $ x\in E $. But this gives that $ f_n\to f $. We just need to show that this convergence is uniform. 
		
		For our assumed inequality, fix $ n $ and let $ m\to \infty $, noting that $ \lim_{m\to \infty}f_m(x)=f(x) $. This gives 
		\begin{align*}
			\lim_{m\to\infty}|f_n(x)-f_m(x)|&\le \lim_{m\to\infty}\varepsilon\\
		 	|f_n(x)-f(x)|&\le \varepsilon
		\end{align*}
		for every $ n\ge N $ and all $ x\in E $.\footnote{How do we know taking the limit as $ m\to\infty $ works? Well if $ m\ge N $, then as we let it grow the inequality will always be satisfied. }
	\end{enumerate}
\end{proof}
There exists a second criterion for uniform convergence that is of more interest, as it will lead to conclusions about the space $ BC(X) $ with the sup metric (Definition 7.4). Suppose we know that $ f_n\to f $. Can we glean any additional information about $ f_n $ that would allow us to conclude $ f_n\uni f $? The answer is found in the supremum of the distance between $ f_n $ and $ f(x) $, $$M_n=\sup_{x\in E}|f_n(x)-f(x)| .$$ This value $ M_n $ is shown in Figure 77. 
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,9) node {$ \R$};
	\draw (15,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,9);
	\draw[thick,->] (0,0) -- (15,0);

	\draw[thick] (1,0.2) -- (1,-0.2);
	\draw (1,-0.5) node {$ a $};
	\draw[thick] (14,0.2) -- (14,-0.2);
	\draw (14,-0.5) node {$ b $};
	\draw[thick,red] (1,1) -- (14,1); 
	\draw[thick,black] (1,1) to[out=0,in=260] (14,9); 
	\draw[thick,black, dashed] (1,1) to[out=0,in=230] (14,6); 
	\draw[thick,black, dashed] (1,1) to[out=0,in=220] (14,5);
	\draw[thick,black, dashed] (1,1) to[out=0,in=190] (14,2);   
	\draw[red] (14,0.7) node {$ f $};
	\draw[black] (13.5,8) node {$ f_n $};
	\draw[blue] (14.5,4) node {$ M_n $};
	\draw[thick, blue, dashed] (14,1) -- (14,9);
	\end{tikzpicture}
	\caption{A sequence of functions $ f_n\to f $ on $ [a,b] $ where $ M_n=\sup_{x\in [a,b]}|f_n(x)-f(x)| $.}
\end{figure}
In this particular illustration, it appears that $ M_n\to 0 $ as $ n\to\infty $. If this is the case, can we relate this to uniform convergence? As Figure 76 shows, the geometric interpretation of uniform convergence for a fixed $ \varepsilon $ is that we can always find some $ N $ such that the functions $ f_N $, $ f_{N+1} $,$ \ldots $, fall within a distance of $ \varepsilon $ from $ f $ on the entirety of $ E $. As it turns out, if $ M_n\to 0 $, then we can do this! If $ \sup_{x\in [a,b]}|f_n(x)-f(x)|<\varepsilon $, then it follows that $ |f_n(x)-f(x)|<\varepsilon $ for all of $ x\in E $, as $ |f_n(x)-f(x)| \le \sup_{x\in [a,b]}|f_n(x)-f(x)| $. As long as $ M_n\to 0 $, we will be able to do this for all $ \varepsilon $, as we can find some large enough $ N $ where $ M_n<\varepsilon $. This is the entire idea of the proof of the result, which will now be stated and proven formally. 
\begin{theorem}
	Suppose $ f_n\to f $ on $ E $.\footnote{You actually don't need to assume that $ f_n\to f $, but it doesn't really make sense to try and show uniform convergence with pointwise convergence.} Let $$M_n=\sup_{x\in E}|f_n(x)-f(x)| .$$ We have $ f_n\uni f $ \textit{if and only if} $ \lim_{n\to\infty}M_n=0 $. 
\end{theorem}
\begin{proof}{\color{white}a}
	\begin{enumerate}
		\item [$ (\Longrightarrow) $] If $ f_n\uni f $ on $E$ there exists an $ N $ such that $$ |f_n(x)-f(x)|<\varepsilon$$ for all $ x\in E $ whenever $ n\ge N $, which includes the value of $ x $ on which $\sup |f_n(x)-f(x)| $ is achieved. Therefore for all $ \varepsilon $ there exists an $ N\in\N $ such that
		\begin{align*}
			\sup_{x\in E}|f_n(x)-f(x)|&<\varepsilon,\\M_n&<\varepsilon,
		\end{align*}
		giving $ M_n\to 0 $. 
		\item [$ (\Longleftarrow) $]  If $ M_n\to 0 $, then for all $ \varepsilon>0 $ there exists an $ N\in\N $ such that \begin{align*}
			&M_n<\varepsilon\\\implies&\sup_{x\in E}|f_n(x)-f(x)|<\varepsilon\\\implies& |f_n(x)-f(x)|<\varepsilon\ (\text{for all }x\in E).
		\end{align*} 
		This is the definition of $ f_n\uni f $. 
	\end{enumerate}
\end{proof}
\begin{example}
	Let $ f_n(x)=x/n $. We have $ f_n\uni 0 $ on $ [a,b] $, as $$ \lim\limits_{n\to\infty}\sup_{x\in [a,b]}|f_n(x)-f(x)|=\lim\limits_{n\to\infty}\left\lvert\frac{\max\{|a|,|b|\}}{n}\right\rvert=0.$$
\end{example}
\begin{example}
	Let $ f_n=1/nx $. While $ f_n\to 0 $, the sequence $ \{f_n\} $ \textit{does not} converge uniformly on $ (0,1) $, as $$ \lim\limits_{n\to\infty}\sup_{x\in (0,1)}|f_n(x)-f(x)|=\lim\limits_{n\to\infty}\sup_{x\in (0,1)}\frac{1}{nx}= \lim\limits_{n\to\infty}\left(\lim\limits_{x\to0}\frac{1}{nx}\right)\neq 0.$$
\end{example}
\begin{corollary}
	Let $ f_n $ be a sequence of continuous real valued functions. We have $ f_n\uni f $ in the Euclidean metric \textit{if and only if} $ f_n\to f $ in the metric space $ BC(X) $ equipped with the supremum norm.
\end{corollary}
\begin{proof}
	If $ f_n\to f $ in $ BC(X) $, then for all $ \varepsilon>0 $ and $ x\in X $ there exists some $ N\in\N $ such that $$ d(f_n,f)=\sup_{x\in X}|f_n(x)-f(x)|=M_n<\varepsilon$$ for all $ n\ge N $. This holds if and only if $ \lim_{n\to\infty}M_n=0 $, which in turn is equivalent to $ f_n\uni f $ in the Euclidean metric by Theorem 7.2.
\end{proof}
\begin{remark}[The Metric of Uniform Convergence]
	Corollary 7.1 tells us that pointwise convergence in $ BC(X) $ with the sup metric is uniform convergence of functions in $ \R $. This is a really cool result, as it shows that our conservative estimate of distance between elements in $ BC(X) $ given by the supremum metric is linked to the idea of a stronger form of convergence of functions in $ \R $. For this reason, you sometimes will hear the supremum metric called the \textit{\color{red}uniform metric}, which may be written as $$ \norm{f-g}_\infty=\norm{f-g}_u.$$ 
\end{remark}

\subsection{Properties of Uniform Convergence}
As it turns out, uniform convergence is just what we need for the three proposed equalities at the end of Page 144 to hold.\footnote{...kind of. We will need to add a single assumption to uniform convergence for the limit of the derivatives to be the derivative of the limit.}
\begin{theorem}
	Suppose $ f_n\uni f $ on a metric space $ E\subset \R $. Let $ x $ be a limit point of $ E $, and suppose $$\lim\limits_{t\to x}f(t)=A_n .$$ Then $ \{A_n\} $ converges, and $$ \lim\limits_{t\to x}\lim\limits_{n\to\infty}f_n(t)=\lim\limits_{n\to\infty} A_n=\lim\limits_{n\to\infty} \lim\limits_{t\to x}f_n(t). $$
\end{theorem}
\begin{proof}
	Fix $ \varepsilon>0 $, and suppose $ f_n\uni f $ on $ E $. There exists an $ N\in\N $ such that $$ |f_n(t)-f_m(t)|<\varepsilon$$ for all $ t\in E $ when $ n,m\ge N $. Taking the limit of both sides of this inequality as $ t\to x $, we have \begin{align*}
\lim\limits_{t\to x}|f_n(t)-f_m(t)|&<\lim\limits_{t\to x}\varepsilon\\|A_n-A_m|&<\varepsilon.
	\end{align*}
	Therefore $ \{A_n\} $ is a Cauchy sequence in $ \R $, so it converges to some $ A\in E $. Now we need to show the limits may be interchanged.
	
	Using the uniform convergence of $ f_n $, we can choose a $ n $ such that \begin{align}
		|f_n(t)-f(t)|<\frac{\varepsilon}{3},
	\end{align}
	\textit{for all} $ t\in E $, \textit{and} such that \begin{align}
		|A_n-A|<\frac{\varepsilon}{3}
	\end{align}by the convergence of $ \{A_n\} $. For this same $ n $ we may choose some neighborhood $ N_\delta(x) $ such that\begin{align}
|f_n(t)-A_n|<\frac{\varepsilon}{3}
	\end{align}  if $ t\in N_\delta(x)\cap E $, and $ t\neq x $.\footnote{Instead of using a neighborhood of $ x $, we could say there exists some $ \delta $ such that $ |f_n(t)-A_n|<\varepsilon/3 $ for all $ t\in E $ which satisfy $ |t-x|<\delta $. Either way, all we're doing is using the definition of $ \lim\limits_{t\to x}f_n(t)=A $. } If we combine (22), (23), and (24), then we can conclude that for all $ \varepsilon $ there exists a $ N\in\N $ such that for all $ n\ge N $, \begin{align*}
	|f(t)-A|&=|f(t)-A+0+0|\\&=|f(t)-A+(f_n(t)-f_n(t))+(A_n-A_n)|\\&=|(f_n(t)-f(t))-(A_n-A)-(f_n(t)-A)|\\&=\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}\\&=\varepsilon.
	\end{align*}
	This shows the desired result.
\end{proof}
\begin{corollary}
	If $ \{f_n\} $ is a sequence of continuous functions on $ E\subset \R $, and if $ f_n\uni f $ on $ E $, then $ f $ is continuous on $ E $.
\end{corollary}[Uniform Convergence Preserves Continuity]
\begin{proof}
	This follows from the fact the alternate definition of continuity given by Corollary 4.2. Each function $ f_n $ is continuous, so we can find the limit as $ t\to x $ by evaluating at $ f $ at $ x $. Combining this with the equality given in Theorem 7.3 gives the result. 
	\begin{align*}
		\lim\limits_{t\to x}\lim\limits_{n\to\infty}f_n(t)&=\lim\limits_{n\to\infty} \lim\limits_{t\to x}f_n(t)\\ 	\lim\limits_{t\to x}f(t)&=\lim\limits_{n\to\infty}f_n\left( \lim\limits_{t\to x} t\right)\\
		\lim\limits_{t\to x}f(t)&=\lim\limits_{n\to\infty}f_n\left(x\right)\\
		\lim\limits_{t\to x}f(t)&=f\left(x\right)
	\end{align*}
\end{proof}
\begin{example}
	By Corollary 7.2, if the limit of a sequence of continuous functions is not continuous, then that sequence must not converge uniformly. This means that $ f_n(x)=x^n $ does not converge uniformly on $ [0,1] $ by Example 7.7.
\end{example}
\begin{example}[The Converse is False]
If a sequence of continuous functions converges to a continuous function, this does not imply the sequence converges uniformly. Let $ f_n(x)=n^2x(1-x^2)^n $ on the interval $ (0,1] $. For all values of $ n\in\N $, $ f_n $ is continuous on $ (0,1] $. We also have a continuous limit, as $ \lim_{n\to\infty}f_n(x)=0 $ on $ (0,1] $. Nevertheless, applying Theorem 7.2 shows that this sequence does not converge uniformly. 
\begin{align*}
	\lim\limits_{n\to\infty}\sup_{x\in (0,1]}|f_n(x)-f(x)|&=\lim\limits_{n\to\infty}\left[\frac{1}{\sqrt{2n-1}}\right]\not\to 0.
\end{align*}
\begin{remark}
	There are certain conditions where the converse of Corollary 7.2 does hold. These conditions are given as Theorem 7.13 in \cite{rudin1964principles}. As it turns out, the previous example works because we restricted our attention to $ (0,1] $, an interval which is not compact. 
\end{remark}

We now have the information necessary to prove a very nice property of the set of functions $ BC(X) $. Recall that a metric space is complete if every Cauchy sequence converges. Until now, we've only known one complete metric space, namely $ \R $. Example 3.28 claimed that the set of function $ C([a,b]) $ was complete, but at the time this could not be proved. Now we will prove this.\footnote{$ C([a,b])=BC([a,b]) $, as every continuous function on a compact domain is bounded.}
 \end{example}
\begin{theorem}($ BC(X) $ is Complete)
	Suppose $ X\subset \R $. The set of functions $ BC(X) $ is complete. 
\end{theorem}
\begin{proof}
	Let $ \{f_n\} $ be a Cauchy sequence in $ BC(X) $. By Theorem 7.1, the function converges uniformly to some limit $ f $ with domain $ X $. All that is left to show is that $ f\in BC(X) $, but this is a consequence of Corollary 7.2. Each $ f_n $ is continuous, and the sequence converges uniformly, so $ f $ is also continuous. By the definition of uniform convergence, there exists an $ n\in\N $ such that $ |f(x)-f_n(x)|<1 $ for all $ x\in X $. Since $ f_n(x) $ is bounded for all $ n $, $ f $ must be bounded, giving $ f\in BC(X) $. 
\end{proof}

Putting continuity aside, we can continue to uniform convergence's behavior with respect to integration. 
\begin{theorem}[Uniform Convergence Preserves Integration]
Suppose $ f_n $ is Riemann integrable on $ [a,b] $ for all $ n $, and $ f_n\uni f $ on $ [a,b] $. Then $ f $ is Riemann integrable on $ [a,b] $, and $$ \lim\limits_{n\to\infty}\int_{a}^{b}f_n(x)\ dx=\int_a^bf(x)\ dx.$$
\end{theorem}
\begin{proof}
	First, we will show that $ f $ is integrable on $ [a,b] $. Define $$ \varepsilon_n=\sup_{x\in [a,b]}|f_n(x)-f(x)|.$$ This definition gives $$ f_n-\varepsilon_n\le f\le f_n+\varepsilon_n, $$ as $ \varepsilon_n $ is the least upper bound of the distance between $ f_n $ and $ f $. Applying Theorem 6.2 to this inequality gives 
	\begin{align}
		\int_{a}^{b}(f_n-\varepsilon_n)\ dx\le\lowint_{a}^b f \ dx\upint_{a}^b f \ dx\le \int_{a}^{b}(f_n+\varepsilon_n)\ dx.
	\end{align}
This implies 
\begin{align*}
		\upint_{a}^b f \ dx - \lowint_{a}^b f \ dx&\le  \int_{a}^{b}(f_n+\varepsilon_n)\ dx - \int_{a}^{b}(f_n-\varepsilon_n)\ dx\\
		&= \int_{a}^{b}(f_n+\varepsilon_n) - (f_n-\varepsilon_n) \ dx\\&=\int_{a}^{b}2\varepsilon_n \ dx\\ &= 2\varepsilon_n (b-a).
\end{align*}
By uniform converge, $ \varepsilon_n\to 0 $ (Theorem 7.2), so $$	\upint_{a}^b f \ dx - \lowint_{a}^b f \ dx\le 0 $$ for all $ n $. The upper and lower integrals must be equal, which is the definition of Riemann integrability. 

Equation (25) also gives the desired equality. 
\begin{align*}
	\int_{a}^b f \ dx&\le \int_{a}^{b}(f_n+\varepsilon_n)\ dx\\
	&\le \int_{a}^{b}f_n\ dx + \int_{a}^{b}\varepsilon_n\ dx\\
	\left\lvert\int_{a}^{b}f\ dx -\int_{a}^{b}f_n\ dx\right\rvert&\le \varepsilon_n(b-a)\\
	\lim_{n\to\infty}\left\lvert\int_{a}^{b}f\ dx -\int_{a}^{b}f_n\ dx\right\rvert&\le \lim_{n\to\infty}\varepsilon_n(b-a)\\
	\left\lvert\int_{a}^{b}f\ dx -\lim_{n\to\infty}\int_{a}^{b}f_n\ dx\right\rvert&\le0.
\end{align*}
Therefore we have $$ \lim\limits_{n\to\infty}\int_{a}^{b}f_n(x)\ dx=\int_a^bf(x)\ dx.$$
\end{proof}

Having seen that continuity and integration play well with uniform convergence, it is tempting to assume the same holds for differentiation. Unfortunately, as the next example shows, this is not true! As far as properties of functions go, differentiability is fairly demanding. This means we will need to make assumptions to reach any result about uniform convergence and derivatives. 
\begin{example}[Uniform Convergence \textit{Does Not} Preserve Differentiation]
Let $$ f_n(x)=\frac{\sin nx}{\sqrt{n}}. $$ We have that $ f_n(x)\uni 0 $ as  $$  \lim\limits_{n\to\infty}\sup_{x\in [a,b]}|f_n(x)-f(x)|=\lim\limits_{n\to\infty}\sup_{x\in [a,b]}=	\left\lvert \frac{\sin nx}{\sqrt{n}} \right\rvert=\lim\limits_{n\to\infty}\frac{1}{\sqrt{n}}=0.$$ Despite having uniform convergence, we have that $$ \lim_{n\to\infty}f_n'(0)=\lim_{n\to\infty}\sqrt{n}\cos (n
\cdot0)\neq 0=\lim_{n\to 0}0=\lim_{n\to\infty}f'(0).$$

It is also possible that the derivative of a uniform limit may not exist. Let $$ f_n(x)=\sqrt{\frac{1}{n^2}+x^2}.$$ We have that $ f_n(x)\uni|x| $, so $ f'(0) $ does not exist. This happens despite the fact that $ f_n'(0) $ exists for all $ n $. 
\end{example}

The next theorem will give the conditions under which uniform convergence will preserve differentiation, but they are fairly strong. For one, the sequence $ \{f_n'\} $ must converge uniformly. Not only that, but the sequence $ \{f_n\} $ will need to converge pointwise for at least one point.  

\begin{theorem}[Uniform Convergence and Differentiation]
Suppose $ \{f_n\} $ is a sequence of functions, differentiable on $ [a,b] $ and such that $ \{f(x_0)\} $ converges pointwise for some $ x_0\in[a,b] $. If $ \{f_n'\} $ converges uniformly on $ [a,b] $, then $ \{f_n\} $ converges uniformly to some function $ f $ on $ [a,b] $, and $$f'(x)=\lim\limits_{n\to\infty}f_n'(x) $$ on $ [a,b] $. 
\end{theorem}
\begin{proof}
First, we will show that $ \{f_n\} $ converges uniformly to $ f $. The sequence $ \{f_n(x_0)\} $ converges, so it must be a Cauchy sequence. Similarly, $ \{f_n'\} $ converges uniformly, so it is also a Cauchy sequence. This means for all $ \varepsilon>0 $, there exists an $ N $ such that for all $ n,m\ge N $ we have \begin{align}
	|f_n(x_0)-f_m(x_0)|&\le \frac{\varepsilon}{2},\\|f_n(t)'-f_m'(t)|&\le \frac{\varepsilon}{2(b-a)}.
\end{align}
Applying the Mean Value Theorem to the function $ f_n-f_m $, there exists an element $ t_0\in[x,t] $ such that $$ |f_n'(t_0) - f_m'(t_0)|=\frac{|f_n(x)-f_m(x)-(f_n(x)-f_m(x))| }{|x-t|}.$$
This combined with (27) gives 
\begin{align}
	|f_n(x)-f_m(x)-f_n(t)+f_m(t)| &=|x-t|\cdot|f_n'(t_0) - f_m'(t_0)|,\nonumber\\&\le |x-t|\cdot \frac{\varepsilon}{2(b-a)}\\&=\frac{|x-t|}{b-a}\cdot\frac{\varepsilon}{2},\nonumber\\&\le \frac{\varepsilon}{2},
\end{align}
for any $ x,t\in[a,b] $ if $ n,m\ge N $.\footnote{The final line of this inequality follows from the fact that $ |x-t|\le b-a $, because $ x,t\in[a,b] $. } Combing (27) and (29)  while invoking the triangle inequality gives \begin{align*}
|f_n(x)-f_m(x)|&\le |f_n(x)-f_m(x) + 0 + 0|\\&=|f_n(x)-f_m(x) + (f_n(x_0)-f_n(x_0)) + (f_m(x_0)-f_m(x_0))|\\&\le 	|f_n(x)-f_m(x)-f_n(x_0)+f_m(x_0)| + |f_n-f_m(x_0)| \\&<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}\\&=\varepsilon
\end{align*}
for all $ x\in[a,b] $ when $ n,m\ge N $. The sequence $ f $ satisfies the Cauchy Criterion for uniform convergence, so $ f_n\uni f $ on $ [a,b] $. 

Now we will show that under our assumptions, the derivative of the limit is the limit of the derivatives. Fix a point $ x\in[a,b] $ and define \begin{align*}
	\phi_n(t)&=\frac{f_n(t)-f_n(x)}{t-x},\\\phi(t)&=\frac{f(t)-f(x)}{t-x},
\end{align*}
for $ x\in[a,b] $ where $ x\neq t  $. By (28), $$ |\phi_n(t)-\phi_m(t)|\le\frac{\varepsilon}{2(b-a)} $$ for all $ x\in[a,b] $ when $ n,m\ge N $. By the Cauchy Criterion for uniform convergence, $ \phi_n $ converges uniformly for $ t\neq x $. Since $ f_n\to f $, the uniform limit of $ \{\phi_n\} $ must be $ \phi $. $$\lim\limits_{n\to\infty}\phi_n(t)=\phi(t)\ \ \ \ \ (a\le t\le b, t\neq x)$$
Because $ \phi_n\uni\phi $ we can interchange limits using Theorem 7.3 to reach the desired equality. 
\begin{align*}
	f'(x)=\lim\limits_{t\to x}\frac{f(t)-f(x)}{t-x}= \lim\limits_{t\to x}\phi(t)= \lim\limits_{t\to x}\lim\limits_{n\to\infty}\phi_n(t)=\lim\limits_{n\to\infty}\lim\limits_{t\to x}\phi_n(t)=\lim\limits_{n\to\infty}\lim\limits_{t\to x}\frac{f_n(t)-f_n(x)}{t-x}=\lim\limits_{n\to\infty}f_n'(x)
\end{align*}
\end{proof}
\begin{example}(What if $ \{f\} $ Doesn't Converge Pointwise?)
	content...
\end{example}
\subsection{Approximation with Polynomials}
As stated earlier, uniform convergence is useful because we may want to take the limit of integral of a sequence of functions' limit, but not know the limit. With uniform convergence, this is no problem, as we can interchange limits freely. It still could be the case that $ f_n $ is difficult to work with. It's worth asking, given some function $ f  $ can we find a sequence of ``easy to work with'' functions which converges uniformly to $ f $? In this case, we know the desired limit of the sequence, but are looking for the sequence. 

Some of the easiest functions to work with our polynomials. As it turns out, the answer to this question becomes yes if we look for a sequence of polynomials. This was first shown by Weierstrass. 
\begin{theorem}[Weierstrass Approximation Theorem]
	If $ f $ is a real continuous function on the interval $ [a,b] $, then there exists a sequence of real polynomials $ \{P_n\} $ such that $ P_n(x)\uni f(x) $ on $ [a,b] $. 
\end{theorem}
\begin{proof}
	First, restrict attention to the case where $ [a,b]=[0,1] $ and $ f(0)=f(1)=0 $. We will construct the desired sequence $ \{P_n\} $. 
	
	Define the function $ Q_n(x) $ as $$Q_n(x)=c_n(1-x^2)^n $$ where $$ c_n=\left[\int_{-1}^{1} (1-t^2)^n\ dt\right]^{-1}.$$ This choice of $ c_n $ is made such that the integral of $ Q_n(x) $ on $ [-1,1] $ is $ 1 $ for all values of $ n $. \footnote{In this sense, $ c_n $ is just a scaling factor which standardizes $ Q_n(x) $ so this integral is always 1.}
	\begin{align}
		\int_{-1}^{1}Q_n(x)\ dx=c_n\int_{-1}^{1}(1-x^2)^n\ dx=\frac{\int_{-1}^{1} (1-x^2)^n\ dx}{\int_{-1}^{1} (1-t^2)^n\ dt}=1
	\end{align}
	Figure 78 shows $ Q_n(x) $ for a few values of $ n $. 
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.9\linewidth]{figures/landau_kern}
		\caption{The function $ Q_n(x) $ on the interval $ [-1,1] $ for several values of $ n $.}
		\label{fig:landaukern}
	\end{figure}
This seemingly random function $ Q_n(x) $ will be used to construct $ \{P_n\} $, so we may want to know some information about $c_n$'s magnitude that can be used to show $ P_n\uni f $. Using the fact that $ (1-x^2)^n\ge1-nx^2 $, $ 1\le 1/\sqrt{n} $, the symmetry of $ Q_n(x) $, and integration, we have 
\begin{align*}
	c_n=\left[\int_{-1}^{1} (1-t^2)^n\ dt\right]^{-1}=\left[2\int_{0}^{1} (1-t^2)^n\ dt\right]^{-1}\le\left[2\int_{0}^{1/\sqrt{n}} (1-t^2)^n \ dt\right]^{-1}&\le\left[2\int_{0}^{1/\sqrt{n}} 1-nt^2 \ dt\right]^{-1}\\&=\frac{3}{4}\sqrt{n}\\&<\sqrt{n}.
\end{align*}
This bound on $ c_n $ gives 
\begin{align}
Q_n(x)\le \sqrt{n}(1-\delta^2)^n
\end{align}
for any $ \delta\le |x| $. 

Now we can define our sequence of polynomials to be $$ P_n(x)=\int_{-1}^{1}f(x+t)Q_n(t)\ dt. $$ Due to the fact that $ f $ is zero outside the interval $ [0,1] $ we know $ f(x+t) $ is zero for $ t $ outside the interval $ [-x,1-x] $, so we can rewrite the bounds of integration in the definition of $ P_n(x) $.  

$$ P_n(x)=\int_{-x}^{1-x}f(x+t)Q_n(t)\ dt$$
If we apply a change of variables (Theorem 6.7) using $ u=x+t $ we get 

$$ P_n(x)=\int_{-x}^{1-x}f(x+t)Q_n(t)\ dt= \int_{0}^{1}f(u)Q_n(u-x)\ du.$$ At this point it's worth asking how we know $ P_n(x) $ is actually a polynomial. It becomes a bit clearer when writing $$ P_n(x)=\int_{0}^1f(u)c_n[1-(t-x)^2]^n\ dt.$$ The Binomial theorem could be used to expand the integrand, and any term with a $ t $ will be integrated and becomes a constant. What remains will be a polynomial in $ x $.  

We are now ready to show that $ P_n\uni f $. By the continuity of $ f $, for all $ \varepsilon>0 $, there exists a corresponding $ \delta>0 $ such that $ |y-x|<\delta $ implies $|f(y)-f(x)|<\varepsilon/2 $. Let $ M=\sup|f(x)| $. By (30), (31), and the fact that $ Q_n(x)\ge 0 $, we see that for any $ x\in[0,1] $,
\begin{align*}
	|P_n(x)-f(x)&=\left\lvert\int_{-1}^{1}f(x+t)Q_n(t) \ dt - f(x) \right\rvert\\
				&=\left\lvert\int_{-1}^{1}f(x+t)Q_n(t)\ dt - f(x)\cdot 1 \right\rvert\\
				&=\left\lvert\int_{-1}^{1}f(x+t)Q_n(t) \ dt- f(x)\int_{-1}^1 Q_n(t) \ dt \right\rvert\\
				&=\left\lvert\int_{-1}^{1}[f(x+t)-f(x)]Q_n(t) \ dt \right\rvert\\ 
				&\le \int_{-1}^{1}|f(x+t)-f(x)|Q_n(t) \ dt \\ 
				&= \int_{-1}^{-\delta}\underbrace{|f(x+t)-f(x)|}_{\le \sup|f(x)|+\sup|f(x)| }Q_n(t) \ dt + \int_{-\delta}^{\delta}\underbrace{|f(x+t)-f(x)|}_{\le \varepsilon/2}Q_n(t) \ dt + \int_{\delta }^{1}\underbrace{|f(x+t)-f(x)|}_{\le \sup|f(x)|+\sup|f(x)| }Q_n(t) \ dt\\
				&\le 2M\int_{-1}^{-\delta} Q_n(t)\ dt + \frac{\varepsilon}{2}\int_{-\delta}^{\delta} Q_n(t)\ dt + 2M\int_{\delta}^{1} Q_n(t)\ dt\\ &\le 4M\sqrt{n}(1-\delta^2)^n + \frac{\varepsilon}{2}\\&<\varepsilon
\end{align*}for a sufficiently large $ n $. This means that $ P_n\uni f $ on $ [0,1] $. 

Finally, suppose $ f $ is any real valued continuous function on $ [a,b] $. We can extend our result by writing $$g(x)=f(a+(b-a)x)$$ so that $ g $ is defined on $ [0,1] $.  We know the Theorem holds for this simpler case, so there exists a sequence of $ P_n $ such that $ P_n\uni g $ on $ [0,1] $. Since $$f(x)=g\left(\frac{x-a}{b-a}\right) ,$$ we have a sequence of polynomials which converge uniformly to $ f $ in the form of $ P_n\left(\frac{x-a}{b-a}\right).$

\end{proof}
At first glance, Weierstrass' Approximation Theorem may not seem useful at all. While polynomials are easy to take limits of, integrate, and differentiate, we know $ f $ to begin with. This means that it would be easier to just take the integral of $ f $ instead of the limit of the integrals of the more complicated $ P_n $'s if we were interested in this value.  Nevertheless this result can be useful when proving other results, as we know polynomials satisfy a myriad of desirable properties. 
\begin{example}
	Define $ f $ to be $$f(x)=\begin{cases}
	\frac{1}{2}-\left\lvert \frac{1}{2}-x\right\rvert \text{ if } -1 <x<1\\
	0\text{ otherwise}
	\end{cases}. $$ If we define $$ P_n(x)=\int_{0}^{1}f(t)Q_n(t-x)\ dt, $$ we should have that $ P_n(x)\uni f $ on $ [0,1] $. Figure 79 shows this convergence. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/SW_approx}
	\caption{The Weierstrass Approximation Theorem applied to $ f $ in Example 7.17.}
	\label{fig:swapprox}
\end{figure}
If we want to be more explicit, we can write out $ P_n $ to a limited extent.
\begin{align*}
	P_n(x)&=\int_{0}^{1}f(t)Q_n(t-x)\ dt\\&=\int_{0}^{1}\left(\frac{1}{2}-\left\lvert \frac{1}{2}-x\right)\right\rvert c_n[1-(t-x)^2]^n\ dt\\
	&=c_n\int_{0}^{1}\left(\frac{1}{2}-\left\lvert \frac{1}{2}-t\right\rvert\right)\sum_{k=0}^{n}(-t^2-2xt-x^2)^k\ dt
\end{align*}
At this point we could use the multinomial theorem to expand things further, but it gets fairly ugly. If you use Mathematica, Python with SymPy, or any other symbolic computation platform,  you can see the coefficients of $ P_n(x) $ for any $ n $. For example, for $ n=1 $ we have $$ P_1(x)=-\frac{3}{16}x^2 + \frac{3}{16}x + \frac{17}{128}$$
\end{example}
\begin{remark}[Stone-Weierstrass]
This approximation theorem would go on to be generalized several times. Not only can this result be generalized to complex functions, but it can be generalized beyond real intervals. A more general version is due to Marshall Stone and is known as the Stone-Weierstrass Theorem, and works in a vastly more general topological setting than that of the real line. 

\end{remark}


\subsection{Series}
Each sequence of functions $ \{f_n\} $ gives rise to a series of functions which results from summing $ f_n $ over $ n $. All the properties related to uniform convergence and sequences will follow for series as well. 


\begin{definition}
	A series of functions $ \sum f_n(x) $ \textit{\color{red}converges (pointwise)} on $ E\subset \R $ if the numerical series $ \sum f_n(x_0) $ converges for all $ x_0\in E $. Alternatively,   $ \sum f_n(x) $ converges on $ E\subset\R $ if the sequence of partial sums defined as $$ s_n(x)=\sum_{k=1}^{n}f_k(x) $$ converges on $ E $. In this case we write $\sum f_n(x)=f $, and call $ f $ the \textit{\color{red} sum} of the series.\footnote{You can assume that $ \sum f_n(x)=\sum_{n=1}^{\infty}f_n(x)$.} 
\end{definition}


\begin{definition}
	A series of functions $ \sum f_n(x) $ \textit{\color{red}converges uniformly} on $ E\subset\R $ if the sequence of partial sums $ \{s_n\} $ converges uniformly on $ E $.  
\end{definition}

Verifying that a series converges uniformly is slightly different than verifying a sequence converges uniformly. We will turn to a test that is analogous to the comparison test for numerical series. 
\begin{theorem}[Weierstrass M-Test]
	Suppose $ \{f_n\} $ is a sequence of functions defined on $ E\subset\R $, and suppose $ |f_n(x)|\le M_n$ for all $ x\in E $ and $ n\in\N $. Then $ \sum f_n $ converges uniformly on $ E $ if $ \sum M_n $ converges. 
\end{theorem}
\begin{proof}
Suppose that $ \sum M_n $ converges. The Cauchy Criterion holds for the partial sums of $ M_n $, so for any $ \varepsilon>0 $, there exists an $ N\in\N $ such that $$ \left\lvert\sum_{k=1}^{n}M_k-\sum_{k=1}^{m} M_k\right\rvert= \sum_{k=n}^{m}M_k< \varepsilon.$$
Because $ |f_n(x)|\le M_n $ for all $ x\in E $, we have $$ |s_n(x)-s_m(x)|=\left\lvert\sum_{k=1}^{n}f_k(x)-\sum_{k=1}^{m} f_k(x)\right\rvert=\left\lvert\sum_{k=n}^{m}f_k(x)\right\rvert\le \sum_{k=n}^{m}M_k< \varepsilon$$ for $ n,m\ge N $. This makes $ s_n(x) $ a Cauchy sequence, so it converges uniformly by Theorem 7.1. This gives the uniform convergence of $ \sum f_n(x) $.  
\end{proof}

Much like the Cauchy Criterion, the Weierstrass M-Test does not necessitate knowledge of the actual limit. 
\begin{example}
	Define $ f_n $ to be $$ f_n(x)=\frac{1}{x^2+n^2}.$$ We have that $$  f_n(x)=\frac{1}{x^2+n^2}\le \frac{1}{n^2}=M_n$$ for each $ n $ and all $ x\in\R $. Because the sequence $ \sum M_n=\sum 1/n^2 $ converges, the series $ \sum f_n(x) $ converges by the Weierstrass M-Test. 
\end{example}

\begin{example}[Converse of Weierstrass M-Test Fails]
	The converse of Theorem 7.8 need not hold. A somewhat trivial example is found in the constant function $ f_n(x)=(-1)^n/n $. The constant series of functions given by $ \sum f_n  $ converges uniformly to $ \ln 2 $. Despite this, there exists no bound on $ |f_n| $ which forms a convergent numerical series. The smallest possible value of $ M_n $ is $ 1/n $, but $ \sum 1/n $ diverges. Any example of a numerical series which converges but fails to converge absolutely would also work.
\end{example}

\begin{theorem}[Uniform Convergence and Series]
	Let $ \sum f_n $ be a series of functions on the set $ E\subset \R $.
	\begin{enumerate}
		\item If $ f_n $ is continuous for all $ n $, and $ \sum_n f_n $ converges uniformly, then $  \sum_n f_n $ is continuous. 
		\item If $ f_n $ is Riemann integrable for all $ n $ on $ [a,b]\subset E $, and $ \sum_n f_n $ converges uniformly on $ [a,b] $, then $  \sum_n f_n $ is Riemann integrable on $ [a,b] $, and $$ \int_{a}^{b}\sum_{n=1}^{\infty}f_n(x)\ dx=\sum_{n=1}^{\infty}\int_{a}^{b}f_n(x)\ dx $$ 
		\item Suppose $ f_n $ is differentiable on $ E $ for all $ n $, $ \sum f_n(x_0) $ converges pointwise for some $ x_0\in E $, and $ \sum f_n' $ converges uniformly. Then $ \sum f_n $ converges uniformly, and $$\frac{d}{dx}\sum_{n=1}^{\infty}f_n(x)=\sum_{n=1}^{\infty}\frac{df_n}{dx} $$
	\end{enumerate}
\end{theorem}
\begin{proof}
	The proofs for these follow from applying Theorem 7.3, Theorem 7.5, and Theorem 7.6 to the sequence of partial sums $ \{s_n\} $.
\end{proof}


\begin{example}(Weierstrass Function)
	We can now construct one of the most famous pathological functions in math. Using series of functions, we can construct a function which is continuous on all of $ \R $ but fails to be differentiable at a single point. Begin by defining $ \varphi(x)=|x|$ on $ [-1,1] $, and extend it to the real line by requiring that $ \varphi(x+2)=\varphi(x) $.\footnote{Formally, $ \varphi(x)= [(x+1)\mod 2] - 1$ .} 
\end{example}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/weierstrass}
	\caption{}
	\label{fig:weierstrass}
\end{figure}

\subsection{Power Series}
\begin{definition}
	A \textit{\color{red} power series (centered about $ a $)} is an infinite series of the form $$ \sum_{n=0}^{\infty}c_n(x-a)^n $$
\end{definition}
\begin{definition}
	A real function $ f $ is \textit{\color{red} (real) analytic} if it can be written as the sum of a power series. 
\end{definition}
\begin{definition}
	Let $ \sum c_n(x-a)^n $ be a power series. We define \textit{\color{red}radius of convergence $ R $} of this series to be the quantity $$ R=\frac{1}{\lim\sup_{n\to\infty}|c_n|^{1/n}}$$ where we adopt the convention that $ 1/0=\infty $ and $ 1/\infty=0 $. 
\end{definition}

As the name suggest, the radius of convergence of a power series dictates the set on which a power series will converge.
\begin{theorem}
	Let $ \sum c_n (x-a)^n $ be a power series. The series will converge on the set $ \{x\in\R\mid |x-a|<R\} $ and diverge on the set $ \{x\in\R\mid |x-a|>R\} $. 
\end{theorem}

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw[very thick, <->] (-8,0) -- (8,0);
	\draw[very thick, <-, red] (-8,0) -- (-4.04,0);
	\draw[very thick, ->, red] (4.04,0) -- (8,0);
	\draw[very thick] (0,0.1) -- (0,-0.1);
	\draw[color=green, very thick] (-4,0)circle (2pt);
	\draw[color=green, very thick] (4,0)circle (2pt);
	\draw[very thick, green] (-3.96,0) -- (3.96,0);
	\draw (0,-.3) node {$ a $};
	\draw (8,-.3) node {$ \R $};
	\draw[green] (-2,.3) node {$ |x-a|<R $};
	\draw[red] (-6,.3) node {$ |x-a|<R $};
	\draw (4,-.3) node {$ x+a $};
	\draw (-4,-.3) node {$ x-a $};
	\end{tikzpicture}
	\caption{The radius of convergence for a power series. The series converges for all values in green, and diverges for all values in red. It is not clear what happens when $ |x-a|=R $. }
\end{figure}

At some point you probably took a calculus course which introduced you to power series, and discussed how to integrate and differentiate power series. If you want to perform these operations on a power series, you an simply apply them to each term in the series. This was something special that could be done with power series, but not in general. Your calculus teacher may have emphasized this, but forgone any formal explanation. Now that we know all about uniform convergence and have Theorem 7.9, we can formally prove that power series exhibit these nice properties.  

\begin{theorem}
	Suppose the power series $ \sum c_n(x-a)^n $ converges for $ |x|<R  $, and define $ f(x)=\sum c_n(x-a)^n $ on the set of values such that $ |x|<R $. Then $ \sum c_n(x-a)^n $ converges uniformly to $ f $ on $ [-R +\varepsilon,R-\varepsilon] $ for all $ \varepsilon>0 $. Furthermore, $ f $ is continuous and differentiable in $ (-R,R) $ and $$f'(x)=\sum_{n=1}^{\infty}nc_nx^{n-1} .$$ 
\end{theorem}
\begin{corollary}
	Under the hypotheses of Theorem 7.10, $ f $ is Riemann integrable in $ (-R,R) $, and for $ [\alpha,\beta]\subset [-R+\varepsilon,R-\varepsilon] $ we have $$ \int_{\alpha}^{\beta}f(x)\ dx=\sum_{n=1}^{\infty}c_n\frac{(\alpha-a)^{n+1}-(\beta-a)^{n+1}}{n+1}.$$
\end{corollary}
\begin{corollary}
	Under the hypotheses of Theorem 7.10, $ f $ is infinitely differentiable in $ (-R,R) $,\footnote{We could simply write $ f\in C^\infty((-R,R)) $} and $$f^{(k)}(x)=\sum_{n=k}^{\infty} n(n-1)\cdots(n-k+1)c_n(x-a)^{n-k}.$$
\end{corollary}

\begin{remark}[Analyticity]

\end{remark}
\subsection{Taylor Series}
\section{Real Functions of Several Variables}
Now we'll take an aside to review linear algebra and multivariable functions. Most of this will look familiar, but some of the notation may differ slightly from what is seen in a linear algebra course. Come Section 16, \textbf{some of these definitions will be repeated with different notation}. Right now the notation will be more tailored to Euclidean space. Many results related to vector spaces will also be stated as fact or given in examples. The proofs of them can be found in any linear algebra textbook. 
\subsection{Euclidean Space}
Before we can define Euclidean space, we need to introduce the concept of a vector space. 
\begin{definition}
	A \textit{\color{red}vector space} is a set of vectors $ V $ over a field of scalers $ F $ equipped with two operations: $ +:V\times V\to V $, and $ \cdot:F\times V\to V $. These operations, called addition and scaler multiplication, must satisfy the following properties for all $ \x,\y,\mathbf{z}\in V $ and $ a,b\in F $:
	\begin{enumerate}
		\item $ \x+\y=\y+\x $,
		\item $ \x+(\y+\z)=(\x+\y)+\z $,
		\item there is a unique $ \mathbf{0}\in V $ such that $ \ze+\x=\x $,
		\item $ \x+(-1)\x=\ze $,
		\item $ 1\x=\x $,
		\item $ a(b\x)=(ab)\x $,
		\item $ (c+d)\x=c\x+d\x $,
		\item $ c(\x+\y)=c\x+c\y $.
	\end{enumerate}
\end{definition}
Certain vector spaces are equipped with a function that measures the ``length'' of a vector in $ V $. Norms can be seen as a generalization of the absolute value of a number.
\begin{definition}
	A \textit{\color{red}normed vector space} is a vector space $ V $ over a field of scalers $ F $ equipped with a function $\norm{\cdot}:V\to[0,\infty)$ called a \textit{\color{red}norm} which for all $ \x,\y\in V $ and $ c\in F $ satisfies:
	\begin{enumerate}
		\item $ \norm{\x}=0\iff \x=\ze$,
		\item $ \norm{c\x}=|c|\cdot \norm{\x} $,
		\item $ \norm{\x+\y}\le\norm{\x}+\norm{\y} $.
	\end{enumerate} 
\end{definition}
\begin{example}
	A trivial normed vector space is the set $ \R $ over $ \R $. In this case, a vector is just a real number. 
\end{example}
The next two examples of normed vector spaces are fairly abstract. The goal with them is to get a sense of just how general vector spaces can be.
\begin{example}[Vector Space of Real Sequences]
	Let $ \R^\N $ be the space of all real sequence $ \{x_n\} $, and $ \R $ be the set of scalers. If we define \begin{align*}
		\{x_n\}+\{y_n\}&=\{x_n+y_n\}\\
		a\{x_n\}&=\{ax_n \}
	\end{align*}
	for $ a\in\R $ and $ \{x_n\},\{y_n\}\in\R^\N $, then we have a vector space. To make this a normed vector space, we can restrict our attention to sequence for which $ \norm{\{x_n\}}_2<\infty $ where $$ \norm{\{x_n\}}_2=\left[x_1^2+x_2^2+\cdots\right]^\frac{1}{2}=\left(\sum_{n\in\N}x_n^2\right)^\frac{1}{2}.$$  We will call this normed vector space $ \ell^2 $. For $ x_n=1/n $ in $ \ell^2 $, then $$\norm{\{1/n\}}=\left(\sum_{n=1}^\infty\frac{1}{n^2}\right)^\frac{1}{2}=\left(\frac{\pi^2}{6}\right)^\frac{1}{2}=\frac{\pi}{\sqrt{6}} .$$ 
\end{example}
\begin{example}[Vector Space of Continuous Functions]
	Let $ C([a,b]) $ be the set of real continuous functions on $ [a,b] $. This set forms a vector space over the set of scalers $ \R $. This is also a normed vector space equipped with the sup norm, which is written as $$\norm{f}_\infty=\sup_{x\in [a,b]}|f(x)| .$$
\end{example}
\begin{remark}[Norm vs. Metric]
If you think all the way back to Example 2.2, we mentioned that norms and metrics are related. This is because every normed vector space is a metric space if we define $$ d(\x,\y)=\norm{\x-\y}.$$ The converse however is not true. A metric space needn't be a vector space, as a metric space isn't necessarily endowed with the algebraic structure of a vector space. In this sense, metric spaces are actually generalizations of normed vector spaces! When we have a normed vector space and want to discuss it in the context of a metric space, we will assume $ d(\x,\y)=\norm{\x-\y}.$ This is why I said that ``supremum metric'' is not a common term. If you're interested in the set $ C([a,b]) $ with $ d(f,g)=\sup_{x\in [a,b]}|f-g| $, then we simply specify that it has the \textit{\color{red}sup norm} (pronounced ``soup norm''), and infer the corresponding metric. 
\end{remark}


Despite having worked in Euclidean space for the majority of the previous sections, having assumed familiarity, we will formally define it now. 
\begin{definition}
	We define the normed vector space comprised of vectors $ \x\in\R^n $, scalers $ c\in\R $, a norm $\norm{\cdot}:\R^n\to\R$ given by $$ \norm{\x}=\sqrt{x_1^2+\cdots +x_2^2},$$and an \textit{\color{red}dot/inner product} $ \cdot:\R^n\times \R^n\to\R$ defiend by $$ \x\cdot\y=\sum_{i=1}^nx_i\cdot y_i,$$ as \textit{\color{red} Euclidean space}.
\end{definition}
\begin{example}[A Trivial Example]
Let $ \R $ be the vector space over a field of scalers $ \R $. The norm on this vector space is $|x| $. In this case, the dot product just becomes multiplication of real numbers. This happens to also be how scaler multiplication behaves, as each scaler is also a real number. 
\end{example}
\begin{example}[Column Vectors]
We represent an element $ \x=(x_1,\ldots, x_n)\in\R^n $ with a column vector $$\x=\begin{bmatrix}
x_1\\\vdots\\x_n
\end{bmatrix} .$$
\end{example}
\begin{note}[$ \norm{\cdot} $ vs. $ \norm{\cdot}_2 $]
	In Example 2.2, we talked about the Euclidean norm in relation to the Euclidean metric (that relationship follows from Remark 8.1). We denoted it by $ \norm{\cdot}_2 $. This is the most technical possible notation (for reasons we will see in the next example). That being said, if you are ever working with a normed vector space and it is obvious how the norm would be defined, feel free to use $ \norm{\cdot} $. If you're working in $ \R^n $, everyone will assume it is with the Euclidean norm. It never hurts to use $ \norm{\cdot}_2 $, but \textbf{you can assume this is the norm being used with $ \R^n $ from here on out unless otherwise specified.}
\end{note}
\begin{example}[But Where Does the 2 Come From?]
	But why do we even denote the Euclidean metric with $ \norm{\cdot}_2 $ in the first place? This comes from a more general norm on $ \R^n $ called the $ p $-norm, written as $$ \norm{\x}_p=(|x_1|^p+\cdots + |x_n|^p)^\frac{1}{p}=\left(\sum_{i=1}^n|x_i|^p\right)^\frac{1}{p}$$ for $ p\in\N $. We take the absolute value of each component of $ \x $ before exponentiating by $ p $ to ensure $ \norm{\x}_p>0 $.\footnote{When we defined a norm, we gave its domain as nonnegative numbers.} If we let $ p=2 $, then we have the Euclidean norm as given in Definition 8.3. If we take $ p=1 $, then we have $$ \norm{\x}_1=|x_1|+\ldots+|x_n|.$$ This particular choice would give rise to the taxi-cab metric from Example 2.3. If we take our vector space to be $ \R $, then $$\norm{x}_p=\left(|x|^p \right)^\frac{1}{p}=|x|$$ for any choice of $ p $.
	
	A useful exercise is picking different choices of $ p $ for $ \norm{\x}_p $ and a fixed $ \x\in\R^n $. This will give us a better sense of how the idea of ``length'' changes as $ p $ changes. Let $ \x=(1/2,1,2) $.  
	\begin{align*}
		\norm{\x}_1&=\frac{1}{2}+1+2=3.5\\
		\norm{\x}_2&=\left(\left(\frac{1}{2}\right)^2+1^2+2^2\right)^\frac{1}{2}=\left(\frac{1}{4}+1+4\right)^\frac{1}{2}=2.29\\
		\norm{\x}_5&=\left(\left(\frac{1}{2}\right)^5+1^5+2^5\right)^\frac{1}{2}=\left(\frac{1}{32}+1+32\right)^\frac{1}{5}=2.01\\
		\norm{\x}_{10}&=\left(\left(\frac{1}{2}\right)^{10}+1^{10}+2^{10}\right)^\frac{1}{{10}}=\left(\frac{1}{4}+1+1024\right)^\frac{1}{{10}}=2.0002\\
		\norm{\x}_{15}&=\left(\left(\frac{1}{2}\right)^{15}+1^{15}+2^{15}\right)^\frac{1}{{15}}=\left(\frac{1}{2^{15}}+1+2^{15}\right)^\frac{1}{{15}}=2.000004\\
	\end{align*}  
	What is happening here? When we let $ p=1 $, the length of $ \x $ is the sum of its components lengths. All the components contribute equally. When we take $ p=2 $, instead of summing the lengths, we first square them. Small components like $ 1/2 $ get smaller, and large components like $ 2 $ get bigger.\footnote{``small'' means less than $ 1 $, ``large'' means greater than $ 1 $.} After doing this, we sum them, and then take the square root so the length is not artificually inflated by use squaring each component. By squaring each number, we let the larger components ``contribute'' more to the length. We do so even more if we take $ p $ to be larger. In fact, if we let $ p\to\infty $, the largest component of $ \x $ will begin to dominate the others, being the only one that contributes to the norm. We actually end up with $$ \lim\limits_{p\to\infty}\norm{\x}_p=\norm{\x}_\infty=\sup\{|x_1|,\ldots, |x_n|\}=\max\{|x_1|,\ldots, |x_n|\}. $$ This is just the $ \sup $ norm, but for a finite set of points! 
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/lp_norm}
		\caption{}
		\label{fig:lpnorm}
	\end{figure}
	
	
	We can also use the $ p $-norm for sequence in $ \R^\N $. We call the space of sequences in $ \R^\N $ for which $ \norm{\{x_n\}}<\infty $ $ \ell^p $ space. We already saw $ \ell^2 $ in Example 8.1. The only real difference between $ \ell^p $ and $ \R^n $ equipped with $ \norm{\cdot}_p $, is that the vectors in $ \ell^p $ are infinite sequences. In $ \R^n $ each vector is finite. This is why for all $ \x\in\R^n $ we have $ \norm{\x}_p<\infty $. We don't need to eliminate any elements of $ \R^n $ to get a normed vector space like we need to for $ \R^\N $. This difference is so subtle, that you may even see the Euclidean metric on $ \R^n $ associated with the notation $ \ell^2 $, and the taxi-cab metric on $ \R^n $ associated with $ \ell^1 $.  
\end{example}
\begin{remark}[The Missing Link]
	We now know that as we let $ p\to\infty $ for $ \norm{\cdot}_p $ we end up with a discrete version of the $ \sup $ norm that we saw in Subsection 7.1. Is it possible to define $ \norm{f}_p $ for $ f\in C([a,b]) $, and conclude that $ \lim\limits_{n\to\infty}\norm{f}_p=\norm{f}_\infty $. The answer is yes.\footnote{Well, \textit{very} technically no. The limit will turn out to \textit{essentially} by the sup norm.} We will see this for the first time in Section 13, and dedicate an entire section to it with Section 17.
\end{remark}
\begin{example}[Euclidean Norm and Dot Products]
The dot product and the norm are related via the following equation:
$$ \norm{\x}=\sqrt{x_1^2+\ldots+x_n^2}=\sqrt{x_1x_1+\ldots+x_nx_n}=\sqrt{\x\cdot\x}.$$
\end{example}
\begin{proposition}[Cauchy-Schwarz Inequality]
	
\end{proposition}
\begin{definition}
	linear combo
\end{definition}
\begin{definition}
	span
\end{definition}
\begin{definition}
	independence 
\end{definition}
\begin{definition}
	basis
\end{definition}
\begin{remark}[Generalizing Euclidean Space]
If you look at the definition of Euclidean space, you should note it has an additional feature that is not required of a normed vector space. We defined a special function which we called the ``dot product''. In Section 16, we will study a type of vector spaces called Hilbert Spaces. These are generalizations of Euclidean space, as not only are they euipped with a norm, but they also have a function analagous to the dot product. 
\end{remark}
\subsection{Linear Transformations}
\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[step=1cm,black,very thin] (0,0) grid (6,6);
	\draw[thick,->] (-0.4,3) -- (6.4,3);
	\draw[thick,->] (3,-0.4) -- (3,6.4);
	\draw[thick, ->] (6.9,4) to[out=30,in=150] (7.9,4);	
	\draw (7.4,4.3) node {$  A $};
	\draw[color = red] (5.5,3.6) node {$  \x $};
	\draw[color = red] (13.9,4.8) node {$ A \x $};
	\draw[thick,->] (7.6,3) -- (14.4,3);
	\draw[very thick,->, color = red] (3,3) -- (6,4);
	\draw[very thick,->, color = red] (11,3) -- (14.5,5.5);
	\draw[thick,->] (11,-0.4) -- (11,6.4);
		\pgftransformcm{1}{0.5}{0.5}{1}{\pgfpoint{0}{-29}}
	\draw[step=1cm,black,very thin] (9,-5) grid (15,1);	
	\end{tikzpicture}
	\caption{.}
\end{figure}
\subsection{Nonlinear Transformations and Functions}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[step=1cm,black,very thin] (0,0) grid (6,6);
	\draw[thick,->] (-0.4,3) -- (6.4,3);
	\draw[thick,->] (3,-0.4) -- (3,6.4);
	\draw[thick, ->] (6.4,4) to[out=30,in=150] (7.4,4);	
	\draw (6.9,4.3) node {$ \mathbf{f}$};	
	\draw [very thin, black] (8,0) sin (9,1) cos (10,0) sin (11,-1) cos (12,0) sin (13,1) cos (14,0);
	\draw [very thin, black] (8,1) sin (9,2) cos (10,1) sin (11,0) cos (12,1) sin (13,2) cos (14,1);
	\draw [very thin, black] (8,2) sin (9,3) cos (10,2) sin (11,1) cos (12,2) sin (13,3) cos (14,2);
	\draw [very thin, black] (8,3) sin (9,4) cos (10,3) sin (11,2) cos (12,3) sin (13,4) cos (14,3);
	\draw [very thin, black] (8,4) sin (9,5) cos (10,4) sin (11,3) cos (12,4) sin (13,5) cos (14,4);
	\draw [very thin, black] (8,5) sin (9,6) cos (10,5) sin (11,4) cos (12,5) sin (13,6) cos (14,5);
	\draw [very thin, black] (8,6) sin (9,7) cos (10,6) sin (11,5) cos (12,6) sin (13,7) cos (14,6);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,7) sin (8,7.5) cos (9,7) sin (10,6.5) cos (11,7) sin (12,7.5) cos (13,7);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,6) sin (9,6.5) cos (10,6) sin (11,5.5) cos (12,6) sin (13,6.5) cos (14,6);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,5) sin (8,5.5) cos (9,5) sin (10,4.5) cos (11,5) sin (12,5.5) cos (13,5);
	\draw [very thin, black, rotate around={90:(11,4)}] (6,4) sin (7,4.5) cos (8,4) sin (9,3.5) cos (10,4) sin (11,4.5) cos (12,4);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,3) sin (8,3.5) cos (9,3) sin (10,2.5) cos (11,3) sin (12,3.5) cos (13,3);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,2) sin (9,2.5) cos (10,2) sin (11,1.5) cos (12,2) sin (13,2.5) cos (14,2);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,1) sin (8,1.5) cos (9,1) sin (10,.5) cos (11,1) sin (12,1.5) cos (13,1);
	\draw[thick,->] (7.6,3) -- (14.4,3);
	\draw[thick,->] (11,-0.4) -- (11,6.4);
	\filldraw (4,4) circle (2pt) node[anchor = south east] {$ \mathbf{x} $};
	\filldraw (11.65,4.5) circle (2pt) node[anchor = south west] {$ \mathbf{f(x)} $};
	\end{tikzpicture}
	\caption{.}
\end{figure}

\subsection{Space of Linear Transformations}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[thick,->] (-0.4,3) -- (6.4,3);
	\draw[thick,->] (3,-0.4) -- (3,6.4);
	\draw[thick, ->] (6.4,4) to[out=30,in=150] (7.4,4);	
	\draw (6.9,4.3) node {$  A $};	
	\draw[red] (5,5.3) node {$ \{\x\in\R^2\mid\norm{x} = 1\}$};
	\draw[red] (13.5,6.5) node {$ \{A\x\in\R^2\mid\norm{x} = 1\}$};
	\draw[blue] (9.7,2.4) node {$ \norm{A}_\text{op}$};
	\draw[very thick, color = red] (3,3) circle (2);
	\draw[very thick, color = blue, dashed] (11,3) -- (8.2,0.2);
	\draw[thick,->] (7.6,3) -- (14.4,3);
	\draw[thick,->] (11,-0.4) -- (11,6.4);
	\draw[very thick, color = red, rotate=45]  (9.9, -5.6) ellipse (4 and 1);
	\end{tikzpicture}
	\caption{.}
\end{figure}

\section{Differentiation with Several Variables}
\subsection{The Derivative as a Linear Map}
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[step=1cm,black,very thin] (0,0) grid (6,6);
	\draw[thick,->] (-0.4,3) -- (6.4,3);
	\draw[thick,->] (3,-0.4) -- (3,6.4);
	\draw[thick, ->] (6.4,4) to[out=30,in=150] (7.4,4);	
	\draw (6.9,4.3) node {$ \mathbf{f}$};	
	\draw [very thin, black] (8,0) sin (9,1) cos (10,0) sin (11,-1) cos (12,0) sin (13,1) cos (14,0);
	\draw [very thin, black] (8,1) sin (9,2) cos (10,1) sin (11,0) cos (12,1) sin (13,2) cos (14,1);
	\draw [very thin, black] (8,2) sin (9,3) cos (10,2) sin (11,1) cos (12,2) sin (13,3) cos (14,2);
	\draw [very thin, black] (8,3) sin (9,4) cos (10,3) sin (11,2) cos (12,3) sin (13,4) cos (14,3);
	\draw [very thin, black] (8,4) sin (9,5) cos (10,4) sin (11,3) cos (12,4) sin (13,5) cos (14,4);
	\draw [very thin, black] (8,5) sin (9,6) cos (10,5) sin (11,4) cos (12,5) sin (13,6) cos (14,5);
	\draw [very thin, black] (8,6) sin (9,7) cos (10,6) sin (11,5) cos (12,6) sin (13,7) cos (14,6);
		\draw [very thin, black, rotate around={90:(11,4)}] (7,7) sin (8,7.5) cos (9,7) sin (10,6.5) cos (11,7) sin (12,7.5) cos (13,7);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,6) sin (9,6.5) cos (10,6) sin (11,5.5) cos (12,6) sin (13,6.5) cos (14,6);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,5) sin (8,5.5) cos (9,5) sin (10,4.5) cos (11,5) sin (12,5.5) cos (13,5);
	\draw [very thin, black, rotate around={90:(11,4)}] (6,4) sin (7,4.5) cos (8,4) sin (9,3.5) cos (10,4) sin (11,4.5) cos (12,4);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,3) sin (8,3.5) cos (9,3) sin (10,2.5) cos (11,3) sin (12,3.5) cos (13,3);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,2) sin (9,2.5) cos (10,2) sin (11,1.5) cos (12,2) sin (13,2.5) cos (14,2);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,1) sin (8,1.5) cos (9,1) sin (10,.5) cos (11,1) sin (12,1.5) cos (13,1);
		\draw[thick,->] (7.6,3) -- (14.4,3);
	\draw[thick,->] (11,-0.4) -- (11,6.4);
	\filldraw (4,4) circle (2pt) node[anchor = south east] {$ \mathbf{x} $};
	\filldraw (11.65,4.5) circle (2pt) node[anchor = south west] {$ \mathbf{f(x)} $};
	\end{tikzpicture}
	\caption{derivative linear map.}
\end{figure}
	\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\draw[step=1cm,black,very thin] (0,0) grid (6,6);
	\draw[thick,->] (-0.4,3) -- (6.4,3);
	\draw[thick,->] (3,-0.4) -- (3,6.4);
	\draw[thick, ->] (6.4,4) to[out=30,in=150] (7.4,4);	
	\draw (6.9,4.3) node {$ \mathbf{f}$};	
	\draw [very thin, black] (8,0) sin (9,1) cos (10,0) sin (11,-1) cos (12,0) sin (13,1) cos (14,0);
	\draw [very thin, black] (8,1) sin (9,2) cos (10,1) sin (11,0) cos (12,1) sin (13,2) cos (14,1);
	\draw [very thin, black] (8,2) sin (9,3) cos (10,2) sin (11,1) cos (12,2) sin (13,3) cos (14,2);
	\draw [very thin, black] (8,3) sin (9,4) cos (10,3) sin (11,2) cos (12,3) sin (13,4) cos (14,3);
	\draw [very thin, black] (8,4) sin (9,5) cos (10,4) sin (11,3) cos (12,4) sin (13,5) cos (14,4);
	\draw [very thin, black] (8,5) sin (9,6) cos (10,5) sin (11,4) cos (12,5) sin (13,6) cos (14,5);
	\draw [very thin, black] (8,6) sin (9,7) cos (10,6) sin (11,5) cos (12,6) sin (13,7) cos (14,6);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,7) sin (8,7.5) cos (9,7) sin (10,6.5) cos (11,7) sin (12,7.5) cos (13,7);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,6) sin (9,6.5) cos (10,6) sin (11,5.5) cos (12,6) sin (13,6.5) cos (14,6);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,5) sin (8,5.5) cos (9,5) sin (10,4.5) cos (11,5) sin (12,5.5) cos (13,5);
	\draw [very thin, black, rotate around={90:(11,4)}] (6,4) sin (7,4.5) cos (8,4) sin (9,3.5) cos (10,4) sin (11,4.5) cos (12,4);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,3) sin (8,3.5) cos (9,3) sin (10,2.5) cos (11,3) sin (12,3.5) cos (13,3);
	\draw [very thin, black, rotate around={90:(11,4)}] (8,2) sin (9,2.5) cos (10,2) sin (11,1.5) cos (12,2) sin (13,2.5) cos (14,2);
	\draw [very thin, black, rotate around={90:(11,4)}] (7,1) sin (8,1.5) cos (9,1) sin (10,.5) cos (11,1) sin (12,1.5) cos (13,1);
	\draw[thick,->] (7.6,3) -- (14.4,3);
	\draw[thick,->] (11,-0.4) -- (11,6.4);
	\draw [ red, opacity=0.8] (6.5,0) -- (11,6);
	\draw [red, opacity=0.8] (6.95,0) -- (11.45,6);
	\draw [red, opacity=0.8] (7.4,0) -- (11.9,6);
	\draw [red, opacity=0.8] (7.85,0) -- (12.35,6);
	\draw [ red, opacity=0.8] (8.3,0) -- (12.8,6); 
	\draw [ red, opacity=0.8] (8.75,0) -- (13.25,6); 
	\draw [red, opacity=0.8] (9.2,0) -- (13.7,6); 
	\draw [red, opacity=0.8] (9.65,0) -- (14.15,6);
	\draw [ red, opacity=0.8] (10.1,0) -- (14.60,6);
	\draw [ red, opacity=0.8] (10.55,0) -- (15.05,6);
	\draw [ red, opacity=0.8] (11,0) -- (15.60,6);
	\draw [ red, opacity=0.8] (8.3,0) -- (12.8,6); 
	\draw [ red, opacity=0.8] (15.39,0) -- (12.89,6);
		\draw [ red, opacity=0.8] (14.76,0) -- (12.26,6);
		\draw [ red, opacity=0.8] (14.13,0) -- (11.63,6);
	\draw [ red, opacity=0.8] (13.5,0) -- (11,6);
	\draw [ red, opacity=0.8] (12.87,0) -- (10.37,6);
	\draw [ red, opacity=0.8] (12.24,0) -- (9.74,6);
	\draw [red, opacity=0.8] (11.61,0) -- (9.11,6);
	\draw [red, opacity=0.8] (10.98,0) -- (8.48,6);
	\draw [red, opacity=0.8] (10.35,0) -- (7.85,6);
	\draw [ red, opacity=0.8] (9.72,0) -- (7.23,6);
	\filldraw (4,4) circle (2pt) node[anchor = south east] {$ \mathbf{x} $};
	\filldraw (11.65,4.5) circle (2pt) node[anchor = south west] {$ \mathbf{f(x)} $};
	\draw[thick, ->,red] (6.4,2) to[out=-30,in=-150] (7.4,2);	
	\draw[red] (6.9,1.5) node {$ \mathbf{f'(x)}$};
	\end{tikzpicture}
	\caption{derivative linear map.}
\end{figure}
\subsection{The Chain Rule}
\subsection{The Inverse Function Theorem}
\subsection{The Implicit Function Theorem}
\section{Riemann Integration with Several Variables}
\subsection{Integration over a Rectangle}
\subsection{Iterated Integrals}
\subsection{Change of Variables}
\subsection{Change of Variables, Proof}
\section{Differential Forms}
\subsection{Motivation and Review of Vector Calculus}
\subsection{Tensors}
\subsection{Wedge Product}
\subsection{Tangent Vectors and Differential Forms}
\subsection{Manifolds}
\subsection{Stokes' Theorem}
\section{Measures}
questions I need to answer: length in $ \R $

How do discontinuities play with integration
\subsection{Motivation}
\subsection{$ \sigma $-Algebras}
\subsection{Measures}
\begin{figure}
	\centering
\begin{tikzpicture}
\draw[thick,black] (5,5) to[out=0,in=230] (10,6); 
\draw[thick,black] (10,6) to[out=50,in=120] (5, 8);
\draw[thick,black] (5, 8) to[out=310,in=120] (3, 6);
\draw[thick,black] (3, 6) to[out=310,in=180] (5,5);

\draw[fill=gray!40, thin, dashed, opacity=0.45] (2.5,4.5) rectangle (5,7);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (5,4.5) rectangle (7.5,7);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (7.5,4.5) rectangle (11,7);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (2.5,7) rectangle (5,9);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (5,7) rectangle (7.5,9);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (7.5,7) rectangle (11,9);

\draw[fill=gray!40, thin, dashed, opacity=0.45] (12,5.3) rectangle (13,6);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (12.5,4.85) rectangle (18.15,5.3);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (13,5.3) rectangle (18.5,7);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (13,6) rectangle (11.9,6.5);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (13,7) rectangle (12,6.5);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (18.5,5.5) rectangle (19,7.3);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (19.2,6) rectangle (19,6.9);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (18.5,7) rectangle (13,7.3);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (18.5,7.3) rectangle (13.7,7.6);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (18.2,7.6) rectangle (13.9,7.9);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (17.7, 7.9) rectangle (13.8,8.2);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (17,8.2) rectangle (13.7,8.5);
\draw[fill=gray!40, thin, dashed, opacity=0.45] (16.5,8.5) rectangle (13.7,8.7);
\draw[thick,black] (14,5) to[out=0,in=230] (19,6); 
\draw[thick,black] (19,6) to[out=50,in=120] (14, 8);
\draw[thick,black] (14, 8) to[out=310,in=120] (12, 6);
\draw[thick,black] (12, 6) to[out=310,in=180] (14,5);
\end{tikzpicture}
\caption{}
\end{figure}

dirac, counting, 
\subsection{Measures on $ \R $}
\section{Integration Revisited}
\subsection{Measurable Functions}
\subsection{Itegration of Simple Functions}
\subsection{Integration of Nonnegative Functions}
\subsection{Integration of Real Functions}

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\draw (-.3,7) node {$ \R$};
	\draw (16,-0.3) node {$ \R$};		
	\draw[thick,->] (0,0) -- (0,7);
	\draw[thick,->] (0,0) -- (16,0);
	\draw[thick] (1,-0.2) -- (1,0.2);
	\draw[thick] (14,-0.2) -- (14,0.2);
	\draw[blue] (14,5.5) node {$ f(x) $};	
	\draw[black] (1,-.5) node {$ a $};
	\draw[black] (14,-.5) node {$ b $};	
	\draw[fill=gray!40, thin, opacity=0.6] (1.77,1) rectangle (1,0);
	\draw[fill=gray!80, thin, opacity=0.6] (1.77,0) rectangle (2.56,2);
	\draw[fill=gray!120, thin, opacity=0.6] (3.5,3) rectangle (2.56,0);
	\draw[fill=gray!160, thin, opacity=0.6] (3.5,0) rectangle (4.7,4);
	\draw[fill=gray!200, thin, opacity=0.6] (8.53,5) rectangle (4.7,0);
	\draw[fill=gray!160, thin, opacity=0.6] (8.53,0) rectangle (8.9,4);
	\draw[fill=gray!120, thin, opacity=0.6] (9.16,3) rectangle (8.9,0);
	\draw[fill=gray!80, thin, opacity=0.6] (9.16,0) rectangle (9.47,2);
	\draw[fill=gray!40, thin, opacity=0.6] (12.32,1) rectangle (9.47,0);
	\draw[fill=gray!80, thin, opacity=0.6] (12.32,0) rectangle (13.06,2);
	\draw[fill=gray!120, thin, opacity=0.6] (13.58,3) rectangle (13.06,0);
		\draw[fill=gray!160, thin, opacity=0.6] (13.58,0) rectangle (14,4);
	\draw[very thin, dashed] (0,1) -- (14,1);
	\draw[very thin, dashed] (0,2) -- (14,2);
	\draw[very thin, dashed] (0,3) -- (14,3);
	\draw[very thin, dashed] (0,4) -- (14,4);
	\draw[very thin, dashed] (0,5) -- (14,5);
	\draw[very thin, dashed] (0,6) -- (14,6);
	\draw[black] (-0.18,2.5) node {$ I_2 $};	
	\draw[thick] (-0.2,2) -- (0.2,2);
	\draw[thick] (-0.2,3) -- (0.2,3);
	\draw[thick] (12.32,-0.2) -- (12.32,0.2);
	\draw[thick] (13.06,-0.2) -- (13.06,0.2);
	\draw[thick] (1.77,-0.2) -- (1.77,0.2);
	\draw[thick] (2.56,-0.2) -- (2.56,0.2);
	\draw[black] (2.165,-.3)node {\tiny $ f^{-1}(I_2) $};
	\draw[black] (12.69,-.3)node {\tiny $ f^{-1}(I_2) $};
	\draw [blue, thick] plot [smooth, tension=1] coordinates { (1,1) (7,6) (10.5,1) (14,5) };
 	
	\end{tikzpicture}
	\caption{Lebesgue Integral.}
\end{figure}

\subsection{Product Measures}
\subsection{Lebesgue Integration in $ n- $Dimensions}
\section{Differentiation with Measures}
\subsection{Motivation in $ \R $}
\subsection{Signed Measures}
\subsection{Radon-Nikodym Derivative}
\section{Point-Set Topology Revisited}
\subsection{Topological Spaces Revisited}
\subsection{Nets}
\subsection{Filters}
\subsection{Various Notions of Compactness}
\subsection{Continuity}
\subsection{The Product Topology}
\subsection{Pointwise and Uniform Convergence}
\section{Foundations of Functional Analysis}
\subsection{Normed Vector Spaces}
$ \ell^p $, $ L^p $
\subsection{Linear Functionals}
\subsection{The Baire Category Theorem}
\subsection{Topological Vector Spaces}
\subsection{Hilbert Spaces}
\section{$ L^p $ Spaces}
\subsection{Basic Theory}
\subsection{The Dual of $ L^p $}
\subsection{Inequalities}
\section{Riesz Representation Theorem}
\section{Foundations of Fourier Analysis}
\subsection{Convolutions}
\section{Distributions}
\section{Basic Probability Theory}
\newpage
\bibliography{analysis}{} 
\end{document}